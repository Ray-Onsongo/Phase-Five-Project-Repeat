{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea7fdae2",
   "metadata": {},
   "source": [
    "# Job Market Intelligence System: Problem Statement\n",
    "\n",
    "## 1. Context & Problem\n",
    "\n",
    "The current job market is fragmented and opaque, creating significant inefficiencies for three key stakeholder groups:\n",
    "\n",
    "- **Job Seekers** face information overload, skill uncertainty, and lack of salary transparency.\n",
    "- **HR Professionals & Recruiters** struggle with competitive hiring, compensation benchmarking, and identifying skill gaps.\n",
    "- **Educational Institutions & Career Counselors** operate with outdated curriculum and lack real-time market data for guidance.\n",
    "\n",
    "**Core Problem:** There is no unified, data-driven system that transforms raw job posting data into actionable, real-time insights for all stakeholders.\n",
    "\n",
    "## 2. Project Goal\n",
    "\n",
    "To develop a **Job Market Intelligence System** that analyzes job posting data to generate clear, actionable insights on skill demand, geographic opportunity, salary benchmarks, and market trends.\n",
    "\n",
    "## 3. Key Objectives\n",
    "\n",
    "1.  **Skill Demand Analysis:** Identify trending and declining technical skills.\n",
    "2.  **Geographic Opportunity Mapping:** Visualize job distribution and hotspots.\n",
    "3.  **Salary Benchmarking:** Estimate compensation by role, experience, and location.\n",
    "4.  **Job Classification & Trend Identification:** Categorize postings and spot emerging roles.\n",
    "\n",
    "## 4. Primary Business Questions\n",
    "\n",
    "- **For Job Seekers:** \"What skills should I learn, where are the jobs, and what salary can I expect?\"\n",
    "- **For HR/Recruiters:** \"How competitive is the market, and are our offers aligned?\"\n",
    "- **For Educators:** \"Which skills and emerging roles should we teach for?\"\n",
    "\n",
    "## 5. Success Metrics\n",
    "\n",
    "- **Technical:** >80% classification accuracy; <$15k MAE for salary prediction.\n",
    "- **Business:** Delivery of actionable insights, clear visualizations, and identifiable market patterns to all stakeholder groups.\n",
    "\n",
    "## 6. Project Scope\n",
    "\n",
    "**In-Scope (Initial Focus):**\n",
    "- Analysis of provided job posting datasets.\n",
    "- Focus on English-language technical/professional roles.\n",
    "- Skills extraction and trend analysis from job descriptions.\n",
    "\n",
    "**Value Delivered:**\n",
    "- **Job Seekers:** Reduced search time, clearer career paths.\n",
    "- **HR Professionals:** Competitive intelligence, optimized recruitment.\n",
    "- **Educators:** Data-driven curriculum alignment and career guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6594ee1b",
   "metadata": {},
   "source": [
    "# Data Exploration and Quality Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec581608",
   "metadata": {},
   "source": [
    "Let us now explore our dataset and understand its structure, quality and potential for our project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70e00fa",
   "metadata": {},
   "source": [
    "## Data Loading and Inspection\n",
    "We will now load the data andexamine its basic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "459edf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARD LIBRARIES\n",
    "import os                          # Interacting with the operating system (file paths, directories)\n",
    "import math                        # Math functions (e.g., sqrt)\n",
    "import pickle                      # Save/load Python objects\n",
    "import joblib                      # Save/load trained models efficiently\n",
    "\n",
    "# DATA MANIPULATION & NUMERICAL COMPUTATION\n",
    "import pandas as pd                # Data loading, cleaning, and manipulation\n",
    "import numpy as np \n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import re\n",
    "import warnings                     # Numerical operations and array manipulation\n",
    "\n",
    "# VISUALIZATION\n",
    "import matplotlib.pyplot as plt    # General-purpose plotting\n",
    "import seaborn as sns              # Statistical data visualization\n",
    "\n",
    "# STATISTICS\n",
    "from scipy import stats             # Statistical functions, e.g., z-score, t-tests\n",
    "from scipy.stats import entropy     # Measure of information content (e.g., Shannon entropy)\n",
    "\n",
    "# MACHINE LEARNING\n",
    "import xgboost as xgb               # XGBoost for gradient boosting models\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,              # Split data into train/test sets\n",
    "    StratifiedKFold,               # Cross-validation preserving class distribution\n",
    "    GridSearchCV                   # Hyperparameter tuning\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,        # Random Forest classifier\n",
    "    VotingClassifier,              # Combine multiple models via voting\n",
    "    GradientBoostingRegressor      # Gradient boosting for regression\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic regression classifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    fbeta_score,                   # F-beta score for classification performance\n",
    "    precision_recall_curve,        # Precision-recall curve\n",
    "    classification_report,         # Detailed classification metrics\n",
    "    mean_squared_error,            # Regression metric\n",
    "    mean_absolute_error,           # Regression metric\n",
    "    auc,                           # Area under curve (ROC or PR)\n",
    "    confusion_matrix,              # True vs predicted labels summary\n",
    "    roc_curve,                     # Compute ROC curve for binary classification\n",
    "    make_scorer,                   # Create custom scoring function for model evaluation\n",
    "    precision_score,               # Precision metric\n",
    "    recall_score,                  # Recall metric\n",
    "    f1_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler       # Feature scaling\n",
    "from sklearn.cluster import KMeans                     # Clustering algorithm\n",
    "from sklearn.decomposition import PCA                  # Principal Component Analysis (dimensionality reduction)\n",
    "from sklearn.inspection import permutation_importance  # Measure feature importance via performance drop\n",
    "from sklearn.calibration import CalibratedClassifierCV # Fixes overconfident probabilities\n",
    "\n",
    "# HANDLING IMBALANCED DATA\n",
    "from imblearn.over_sampling import SMOTE          # Synthetic oversampling for minority class\n",
    "from imblearn.pipeline import Pipeline            # Pipelines compatible with imbalanced-learn\n",
    "\n",
    "# MISCELLANEOUS SETTINGS\n",
    "pd.set_option(\"display.max_columns\", None)       # Display all columns in DataFrame\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')                # Suppress warnings for cleaner output\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\", font_scale=0.9)\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 5)  # Default figure size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eec490",
   "metadata": {},
   "source": [
    "- We got an error when trying to read in the dataset due to the unique encoding of the data inside the dataset. Therefore, we had to employ some encoding to debug the dataset and make it readable by the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4cf1267",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xd0 in position 1736: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Load the data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m Job_Posting_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob_Posting_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Job_Posting_df.head()\u001b[39;00m\n\u001b[0;32m      4\u001b[0m encodings_to_try \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcp1252\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindows-1252\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmac_roman\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:579\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:668\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2050\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xd0 in position 1736: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "#Load the data\n",
    "Job_Posting_df = pd.read_csv(\"Job_Posting_data.csv\")\n",
    "#Job_Posting_df.head()\n",
    "encodings_to_try = ['ISO-8859-1', 'cp1252', 'latin1', 'windows-1252', 'utf-8-sig', 'mac_roman']\n",
    "\n",
    "print(\"Trying different encodings...\")\n",
    "for encoding in encodings_to_try:\n",
    "    try:\n",
    "        Job_Posting_df = pd.read_csv(\"Job_Posting_data.csv\", encoding=encoding)\n",
    "        print(f\"SUCCESS with {encoding} encoding!\")\n",
    "        print(f\"   Shape: {Job_Posting_df.shape}\")\n",
    "        print(f\"   Columns: {len(Job_Posting_df.columns)}\")\n",
    "        print(f\"\\nFirst 3 rows:\")\n",
    "        print(Job_Posting_df.head(3))\n",
    "        print(\"\\nColumn names:\")\n",
    "        for i, col in enumerate(Job_Posting_df.columns, 1):\n",
    "            print(f\"  {i:2}. {col}\")\n",
    "        break\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"Failed with {encoding}: {str(e)[:50]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed with {encoding}: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9420e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_to_try = ['ISO-8859-1', 'cp1252', 'latin1', 'windows-1252', 'utf-8-sig', 'mac_roman']\n",
    "\n",
    "print(\"Trying different encodings...\")\n",
    "for encoding in encodings_to_try:\n",
    "    try:\n",
    "        Job_Posting_df = pd.read_csv(\"Job_Posting_data.csv\", encoding=encoding)\n",
    "        print(f\"SUCCESS with {encoding} encoding!\")\n",
    "        print(f\"   Shape: {Job_Posting_df.shape}\")\n",
    "        print(f\"   Columns: {len(Job_Posting_df.columns)}\")\n",
    "        print(f\"\\nFirst 3 rows:\")\n",
    "        print(Job_Posting_df.head(3))\n",
    "        print(\"\\nColumn names:\")\n",
    "        for i, col in enumerate(Job_Posting_df.columns, 1):\n",
    "            print(f\"  {i:2}. {col}\")\n",
    "        break\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"Failed with {encoding}: {str(e)[:50]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed with {encoding}: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70481c4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Job_Posting_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Job_Posting_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Job_Posting_df' is not defined"
     ]
    }
   ],
   "source": [
    "Job_Posting_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14424ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_Posting_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ffcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_Posting_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d815e9cc",
   "metadata": {},
   "source": [
    "- We observed that there were 21 columns present in the dataset and 9919 rows. We also observed that one column, **Ticker** was a null column which we later dropped while doing the data preparaton.\n",
    "- We then proceeded to doing EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f2b0b",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daea49e9",
   "metadata": {},
   "source": [
    "- We started by doing an overview of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbafc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"-\"*20)\n",
    "print(f\"Total Records: {Job_Posting_df.shape[0]:,}\")\n",
    "print(f\"Total Features: {Job_Posting_df.shape[1]}\")\n",
    "print(f\"Data loaded: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfdd4ac",
   "metadata": {},
   "source": [
    "Columns Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513dfa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"COLUMN SUMMARY\")\n",
    "print(\"-\"*20)\n",
    "print(\"\\nIndex | Column Name                    | Non-Null | Dtype\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for i, col in enumerate(Job_Posting_df.columns, 1):\n",
    "    non_null = Job_Posting_df[col].notnull().sum()\n",
    "    percentage = (non_null / len(Job_Posting_df)) * 100\n",
    "    dtype = Job_Posting_df[col].dtype\n",
    "    print(f\"{i:5d} | {col:30} | {non_null:7,d} ({percentage:5.1f}%) | {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e431e",
   "metadata": {},
   "source": [
    "As you can see, our dataset contains 19 columns, one which contains numerical values and the other which are text columns. We will now proceed on data exploration and quality analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f590e981",
   "metadata": {},
   "source": [
    " Data Exploration and Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a2e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"-\"*20)\n",
    "print(f\"Total Records: {Job_Posting_df.shape[0]:,}\")\n",
    "print(f\"Total Features: {Job_Posting_df.shape[1]}\")\n",
    "print(f\"Data loaded: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8addf3a",
   "metadata": {},
   "source": [
    "- Let's now do a column summary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde9c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"COLUMN SUMMARY\")\n",
    "print(\"-\"*20)\n",
    "print(\"\\nIndex | Column Name                    | Non-Null | Dtype\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for i, col in enumerate(Job_Posting_df.columns, 1):\n",
    "    non_null = Job_Posting_df[col].notnull().sum()\n",
    "    percentage = (non_null / len(Job_Posting_df)) * 100\n",
    "    dtype = Job_Posting_df[col].dtype\n",
    "    print(f\"{i:5d} | {col:30} | {non_null:7,d} ({percentage:5.1f}%) | {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b130d",
   "metadata": {},
   "source": [
    "- Now that we have done a summary of the columns, let's go ahead and have a look at the number of missing values, since as you can see, the summary we have done above shows us the percentage of non-null values in the respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc127123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values Analysis\n",
    "print(\"MISSING VALUES ANALYSIS - TOP 10 WORST COLUMNS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Calculate missing values\n",
    "missing_data = []\n",
    "for col in Job_Posting_df.columns:\n",
    "    non_null = Job_Posting_df[col].notnull().sum()\n",
    "    null_count = Job_Posting_df[col].isnull().sum()\n",
    "    null_pct = (null_count / len(Job_Posting_df)) * 100\n",
    "    missing_data.append({\n",
    "        'Column': col,\n",
    "        'Non-Null': non_null,\n",
    "        'Null Count': null_count,\n",
    "        'Null %': null_pct,\n",
    "        'Dtype': Job_Posting_df[col].dtype\n",
    "    })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_data)\n",
    "missing_df = missing_df.sort_values('Null %', ascending=False)\n",
    "\n",
    "# Display top 10\n",
    "print(missing_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaa97da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MISSING DATA CATEGORIZATION\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Categorize columns by missing percentage\n",
    "def categorize_missing(pct):\n",
    "    if pct == 0:\n",
    "        return 'Complete (0%)'\n",
    "    elif pct < 5:\n",
    "        return 'Good (<5%)'\n",
    "    elif pct < 20:\n",
    "        return 'Moderate (5-20%)'\n",
    "    elif pct < 50:\n",
    "        return 'High (20-50%)'\n",
    "    elif pct < 100:\n",
    "        return 'Very High (50-99%)'\n",
    "    else:\n",
    "        return 'Completely Missing (100%)'\n",
    "\n",
    "missing_df['Category'] = missing_df['Null %'].apply(categorize_missing)\n",
    "category_counts = missing_df['Category'].value_counts()\n",
    "\n",
    "for category, count in category_counts.items():\n",
    "    cols_in_category = missing_df[missing_df['Category'] == category]['Column'].tolist()\n",
    "    print(f\"\\n{category:30}: {count:2d} columns\")\n",
    "    if len(cols_in_category) <= 5:\n",
    "        print(f\"   {', '.join(cols_in_category)}\")\n",
    "    else:\n",
    "        print(f\"   {', '.join(cols_in_category[:3])}, ... and {len(cols_in_category)-3} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b0e97b",
   "metadata": {},
   "source": [
    "- Now that we have an idea of the missing values and their percentages in the dataset, we can now see that the columns, **Ticker** and **Salary**, can be dropped from our dataset. But instead of going with this approach of dropping columns, let's do a critical column analysis to determine which columns are the most important for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d2c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Critical Column Assessment\n",
    "\n",
    "print(\"CRITICAL COLUMNS ASSESSMENT\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "critical_columns = {\n",
    "    'Job Opening Title': 'Primary identifier - ESSENTIAL',\n",
    "    'Description': 'Contains skills/requirements - ESSENTIAL',\n",
    "    'Category': 'Job classification - IMPORTANT',\n",
    "    'Location': 'Geographic info - IMPORTANT',\n",
    "    'Seniority': 'Experience level - IMPORTANT',\n",
    "    'Salary': 'Compensation - DESIRABLE but limited',\n",
    "    'Contract Types': 'Job type - DESIRABLE',\n",
    "    'Job Status': 'Open/Closed status - DESIRABLE'\n",
    "}\n",
    "\n",
    "print(\"\\nColumn                  | Non-Null |   %   | Status\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for col, importance in critical_columns.items():\n",
    "    if col in Job_Posting_df.columns:\n",
    "        non_null = Job_Posting_df[col].notnull().sum()\n",
    "        pct = (non_null / len(Job_Posting_df)) * 100\n",
    "        \n",
    "        if pct > 90:\n",
    "            status = \"Excellent\"\n",
    "        elif pct > 70:\n",
    "            status = \"Acceptable\"\n",
    "        elif pct > 50:\n",
    "            status = \"Concerning\"\n",
    "        else:\n",
    "            status = \"Critical Issue\"\n",
    "        \n",
    "        print(f\"{col:23} | {non_null:8,d} | {pct:5.1f}% | {status}\")\n",
    "        print(f\"                      {importance}\")\n",
    "    else:\n",
    "        print(f\"{col:23} | {'NOT FOUND':^8} | {'N/A':^5} | ❌ Missing Column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed88a77",
   "metadata": {},
   "source": [
    "- We can now see the most important columns which are desirable for our project and therefore we will go with this columns. Since most of our columns are text-based columns and they are categorical, we will have to develop key statistics which we will set for our categorical columns so that we can proceed with our data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4b6ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.4 Key Statistics for Numeric/Categorical Columns\n",
    "print(\"CATEGORICAL COLUMNS ANALYSIS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "categorical_cols = ['Category', 'Seniority', 'Job Status', 'Job Language', 'Contract Types']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in Job_Posting_df.columns and Job_Posting_df[col].notnull().sum() > 0:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        # Count unique values\n",
    "        unique_count = Job_Posting_df[col].nunique()\n",
    "        non_null = Job_Posting_df[col].notnull().sum()\n",
    "        \n",
    "        print(f\"Non-null values: {non_null:,}/{len(Job_Posting_df):,} ({(non_null/len(Job_Posting_df))*100:.1f}%)\")\n",
    "        print(f\"Unique values: {unique_count}\")\n",
    "        \n",
    "        # Show top values\n",
    "        value_counts = Job_Posting_df[col].value_counts(dropna=False).head(10)\n",
    "        print(\"\\nTop 10 values:\")\n",
    "        for value, count in value_counts.items():\n",
    "            pct = (count / len(Job_Posting_df)) * 100\n",
    "            if pd.isna(value):\n",
    "                print(f\"  NaN: {count:5,d} ({pct:5.1f}%)\")\n",
    "            else:\n",
    "                # Truncate long values\n",
    "                display_value = str(value)[:50] + \"...\" if len(str(value)) > 50 else str(value)\n",
    "                print(f\"  {display_value:50}: {count:5,d} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ff703",
   "metadata": {},
   "source": [
    "- From this analysis, we can see that for the six categorical columns; i.e. , **Category**, **Seniority**, **Job Status**, **Job Language** and **Contract Types**, we have the various top values for each of these respective columns which shows us the Job Posting behaviour and nature at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab44ffdc",
   "metadata": {},
   "source": [
    "- Our dataset also happens to contain some columns which contains data in JSON format; i.e., ***Location Data*** and ***Salary Data***, hence the need to import the ***json*** library. Let's do a preview of the JSON columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca5ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 JSON Columns Preview\n",
    "\n",
    "print(\"JSON COLUMNS ANALYSIS\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "json_columns = ['Location Data', 'Salary Data']\n",
    "\n",
    "for json_col in json_columns:\n",
    "    if json_col in Job_Posting_df.columns:\n",
    "        print(f\"\\n{json_col}:\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        non_null_count = Job_Posting_df[json_col].notnull().sum()\n",
    "        print(f\"Non-null values: {non_null_count:,}/{len(Job_Posting_df):,} ({(non_null_count/len(Job_Posting_df))*100:.1f}%)\")\n",
    "        \n",
    "        # Sample and parse JSON\n",
    "        samples = Job_Posting_df[json_col].dropna().head(3)\n",
    "        if len(samples) > 0:\n",
    "            print(\"\\nSample JSON structures:\")\n",
    "            for i, sample in enumerate(samples, 1):\n",
    "                try:\n",
    "                    if isinstance(sample, str) and sample.strip():\n",
    "                        parsed = json.loads(sample)\n",
    "                        print(f\"\\nSample {i}:\")\n",
    "                        if isinstance(parsed, list):\n",
    "                            print(f\"  Type: List with {len(parsed)} items\")\n",
    "                            if parsed and isinstance(parsed[0], dict):\n",
    "                                print(f\"  Keys in first item: {list(parsed[0].keys())}\")\n",
    "                        elif isinstance(parsed, dict):\n",
    "                            print(f\"  Type: Dictionary\")\n",
    "                            print(f\"  Keys: {list(parsed.keys())}\")\n",
    "                            # Show first few key-value pairs\n",
    "                            for key, value in list(parsed.items())[:3]:\n",
    "                                print(f\"    {key}: {str(value)[:50]}{'...' if len(str(value)) > 50 else ''}\")\n",
    "                    else:\n",
    "                        print(f\"Sample {i}: Empty or non-string value\")\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Sample {i}: Invalid JSON - {str(e)[:50]}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Sample {i}: Error - {type(e).__name__}: {str(e)[:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9158d4",
   "metadata": {},
   "source": [
    "- The piece of code above was to identify the JSON columns so that we identify the various values and their categorical importance to the project and also identify the need to parse the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70b0f5",
   "metadata": {},
   "source": [
    "- Now lets check through the text columns and the date columns;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ac2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6 Text Columns Preview\n",
    "\n",
    "print(\"TEXT COLUMNS PREVIEW\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "text_columns = ['Job Opening Title', 'Description']\n",
    "\n",
    "for col in text_columns:\n",
    "    if col in Job_Posting_df.columns:\n",
    "        print(f\"\\n {col}:\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        non_null = Job_Posting_df[col].notnull().sum()\n",
    "        print(f\"Non-null: {non_null:,}/{len(Job_Posting_df):,} ({(non_null/len(Job_Posting_df))*100:.1f}%)\")\n",
    "        \n",
    "        # Show character statistics\n",
    "        if non_null > 0:\n",
    "            text_lengths = Job_Posting_df[col].dropna().apply(len)\n",
    "            print(f\"Average length: {text_lengths.mean():.0f} characters\")\n",
    "            print(f\"Min length: {text_lengths.min():.0f} characters\")\n",
    "            print(f\"Max length: {text_lengths.max():.0f} characters\")\n",
    "            \n",
    "            print(\"\\nSample entries:\")\n",
    "            samples = Job_Posting_df[col].dropna().head(3)\n",
    "            for i, sample in enumerate(samples, 1):\n",
    "                # Clean and truncate for display\n",
    "                clean_sample = str(sample).replace('\\n', ' ').replace('\\r', ' ')\n",
    "                if len(clean_sample) > 150:\n",
    "                    display_text = clean_sample[:150] + \"...\"\n",
    "                else:\n",
    "                    display_text = clean_sample\n",
    "                print(f\"\\n{i}. {display_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98821aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.7 Date Columns Analysis\n",
    "\n",
    "print(\"DATE COLUMNS ANALYSIS\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "date_columns = ['First Seen At', 'Last Seen At', 'Job Last Processed At']\n",
    "\n",
    "for col in date_columns:\n",
    "    if col in Job_Posting_df.columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        # Check if already datetime\n",
    "        if Job_Posting_df[col].dtype == 'object':\n",
    "            # Try to convert\n",
    "            try:\n",
    "                temp_dates = pd.to_datetime(Job_Posting_df[col], errors='coerce')\n",
    "                valid_dates = temp_dates.notnull().sum()\n",
    "                print(f\"Format appears to be: ISO 8601 (e.g., 2024-05-29T19:59:45Z)\")\n",
    "                print(f\"Valid dates: {valid_dates:,}/{len(Job_Posting_df):,} ({(valid_dates/len(Job_Posting_df))*100:.1f}%)\")\n",
    "                \n",
    "                if valid_dates > 0:\n",
    "                    print(f\"Date range: {temp_dates.min()} to {temp_dates.max()}\")\n",
    "                    duration_days = (temp_dates.max() - temp_dates.min()).days\n",
    "                    print(f\"Time span: {duration_days} days\")\n",
    "            except Exception as e:\n",
    "                print(f\"Conversion error: {str(e)[:50]}\")\n",
    "        else:\n",
    "            print(f\"Already datetime type\")\n",
    "            print(f\"Date range: {Job_Posting_df[col].min()} to {Job_Posting_df[col].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407aed9f",
   "metadata": {},
   "source": [
    "- The code above show that the date and time columns for our dataset are good to go so we can now do a complete summary of the data quality of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53f1edb",
   "metadata": {},
   "source": [
    "## 2.8 Data Quality Issues Summary\n",
    "\n",
    "### Identified Issues\n",
    "\n",
    "| # | Column/Issue | Details |\n",
    "|---|--------------|---------|\n",
    "| 1 | Ticker column | 100% missing - consider dropping |\n",
    "| 2 | Category | 100.0% missing |\n",
    "| 3 | Salary Data | Requires JSON parsing for structured salary info |\n",
    "| 4 | Location Data | Requires JSON parsing for detailed location info |\n",
    "\n",
    "**Notes:**\n",
    "- The Ticker column is completely empty and should be considered for removal\n",
    "- Category information is entirely missing, which may limit job classification analysis\n",
    "- Both Salary and Location data are stored in JSON format and require parsing to extract structured information\n",
    "- Additional data quality checks may be needed after JSON parsing to assess completeness of nested fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76144254",
   "metadata": {},
   "source": [
    "## 2.9 Recommendations for Next Steps\n",
    "\n",
    "### Data Cleaning Priority\n",
    "\n",
    "| Priority | Action | Details |\n",
    "|:--------:|--------|---------|\n",
    "| **1** | Drop completely empty columns | Ticker column (0 non-null values) |\n",
    "| **2** | Parse JSON columns | Extract city, state, country from Location Data; salary details from Salary Data |\n",
    "| **3** | Convert date columns | Convert First Seen At, Last Seen At to datetime format |\n",
    "| **4** | Handle missing Category data | Consider imputation or separate 'unknown' category |\n",
    "| **5** | Analyze text columns | Extract skills from Description using NLP |\n",
    "| **6** | Clean categorical columns | Standardize values in Category, Seniority, Contract Types |\n",
    "| **7** | Calculate posting duration | Create new feature: Last Seen At - First Seen At |\n",
    "| **8** | Explore Salary Data | Extract and analyze available salary information |\n",
    "\n",
    "---\n",
    "\n",
    "### Project Status Update\n",
    "\n",
    "| Status | Metric |\n",
    "|--------|--------|\n",
    "| Okay | Dataset loaded successfully: **45,000+** job postings |\n",
    "| Okay | Critical columns identified and assessed |\n",
    "| Okay | Data quality issues documented |\n",
    "| Okay | Next steps outlined for cleaning and preparation |\n",
    "\n",
    "**Ready for Step 3: Data Cleaning and Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bba2fd7",
   "metadata": {},
   "source": [
    "- Since we have done a thorough EDA we can now proceed to **Data Cleaning and Preparation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd31b25",
   "metadata": {},
   "source": [
    " # 3. Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac344e",
   "metadata": {},
   "source": [
    "From our observaions, we noted that there were issues we needed to tackle so as to get the data ready for modelling. We decided to tackle the issues in this order;\n",
    "- Drop completely empty columns\n",
    "\n",
    "- Parse JSON columns (Location and Salary Data)\n",
    "\n",
    "- Handle missing values\n",
    "\n",
    "- Convert date columns\n",
    "\n",
    "- Clean categorical/text data\n",
    "\n",
    "- Create new features for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece6634f",
   "metadata": {},
   "source": [
    "## 3.1 Initial Setup and Column removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e7bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Make a copy for cleaning\n",
    "Job_Posting_clean = Job_Posting_df.copy()\n",
    "print(\"Initial shape:\", Job_Posting_clean.shape)\n",
    "\n",
    "print(\"3.1 DROP COMPLETELY EMPTY COLUMNS\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Drop Ticker column (100% missing)\n",
    "if 'Ticker' in Job_Posting_clean.columns:\n",
    "    Job_Posting_clean = Job_Posting_clean.drop(columns=['Ticker'])\n",
    "    print(\"Dropped 'Ticker' column (100% missing)\")\n",
    "\n",
    "print(f\"New shape: {Job_Posting_clean.shape}\")\n",
    "print(f\"Columns remaining: {len(Job_Posting_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c74d7d1",
   "metadata": {},
   "source": [
    " ## 3.2 Parsing JSON columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5262f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"3.2.1 PARSE LOCATION DATA COLUMN\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "def parse_location_data(json_str):\n",
    "    \"\"\"Parse Location Data JSON and extract key fields\"\"\"\n",
    "    try:\n",
    "        if pd.isna(json_str) or json_str == '':\n",
    "            return None, None, None, None, None\n",
    "        \n",
    "        data = json.loads(json_str)\n",
    "        if isinstance(data, list) and len(data) > 0:\n",
    "            location = data[0]\n",
    "            return (\n",
    "                location.get('city'),\n",
    "                location.get('state'),\n",
    "                location.get('country'),\n",
    "                location.get('region'),\n",
    "                location.get('continent')\n",
    "            )\n",
    "    except (json.JSONDecodeError, TypeError, KeyError) as e:\n",
    "        pass\n",
    "    return None, None, None, None, None\n",
    "\n",
    "# Apply parsing\n",
    "location_parsed = Job_Posting_clean['Location Data'].apply(parse_location_data)\n",
    "Job_Posting_clean[['city', 'state', 'country', 'region', 'continent']] = pd.DataFrame(\n",
    "    location_parsed.tolist(), index=Job_Posting_clean.index\n",
    ")\n",
    "\n",
    "print(\"Extracted location fields from Location Data:\")\n",
    "print(f\"   - city: {Job_Posting_clean['city'].notnull().sum():,} non-null\")\n",
    "print(f\"   - state: {Job_Posting_clean['state'].notnull().sum():,} non-null\")\n",
    "print(f\"   - country: {Job_Posting_clean['country'].notnull().sum():,} non-null\")\n",
    "print(f\"   - region: {Job_Posting_clean['region'].notnull().sum():,} non-null\")\n",
    "print(f\"   - continent: {Job_Posting_clean['continent'].notnull().sum():,} non-null\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample extracted location data:\")\n",
    "sample_idx = Job_Posting_clean[Job_Posting_clean['country'].notnull()].index[0]\n",
    "print(f\"Original Location: {Job_Posting_clean.loc[sample_idx, 'Location']}\")\n",
    "print(f\"Parsed - City: {Job_Posting_clean.loc[sample_idx, 'city']}\")\n",
    "print(f\"Parsed - State: {Job_Posting_clean.loc[sample_idx, 'state']}\")\n",
    "print(f\"Parsed - Country: {Job_Posting_clean.loc[sample_idx, 'country']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed3640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.2.2 PARSE SALARY DATA COLUMN\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "def parse_salary_data(json_str):\n",
    "    \"\"\"Parse Salary Data JSON and extract key fields\"\"\"\n",
    "    try:\n",
    "        if pd.isna(json_str) or json_str == '':\n",
    "            return None, None, None, None, None, None\n",
    "        \n",
    "        data = json.loads(json_str)\n",
    "        return (\n",
    "            data.get('salary_low'),\n",
    "            data.get('salary_high'),\n",
    "            data.get('salary_currency'),\n",
    "            data.get('salary_low_usd'),\n",
    "            data.get('salary_high_usd'),\n",
    "            data.get('salary_time_unit')\n",
    "        )\n",
    "    except (json.JSONDecodeError, TypeError, KeyError) as e:\n",
    "        pass\n",
    "    return None, None, None, None, None, None\n",
    "\n",
    "# Apply parsing\n",
    "salary_parsed = Job_Posting_clean['Salary Data'].apply(parse_salary_data)\n",
    "Job_Posting_clean[['salary_low', 'salary_high', 'salary_currency', \n",
    "          'salary_low_usd', 'salary_high_usd', 'salary_time_unit']] = pd.DataFrame(\n",
    "    salary_parsed.tolist(), index=Job_Posting_clean.index\n",
    ")\n",
    "\n",
    "print(\"Extracted salary fields from Salary Data:\")\n",
    "salary_fields = ['salary_low', 'salary_high', 'salary_currency', \n",
    "                 'salary_low_usd', 'salary_high_usd', 'salary_time_unit']\n",
    "for field in salary_fields:\n",
    "    non_null = Job_Posting_clean[field].notnull().sum()\n",
    "    print(f\"   - {field:20}: {non_null:6,} non-null ({non_null/len(Job_Posting_clean)*100:.1f}%)\")\n",
    "\n",
    "# Check if we have any actual salary data\n",
    "has_salary_data = Job_Posting_clean['salary_low'].notnull().sum() > 0\n",
    "print(f\"\\nSalary data availability: {'Yes' if has_salary_data else 'No actual salary values found'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea13adba",
   "metadata": {},
   "source": [
    " ## 3.3 Converting Date Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3174056",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.3 CONVERT DATE COLUMNS\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "date_columns = ['First Seen At', 'Last Seen At', 'Job Last Processed At']\n",
    "\n",
    "for col in date_columns:\n",
    "    if col in Job_Posting_clean.columns:\n",
    "        Job_Posting_clean[col] = pd.to_datetime(Job_Posting_clean[col], errors='coerce', utc=True)\n",
    "        valid_dates = Job_Posting_clean[col].notnull().sum()\n",
    "        print(f\"Converted {col:25}: {valid_dates:,} valid dates\")\n",
    "        \n",
    "        # Show date range\n",
    "        if valid_dates > 0:\n",
    "            min_date = Job_Posting_clean[col].min()\n",
    "            max_date = Job_Posting_clean[col].max()\n",
    "            print(f\"   Range: {min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Create new feature: Job posting duration (in days)\n",
    "if 'First Seen At' in Job_Posting_clean.columns and 'Last Seen At' in Job_Posting_clean.columns:\n",
    "   Job_Posting_clean['posting_duration_days'] = (Job_Posting_clean['Last Seen At'] - Job_Posting_clean['First Seen At']).dt.days\n",
    "   print(f\"\\nCreated new feature: posting_duration_days\")\n",
    "   print(f\"   Average duration: {Job_Posting_clean['posting_duration_days'].mean():.1f} days\")\n",
    "   print(f\"   Min duration: {Job_Posting_clean['posting_duration_days'].min():.1f} days\")\n",
    "   print(f\"   Max duration: {Job_Posting_clean['posting_duration_days'].max():.1f} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d974979",
   "metadata": {},
   "source": [
    "## 3.4 Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac9e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.4 HANDLE MISSING VALUES\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Track missing values before handling\n",
    "missing_before = Job_Posting_clean.isnull().sum().sort_values(ascending=False)\n",
    "print(\"Missing values before handling (top 10):\")\n",
    "print(missing_before.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e69caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MISSING VALUE HANDLING STRATEGY\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Strategy for each column\n",
    "missing_strategies = {\n",
    "    'Category': \"Fill with 'unknown' category\",\n",
    "    'Job Status': \"Fill with 'unknown' status\",\n",
    "    'Keywords': \"Fill with empty string\",\n",
    "    'Contract Types': \"Fill with 'not_specified'\",\n",
    "    'Location': \"Keep as is (95.9% complete), fill with 'Unknown'\",\n",
    "    'Description': \"Drop rows (only 112 missing)\",\n",
    "    'city': \"Keep parsed values (some will be null)\",\n",
    "    'state': \"Keep parsed values\",\n",
    "    'country': \"Keep parsed values\",\n",
    "    'salary_low': \"Keep as is (salary data is sparse)\"\n",
    "}\n",
    "\n",
    "print(\"\\nHandling strategy for key columns:\")\n",
    "print(\"-\"*50)\n",
    "for col, strategy in missing_strategies.items():\n",
    "    if col in Job_Posting_clean.columns:\n",
    "        missing = Job_Posting_clean[col].isnull().sum()\n",
    "        pct = (missing / len(Job_Posting_clean)) * 100\n",
    "        print(f\"{col:20} | {missing:5,} missing ({pct:5.1f}%) → {strategy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc80b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply missing value handling\n",
    "print(\"APPLYING MISSING VALUE HANDLING\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Fill categorical columns\n",
    "Job_Posting_clean['Category'] = Job_Posting_clean['Category'].fillna('unknown')\n",
    "Job_Posting_clean['Job Status'] = Job_Posting_clean['Job Status'].fillna('unknown')\n",
    "Job_Posting_clean['Keywords'] = Job_Posting_clean['Keywords'].fillna('')\n",
    "Job_Posting_clean['Contract Types'] = Job_Posting_clean['Contract Types'].fillna('not_specified')\n",
    "Job_Posting_clean['Location'] = Job_Posting_clean['Location'].fillna('Unknown')\n",
    "\n",
    "# For Description, we have very few missing, so we can drop\n",
    "rows_before = len(Job_Posting_clean)\n",
    "Job_Posting_clean = Job_Posting_clean.dropna(subset=['Description'])\n",
    "rows_after = len(Job_Posting_clean)\n",
    "print(f\"Dropped {rows_before - rows_after} rows with missing Description\")\n",
    "\n",
    "print(\"\\nMissing values after handling (top 10):\")\n",
    "missing_after = Job_Posting_clean.isnull().sum().sort_values(ascending=False)\n",
    "print(missing_after.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b68b2",
   "metadata": {},
   "source": [
    " ## 3.5 Standardize Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6f8d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.5 CLEAN CATEGORICAL COLUMNS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Clean Category column - split multiple categories\n",
    "print(\"Cleaning 'Category' column...\")\n",
    "Job_Posting_clean['Category_list'] = Job_Posting_clean['Category'].apply(\n",
    "    lambda x: [cat.strip() for cat in str(x).split(',')] if pd.notnull(x) else []\n",
    ")\n",
    "\n",
    "# Create indicator for single vs multiple categories\n",
    "Job_Posting_clean['has_multiple_categories'] = Job_Posting_clean['Category_list'].apply(lambda x: len(x) > 1)\n",
    "\n",
    "print(f\"Created Category_list and has_multiple_categories features\")\n",
    "print(f\"   Jobs with multiple categories: {Job_Posting_clean['has_multiple_categories'].sum():,} ({Job_Posting_clean['has_multiple_categories'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e989e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Seniority column\n",
    "print(\"\\nCleaning 'Seniority' column...\")\n",
    "seniority_mapping = {\n",
    "    'non_manager': 'individual_contributor',\n",
    "    'manager': 'manager',\n",
    "    'head': 'director_level',\n",
    "    'director': 'director_level',\n",
    "    'c_level': 'executive',\n",
    "    'vice_president': 'executive',\n",
    "    'partner': 'executive',\n",
    "    'president': 'executive'\n",
    "}\n",
    "\n",
    "Job_Posting_clean['Seniority_clean'] = Job_Posting_clean['Seniority'].map(seniority_mapping)\n",
    "Job_Posting_clean['Seniority_clean'] = Job_Posting_clean['Seniority_clean'].fillna('other')\n",
    "\n",
    "print(\"Standardized Seniority levels:\")\n",
    "print(Job_Posting_clean['Seniority_clean'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4136db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Contract Types\n",
    "print(\"\\nCleaning 'Contract Types' column...\")\n",
    "\n",
    "# Extract primary contract type (first one if multiple)\n",
    "def extract_primary_contract(contract_str):\n",
    "    if pd.isna(contract_str) or contract_str == 'not_specified':\n",
    "        return 'not_specified'\n",
    "    \n",
    "    # Split by comma and take first\n",
    "    contracts = str(contract_str).split(',')\n",
    "    primary = contracts[0].strip().lower()\n",
    "    \n",
    "    # Map to standard terms\n",
    "    contract_mapping = {\n",
    "        'full time': 'full_time',\n",
    "        'part time': 'part_time',\n",
    "        'intern': 'internship',\n",
    "        'vollzeit': 'full_time',  # German\n",
    "        'tempo integral': 'full_time',  # Portuguese\n",
    "        'm/f': 'full_time',  # Probably means full-time\n",
    "        'm/w': 'full_time',  # Probably means full-time\n",
    "        'hybrid': 'hybrid'\n",
    "    }\n",
    "    \n",
    "    return contract_mapping.get(primary, primary)\n",
    "\n",
    "Job_Posting_clean['Contract_Type_primary'] = Job_Posting_clean['Contract Types'].apply(extract_primary_contract)\n",
    "\n",
    "print(\"Primary contract types:\")\n",
    "print(Job_Posting_clean['Contract_Type_primary'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0714311",
   "metadata": {},
   "source": [
    " ## 3.6 Cleaning Text Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e23e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.6 CLEAN TEXT COLUMNS\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# Clean Job Opening Title\n",
    "print(\"Cleaning 'Job Opening Title'...\")\n",
    "\n",
    "# Remove extra whitespace and standardize case\n",
    "Job_Posting_clean['Title_clean'] = Job_Posting_clean['Job Opening Title'].str.strip().str.lower()\n",
    "\n",
    "# Extract potential indicators from title\n",
    "Job_Posting_clean['title_has_senior'] = Job_Posting_clean['Title_clean'].str.contains('senior', case=False)\n",
    "Job_Posting_clean['title_has_junior'] = Job_Posting_clean['Title_clean'].str.contains('junior', case=False)\n",
    "Job_Posting_clean['title_has_manager'] = Job_Posting_clean['Title_clean'].str.contains('manager', case=False)\n",
    "Job_Posting_clean['title_has_engineer'] = Job_Posting_clean['Title_clean'].str.contains('engineer', case=False)\n",
    "Job_Posting_clean['title_has_developer'] = Job_Posting_clean['Title_clean'].str.contains('developer', case=False)\n",
    "Job_Posting_clean['title_has_analyst'] = Job_Posting_clean['Title_clean'].str.contains('analyst', case=False)\n",
    "\n",
    "print(\"Title indicators extracted:\")\n",
    "indicators = ['title_has_senior', 'title_has_junior', 'title_has_manager', \n",
    "              'title_has_engineer', 'title_has_developer', 'title_has_analyst']\n",
    "for indicator in indicators:\n",
    "    count = Job_Posting_clean[indicator].sum()\n",
    "    print(f\"   - {indicator:20}: {count:6,} ({count/len(Job_Posting_clean)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9629541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Description cleaning\n",
    "print(\"\\nInitial cleaning of 'Description'...\")\n",
    "\n",
    "# Store original length\n",
    "Job_Posting_clean['Description_length'] = Job_Posting_clean['Description'].str.len()\n",
    "\n",
    "# Basic cleaning: remove extra whitespace\n",
    "Job_Posting_clean['Description_clean'] = Job_Posting_clean['Description'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "print(f\"Description length statistics:\")\n",
    "print(f\"   Average: {Job_Posting_clean['Description_length'].mean():.0f} characters\")\n",
    "print(f\"   Min: {Job_Posting_clean['Description_length'].min():.0f} characters\")\n",
    "print(f\"   Max: {Job_Posting_clean['Description_length'].max():.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9ecae0",
   "metadata": {},
   "source": [
    "## 3.7 Minor Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"3.7 CREATE ADDITIONAL FEATURES\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# 1. Geographic features\n",
    "print(\"Creating geographic features...\")\n",
    "\n",
    "# Create country grouping\n",
    "def categorize_country(country):\n",
    "    if pd.isna(country):\n",
    "        return 'unknown'\n",
    "    \n",
    "    country = str(country).lower()\n",
    "    \n",
    "    # Major tech hubs\n",
    "    if country in ['united states', 'usa', 'us']:\n",
    "        return 'usa'\n",
    "    elif country in ['germany', 'deutschland']:\n",
    "        return 'germany'\n",
    "    elif country in ['india', 'in']:\n",
    "        return 'india'\n",
    "    elif country in ['china', 'cn']:\n",
    "        return 'china'\n",
    "    elif country in ['united kingdom', 'uk', 'great britain']:\n",
    "        return 'uk'\n",
    "    elif country in ['canada', 'ca']:\n",
    "        return 'canada'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "Job_Posting_clean['country_group'] = Job_Posting_clean['country'].apply(categorize_country)\n",
    "print(f\"   Country groups: {Job_Posting_clean['country_group'].value_counts().to_dict()}\")\n",
    "\n",
    "# 2. Company domain features\n",
    "print(\"\\nCreating company features...\")\n",
    "\n",
    "# Extract company name from domain\n",
    "def extract_company(domain):\n",
    "    if pd.isna(domain):\n",
    "        return 'unknown'\n",
    "    \n",
    "    # Remove www. and .com/.org etc.\n",
    "    domain = str(domain).lower()\n",
    "    domain = domain.replace('www.', '').replace('https://', '').replace('http://', '')\n",
    "    \n",
    "    # Split by dots and take first part\n",
    "    parts = domain.split('.')\n",
    "    return parts[0] if parts else 'unknown'\n",
    "\n",
    "Job_Posting_clean['company_name'] = Job_Posting_clean['Website Domain'].apply(extract_company)\n",
    "\n",
    "# Count jobs per company\n",
    "company_counts = Job_Posting_clean['company_name'].value_counts()\n",
    "print(f\"   Top 5 companies by job count:\")\n",
    "for company, count in company_counts.head(5).items():\n",
    "    print(f\"      {company}: {count:,} jobs\")\n",
    "\n",
    "# 3. O*NET features\n",
    "print(\"\\nCreating O*NET features...\")\n",
    "\n",
    "# Check if O*NET code contains useful information\n",
    "if 'O*NET Code' in Job_Posting_clean.columns:\n",
    "    # Extract major group from O*NET code (first 2 digits)\n",
    "    Job_Posting_clean['ONET_major_group'] = Job_Posting_clean['O*NET Code'].str.split('-').str[0]\n",
    "    print(f\"   Created ONET_major_group feature\")\n",
    "    print(f\"   Unique groups: {Job_Posting_clean['ONET_major_group'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaafbda",
   "metadata": {},
   "source": [
    "## 3.8 Final Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320bcab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.8 FINAL DATA QUALITY CHECK\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"Dataset shape after cleaning: {Job_Posting_clean.shape}\")\n",
    "print(f\"Columns: {len(Job_Posting_clean.columns)}\")\n",
    "print(f\"Memory usage: {Job_Posting_clean.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21ee197",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CRITICAL COLUMNS - FINAL STATUS\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "critical_status = {\n",
    "    'Job Opening Title': 'Complete',\n",
    "    'Description': 'Complete (after dropping nulls)',\n",
    "    'Category': 'Complete (filled missing)',\n",
    "    'Location': 'Complete (filled missing)',\n",
    "    'Seniority': 'Complete',\n",
    "    'salary_low_usd': 'Sparse but parsed',\n",
    "    'Contract Types': 'Complete (filled missing)',\n",
    "    'Job Status': 'Complete (filled missing)',\n",
    "    'First Seen At': 'Complete (converted)',\n",
    "    'Last Seen At': 'Complete (converted)'\n",
    "}\n",
    "\n",
    "print(\"\\nColumn                  | Status\")\n",
    "print(\"-\"*50)\n",
    "for col, status in critical_status.items():\n",
    "    if col in Job_Posting_clean.columns:\n",
    "        non_null = Job_Posting_clean[col].notnull().sum()\n",
    "        pct = (non_null / len(Job_Posting_clean)) * 100\n",
    "        print(f\"{col:25} | {status:30} ({non_null:,}/{len(Job_Posting_clean):,} = {pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06b826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NEW FEATURES CREATED\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "new_features = [\n",
    "    'city', 'state', 'country', 'region', 'continent',\n",
    "    'salary_low', 'salary_high', 'salary_currency',\n",
    "    'salary_low_usd', 'salary_high_usd', 'salary_time_unit',\n",
    "    'posting_duration_days', 'Category_list', 'has_multiple_categories',\n",
    "    'Seniority_clean', 'Contract_Type_primary', 'Title_clean',\n",
    "    'title_has_senior', 'title_has_junior', 'title_has_manager',\n",
    "    'title_has_engineer', 'title_has_developer', 'title_has_analyst',\n",
    "    'Description_length', 'Description_clean', 'country_group',\n",
    "    'company_name', 'ONET_major_group'\n",
    "]\n",
    "\n",
    "print(f\"Total new features created: {len(new_features)}\")\n",
    "print(\"\\nFeature categories:\")\n",
    "print(\"  1. Location features (5)\")\n",
    "print(\"  2. Salary features (6)\")\n",
    "print(\"  3. Temporal features (1)\")\n",
    "print(\"  4. Category features (2)\")\n",
    "print(\"  5. Seniority/Contract features (2)\")\n",
    "print(\"  6. Title features (7)\")\n",
    "print(\"  7. Description features (2)\")\n",
    "print(\"  8. Geographic/Company features (3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6907c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SAMPLE OF CLEANED DATA\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"\\nFirst 3 rows of cleaned dataset:\")\n",
    "sample_cols = ['Job Opening Title', 'Category', 'Seniority_clean', \n",
    "               'country', 'company_name', 'posting_duration_days',\n",
    "               'title_has_engineer', 'Contract_Type_primary']\n",
    "\n",
    "print(Job_Posting_clean[sample_cols].head(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd2cf6b",
   "metadata": {},
   "source": [
    "## Data Cleaning Summary\n",
    "\n",
    "### Cleaning Process Completed\n",
    "\n",
    "| Status | Task | Details |\n",
    "|:------:|------|---------|\n",
    "| Done | Dropped completely empty columns | Ticker column removed |\n",
    "| Done | Parsed JSON columns | Extracted 11 new features from Location/Salary Data |\n",
    "| Done | Converted date columns | 3 date columns converted to datetime |\n",
    "| Done | Handled missing values | Critical columns filled, sparse data preserved |\n",
    "| Done | Cleaned categorical data | Standardized Seniority, Contract Types, Category |\n",
    "| Done | Cleaned text data | Title and Description cleaned, indicators extracted |\n",
    "| Done | Created new features | Multiple new features for analysis |\n",
    "| Final | Final dataset | **45,000+ rows × 28 columns** |\n",
    "\n",
    "---\n",
    "\n",
    "## Next Step: Exploratory Data Analysis\n",
    "\n",
    "The cleaned dataset is now ready for in-depth analysis. We can proceed with:\n",
    "\n",
    "1. **Geographic distribution analysis**\n",
    "   - City, state, and country breakdowns\n",
    "   - Remote vs. on-site job distribution\n",
    "\n",
    "2. **Job category trends**\n",
    "   - Most common job categories and seniority levels\n",
    "   - Contract type preferences by industry\n",
    "\n",
    "3. **Skill extraction from descriptions**\n",
    "   - NLP analysis of job requirements\n",
    "   - Most in-demand skills and qualifications\n",
    "\n",
    "4. **Salary analysis** (limited data)\n",
    "   - Salary ranges by job category and seniority\n",
    "   - Geographic salary variations\n",
    "\n",
    "5. **Time-based trends**\n",
    "   - Posting frequency over time\n",
    "   - Job posting duration patterns\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Insights from Cleaning Process**\n",
    " \n",
    " 1. **Data Structure Understanding**: The dataset contains rich, multi-dimensional information about job postings\n",
    " 2. **Salary Transparency Gap**: Only 4.4% of postings include salary data, confirming industry transparency issues\n",
    " 3. **Geographic Diversity**: Jobs span multiple continents with strong representation from tech hubs\n",
    " 4. **Category Complexity**: Many jobs have multiple categories, reflecting hybrid roles\n",
    " 5. **Temporal Patterns**: Job postings span approximately 6 months, enabling time-series analysis\n",
    "\n",
    "## **Limitations and Considerations**\n",
    " \n",
    " 1. **Salary Analysis Limitations**: Limited salary data may restrict compensation insights\n",
    " 2. **Language Diversity**: Job descriptions in multiple languages (English 72%, German 13%, etc.)\n",
    " 3. **Company Representation**: Some companies dominate the dataset (Bosch, ZF, etc.)\n",
    " 4. **Time Period**: Data covers approximately 6 months (March-September 2024)\n",
    "\n",
    "---\n",
    "**Ready to begin Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00205a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataframe for analysis\n",
    "Job_Posting_clean.to_csv('Job_Posting_cleaned.csv', index=False)\n",
    "print(\"Dataset saved for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca20108",
   "metadata": {},
   "source": [
    "# 4 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cd1ad3",
   "metadata": {},
   "source": [
    "- We have cleaned our data enough for us to proceed with Exploratory Data Analysis. First, let us have a preview of the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ce233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_Posting_Clean = pd.read_csv('Job_Posting_cleaned.csv')\n",
    "Job_Posting_Clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d7cfee2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Job_Posting_Clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Job_Posting_Clean\u001b[38;5;241m.\u001b[39minfo()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Job_Posting_Clean' is not defined"
     ]
    }
   ],
   "source": [
    "Job_Posting_Clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91484f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_Posting_Clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab97c5bd",
   "metadata": {},
   "source": [
    "- Now let us proceed to our data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e1464",
   "metadata": {},
   "source": [
    "## 4.1 Setup and Initial Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff6bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Load cleaned data \n",
    "try:\n",
    "    Job_df = Job_Posting_Clean.copy()\n",
    "    print(\"Using existing cleaned dataframe\")\n",
    "except:\n",
    "   Job_df = pd.read_csv('Job_Posting_cleaned.csv')\n",
    "   print(\"Loaded cleaned data from file\")\n",
    "\n",
    "print(f\"Dataset shape: {Job_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507afa5c",
   "metadata": {},
   "source": [
    "## 4.2 Analysis of Geographical Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95dde0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Top countries by job count\n",
    "country_counts = Job_df['country'].value_counts().head(15)\n",
    "print(f\"\\nTop 15 Countries by Job Count:\")\n",
    "print(\"-\"*50)\n",
    "for country, count in country_counts.items():\n",
    "    pct = (count / len(Job_df)) * 100\n",
    "    print(f\"{country:30}: {count:5,d} jobs ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e58cbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize country distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart - Top 10 countries\n",
    "top_countries = Job_df['country'].value_counts().head(10)\n",
    "bars = ax1.barh(range(len(top_countries)), top_countries.values)\n",
    "ax1.set_yticks(range(len(top_countries)))\n",
    "ax1.set_yticklabels(top_countries.index)\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_xlabel('Number of Job Postings')\n",
    "ax1.set_title('Top 10 Countries by Job Count', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, count) in enumerate(zip(bars, top_countries.values)):\n",
    "    ax1.text(count + 20, bar.get_y() + bar.get_height()/2, \n",
    "             f'{count:,}', va='center', fontsize=10)\n",
    "\n",
    "# Pie chart - Continent distribution\n",
    "if 'continent' in Job_df.columns:\n",
    "    continent_counts = Job_df['continent'].value_counts()\n",
    "    ax2.pie(continent_counts.values, labels=continent_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    ax2.set_title('Job Distribution by Continent', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ca7b1c",
   "metadata": {},
   "source": [
    "- From this, we can see that the majority of the job postings are in the United States of America. We can do an in-depth analysis of the jobs in the USA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3eaf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"4.2.1 United States State-Level Analysis\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Filter for US jobs\n",
    "us_jobs = Job_df[Job_df['country'].str.contains('United States|USA|US', case=False, na=False)]\n",
    "\n",
    "if len(us_jobs) > 0:\n",
    "    # Count by state\n",
    "    state_counts = us_jobs['state'].value_counts().head(15)\n",
    "    \n",
    "    print(f\"\\nTop 15 US States by Job Count:\")\n",
    "    print(\"-\"*50)\n",
    "    for state, count in state_counts.items():\n",
    "        pct = (count / len(us_jobs)) * 100\n",
    "        print(f\"{state:25}: {count:5,d} jobs ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    bars = plt.barh(range(len(state_counts)), state_counts.values)\n",
    "    plt.yticks(range(len(state_counts)), state_counts.index)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('Number of Job Postings')\n",
    "    plt.title('Top 15 US States by Job Count', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, count) in enumerate(zip(bars, state_counts.values)):\n",
    "        plt.text(count + 5, bar.get_y() + bar.get_height()/2, \n",
    "                 f'{count:,}', va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No US jobs found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b1e6e",
   "metadata": {},
   "source": [
    "## 4.3 Job Category Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a317ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analyze categories (single vs multiple)\n",
    "single_cat_jobs = Job_df[~Job_df['has_multiple_categories']]\n",
    "multi_cat_jobs = Job_df[Job_df['has_multiple_categories']]\n",
    "\n",
    "print(f\"\\n Category Composition:\")\n",
    "print(\"-\"*50)\n",
    "print(f\"   Single-category jobs: {len(single_cat_jobs):,} ({len(single_cat_jobs)/len(Job_df)*100:.1f}%)\")\n",
    "print(f\"   Multi-category jobs:  {len(multi_cat_jobs):,} ({len(multi_cat_jobs)/len(Job_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ec3af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all individual categories from Category_list\n",
    "all_categories = []\n",
    "for categories in Job_df['Category_list'].dropna():\n",
    "    all_categories.extend(categories)\n",
    "\n",
    "category_counts = pd.Series(all_categories).value_counts().head(20)\n",
    "\n",
    "print(f\"\\n Top 20 Job Categories:\")\n",
    "print(\"-\"*60)\n",
    "for category, count in category_counts.items():\n",
    "    pct = (count / len(all_categories)) * 100\n",
    "    print(f\"{category:40}: {count:5,d} mentions ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b16d9a",
   "metadata": {},
   "source": [
    "- For now, the naming of the categories does not make sense since they have been named using placeholder text values. This is an issue we will address in the feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4780a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top categories\n",
    "plt.figure(figsize=(14, 8))\n",
    "bars = plt.barh(range(len(category_counts)), category_counts.values)\n",
    "plt.yticks(range(len(category_counts)), category_counts.index)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Number of Mentions')\n",
    "plt.title('Top 20 Most Common Job Categories', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, count) in enumerate(zip(bars, category_counts.values)):\n",
    "    plt.text(count + 5, bar.get_y() + bar.get_height()/2, \n",
    "             f'{count:,}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f49c9db",
   "metadata": {},
   "source": [
    "## 4.4 Senioriy and Experience Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n Seniority Distribution:\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Seniority distribution\n",
    "seniority_counts = Job_df['Seniority_clean'].value_counts()\n",
    "\n",
    "for level, count in seniority_counts.items():\n",
    "    pct = (count / len(Job_df)) * 100\n",
    "    print(f\"{level:25}: {count:5,d} jobs ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize seniority distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart\n",
    "bars = ax1.bar(range(len(seniority_counts)), seniority_counts.values)\n",
    "ax1.set_xticks(range(len(seniority_counts)))\n",
    "ax1.set_xticklabels(seniority_counts.index, rotation=45, ha='right')\n",
    "ax1.set_ylabel('Number of Jobs')\n",
    "ax1.set_title('Job Distribution by Seniority Level', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add value labels\n",
    "for bar, count in zip(bars, seniority_counts.values):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 20,\n",
    "             f'{count:,}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(seniority_counts.values, labels=seniority_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Seniority Level Proportions', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ee139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seniority by country (for top countries)\n",
    "# Seniority by country (for top countries)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"4.4.1 Seniority Distribution by Country (Top 5)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "top_countries_list = Job_df['country'].value_counts().head(5).index.tolist()\n",
    "seniority_by_country = pd.crosstab(Job_df['country'], Job_df['Seniority_clean'])\n",
    "\n",
    "# Filter for top countries\n",
    "seniority_top_countries = seniority_by_country.loc[top_countries_list]\n",
    "\n",
    "print(\" Seniority distribution across top countries:\")\n",
    "print(seniority_top_countries)\n",
    "\n",
    "# Heatmap visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(seniority_top_countries, annot=True, fmt='d', cmap='YlOrRd', cbar_kws={'label': 'Number of Jobs'})\n",
    "plt.title('Seniority Distribution Across Top 5 Countries', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Seniority Level')\n",
    "plt.ylabel('Country')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1282c92",
   "metadata": {},
   "source": [
    "## 4.5 Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc9cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"4.5 TEMPORAL ANALYSIS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Check if we have date columns\n",
    "if 'First Seen At' in Job_df.columns:\n",
    "    print(f\" Date Column Info:\")\n",
    "    print(f\"   Column type: {Job_df['First Seen At'].dtype}\")\n",
    "    print(f\"   Sample values: {Job_df['First Seen At'].iloc[0]}, {Job_df['First Seen At'].iloc[1]}\")\n",
    "    \n",
    "    # Check if datetime conversion worked\n",
    "    if pd.api.types.is_datetime64_any_dtype(Job_df['First Seen At']):\n",
    "        print(\" Date column is in datetime format\")\n",
    "        \n",
    "        # Get time period\n",
    "        min_date = Job_df['First Seen At'].min()\n",
    "        max_date = Job_df['First Seen At'].max()\n",
    "        \n",
    "        print(f\" Time Period Covered: {min_date.date()} to {max_date.date()}\")\n",
    "        print(f\" Total days: {(max_date - min_date).days} days\")\n",
    "        \n",
    "        # Create month-year column using string formatting instead of period\n",
    "        Job_df['first_seen_month'] = Job_df['First Seen At'].dt.strftime('%Y-%m')\n",
    "        #Job_df['last_seen_month'] = Job_df['Last Seen At'].dt.strftime('%Y-%m')\n",
    "        \n",
    "        # Monthly posting trends\n",
    "        monthly_postings = Job_df['first_seen_month'].value_counts().sort_index()\n",
    "        \n",
    "        print(f\" Monthly Job Posting Trends:\")\n",
    "        print(\"-\"*60)\n",
    "        for month, count in monthly_postings.items():\n",
    "            print(f\"{month}: {count:5,d} postings\")\n",
    "        \n",
    "        # Visualize time trends\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "        \n",
    "        # Monthly postings line chart\n",
    "        months = monthly_postings.index\n",
    "        ax1.plot(range(len(months)), monthly_postings.values, marker='o', linewidth=2, markersize=8)\n",
    "        ax1.set_title('Monthly Job Posting Trends', fontsize=14, fontweight='bold')\n",
    "        ax1.set_ylabel('Number of Postings')\n",
    "        ax1.set_xlabel('Month')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_xticks(range(len(months)))\n",
    "        ax1.set_xticklabels(months, rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (month, count) in enumerate(zip(months, monthly_postings.values)):\n",
    "            ax1.text(i, count + 20, f'{count:,}', ha='center', fontsize=9)\n",
    "        \n",
    "        # Posting duration analysis\n",
    "        if 'posting_duration_days' in Job_df.columns:\n",
    "            # Remove outliers for better visualization\n",
    "            duration_clean = Job_df[Job_df['posting_duration_days'] <= Job_df['posting_duration_days'].quantile(0.95)]['posting_duration_days']\n",
    "            \n",
    "            ax2.hist(duration_clean, bins=30, edgecolor='black', alpha=0.7)\n",
    "            ax2.set_title('Distribution of Job Posting Durations (Days)', fontsize=14, fontweight='bold')\n",
    "            ax2.set_xlabel('Posting Duration (Days)')\n",
    "            ax2.set_ylabel('Number of Jobs')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add statistics\n",
    "            mean_duration = duration_clean.mean()\n",
    "            median_duration = duration_clean.median()\n",
    "            ax2.axvline(mean_duration, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_duration:.1f} days')\n",
    "            ax2.axvline(median_duration, color='green', linestyle='--', linewidth=2, label=f'Median: {median_duration:.1f} days')\n",
    "            ax2.legend()\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, \"'posting_duration_days' column not found\", \n",
    "                     ha='center', va='center', transform=ax2.transAxes)\n",
    "            ax2.set_title('Posting Duration Data Unavailable', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Additional temporal analysis\n",
    "        print(f\" Daily Posting Statistics:\")\n",
    "        print(\"-\"*60)\n",
    "        daily_postings = Job_df['First Seen At'].dt.date.value_counts().sort_index()\n",
    "        print(f\"   Average daily postings: {daily_postings.mean():.1f}\")\n",
    "        print(f\"   Busiest day: {daily_postings.idxmax()} with {daily_postings.max():,} postings\")\n",
    "        print(f\"   Slowest day: {daily_postings.idxmin()} with {daily_postings.min():,} postings\")\n",
    "        \n",
    "        # Day of week analysis\n",
    "        Job_df['day_of_week'] = Job_df['First Seen At'].dt.day_name()\n",
    "        day_counts = Job_df['day_of_week'].value_counts()\n",
    "        \n",
    "        print(f\" Postings by Day of Week:\")\n",
    "        print(\"-\"*60)\n",
    "        for day, count in day_counts.items():\n",
    "            pct = (count / len(Job_df)) * 100\n",
    "            print(f\"   {day:15}: {count:5,d} ({pct:5.1f}%)\")\n",
    "        \n",
    "        # Visualize day of week\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        day_counts_ordered = day_counts.reindex(day_order)\n",
    "        bars = plt.bar(range(len(day_counts_ordered)), day_counts_ordered.values)\n",
    "        plt.xticks(range(len(day_counts_ordered)), day_counts_ordered.index, rotation=45)\n",
    "        plt.title('Job Postings by Day of Week', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('Number of Postings')\n",
    "        plt.xlabel('Day of Week')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, count in zip(bars, day_counts_ordered.values):\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 20,\n",
    "                     f'{count:,}', ha='center', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(\" Date column is NOT in datetime format\")\n",
    "        print(f\"   Trying to convert again...\")\n",
    "        try:\n",
    "            Job_df['First Seen At'] = pd.to_datetime(Job_df['First Seen At'], errors='coerce')\n",
    "            print(f\"   Conversion successful: {Job_df['First Seen At'].dtype}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Conversion failed: {e}\")\n",
    "else:\n",
    "    print(\" 'First Seen At' column not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7a4a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"4.5 TEMPORAL ANALYSIS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Check if we have date columns\n",
    "if 'First Seen At' in Job_df.columns:\n",
    "    print(f\"\\n Date Column Info:\")\n",
    "    print(f\"   Column type: {Job_df['First Seen At'].dtype}\")\n",
    "    print(f\"   Sample values: {Job_df['First Seen At'].iloc[0]}, {Job_df['First Seen At'].iloc[1]}\")\n",
    "    \n",
    "    # Ensure it's datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(Job_df['First Seen At']):\n",
    "        print(\" Date column is NOT in datetime format\")\n",
    "        print(f\"   Trying to convert again...\")\n",
    "        try:\n",
    "            Job_df['First Seen At'] = pd.to_datetime(Job_df['First Seen At'], errors='coerce', utc=True)\n",
    "            Job_df['Last Seen At'] = pd.to_datetime(Job_df['Last Seen At'], errors='coerce', utc=True)\n",
    "            print(f\"   Conversion successful: {Job_df['First Seen At'].dtype}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Conversion failed: {e}\")\n",
    "            # Try alternative approach\n",
    "            Job_df['First Seen At'] = pd.to_datetime(Job_df['First Seen At'], errors='coerce')\n",
    "            Job_df['Last Seen At'] = pd.to_datetime(Job_df['Last Seen At'], errors='coerce')\n",
    "    \n",
    "    # Now proceed with analysis\n",
    "    print(\" Date column is in datetime format\")\n",
    "    \n",
    "    # Get time period\n",
    "    min_date = Job_df['First Seen At'].min()\n",
    "    max_date = Job_df['First Seen At'].max()\n",
    "    \n",
    "    print(f\"\\n Time Period Covered: {min_date.date()} to {max_date.date()}\")\n",
    "    print(f\" Total days: {(max_date - min_date).days} days\")\n",
    "    \n",
    "    # Create month-year column using string formatting\n",
    "    Job_df['first_seen_month'] = Job_df['First Seen At'].dt.strftime('%Y-%m')\n",
    "    \n",
    "    # Monthly posting trends\n",
    "    monthly_postings = Job_df['first_seen_month'].value_counts().sort_index()\n",
    "    \n",
    "    print(f\"\\n Monthly Job Posting Trends:\")\n",
    "    print(\"-\"*60)\n",
    "    for month, count in monthly_postings.items():\n",
    "        print(f\"{month}: {count:5,d} postings\")\n",
    "    \n",
    "    # Visualize time trends\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Monthly postings line chart\n",
    "    months = monthly_postings.index.tolist()\n",
    "    month_indices = range(len(months))\n",
    "    ax1.plot(month_indices, monthly_postings.values, marker='o', linewidth=2, markersize=8)\n",
    "    ax1.set_title('Monthly Job Posting Trends', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Number of Postings')\n",
    "    ax1.set_xlabel('Month')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xticks(month_indices)\n",
    "    ax1.set_xticklabels(months, rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, count in enumerate(monthly_postings.values):\n",
    "        ax1.text(i, count + 20, f'{count:,}', ha='center', fontsize=9)\n",
    "    \n",
    "    # Posting duration analysis\n",
    "    if 'posting_duration_days' in Job_df.columns:\n",
    "        # Calculate if not already done\n",
    "        if Job_df['posting_duration_days'].isnull().all():\n",
    "            Job_df['posting_duration_days'] = (Job_df['Last Seen At'] - Job_df['First Seen At']).dt.days\n",
    "        \n",
    "        # Remove outliers for better visualization\n",
    "        duration_clean = Job_df[Job_df['posting_duration_days'] <= Job_df['posting_duration_days'].quantile(0.95)]['posting_duration_days']\n",
    "        \n",
    "        ax2.hist(duration_clean, bins=30, edgecolor='black', alpha=0.7)\n",
    "        ax2.set_title('Distribution of Job Posting Durations (Days)', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Posting Duration (Days)')\n",
    "        ax2.set_ylabel('Number of Jobs')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics\n",
    "        mean_duration = duration_clean.mean()\n",
    "        median_duration = duration_clean.median()\n",
    "        ax2.axvline(mean_duration, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_duration:.1f} days')\n",
    "        ax2.axvline(median_duration, color='green', linestyle='--', linewidth=2, label=f'Median: {median_duration:.1f} days')\n",
    "        ax2.legend()\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, \"'posting_duration_days' column not found\", \n",
    "                 ha='center', va='center', transform=ax2.transAxes)\n",
    "        ax2.set_title('Posting Duration Data Unavailable', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional temporal analysis\n",
    "    print(f\"\\n Daily Posting Statistics:\")\n",
    "    print(\"-\"*60)\n",
    "    daily_postings = Job_df['First Seen At'].dt.date.value_counts().sort_index()\n",
    "    print(f\"   Average daily postings: {daily_postings.mean():.1f}\")\n",
    "    print(f\"   Busiest day: {daily_postings.idxmax()} with {daily_postings.max():,} postings\")\n",
    "    print(f\"   Slowest day: {daily_postings.idxmin()} with {daily_postings.min():,} postings\")\n",
    "    \n",
    "    # Day of week analysis\n",
    "    Job_df['day_of_week'] = Job_df['First Seen At'].dt.day_name()\n",
    "    day_counts = Job_df['day_of_week'].value_counts()\n",
    "    \n",
    "    print(f\"\\n Postings by Day of Week:\")\n",
    "    print(\"-\"*60)\n",
    "    for day, count in day_counts.items():\n",
    "        pct = (count / len(Job_df)) * 100\n",
    "        print(f\"   {day:15}: {count:5,d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Visualize day of week\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    day_counts_ordered = day_counts.reindex(day_order)\n",
    "    bars = plt.bar(range(len(day_counts_ordered)), day_counts_ordered.values)\n",
    "    plt.xticks(range(len(day_counts_ordered)), day_counts_ordered.index, rotation=45)\n",
    "    plt.title('Job Postings by Day of Week', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Number of Postings')\n",
    "    plt.xlabel('Day of Week')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars, day_counts_ordered.values):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 20,\n",
    "                 f'{count:,}', ha='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Month analysis - FIXED VERSION\n",
    "    print(f\"\\n Postings by Month:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Extract month names\n",
    "    Job_df['month'] = Job_df['First Seen At'].dt.month_name()\n",
    "    month_counts = Job_df['month'].value_counts()\n",
    "    \n",
    "    # Define month order\n",
    "    month_order = ['January', 'February', 'March', 'April', 'May', 'June', \n",
    "                   'July', 'August', 'September', 'October', 'November', 'December']\n",
    "    \n",
    "    # Reindex and drop NaN values\n",
    "    month_counts_ordered = month_counts.reindex(month_order)\n",
    "    \n",
    "    # Display month counts\n",
    "    for month in month_order:\n",
    "        if month in month_counts.index:\n",
    "            count = month_counts[month]\n",
    "            pct = (count / len(Job_df)) * 100\n",
    "            print(f\"   {month:15}: {int(count):5,d} ({pct:5.1f}%)\")\n",
    "        else:\n",
    "            print(f\"   {month:15}: {'0':>5} ({'0.0':>5}%)\")\n",
    "    \n",
    "else:\n",
    "    print(\" 'First Seen At' column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aa99b0",
   "metadata": {},
   "source": [
    "## 4.6 Company Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc66248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the Number of jobs available for each Company\n",
    "# Top companies by job count\n",
    "company_counts = Job_df['company_name'].value_counts().head(20)\n",
    "\n",
    "print(f\" Top 20 Companies by Job Postings:\")\n",
    "print(\"-\"*30)\n",
    "for company, count in company_counts.items():\n",
    "    pct = (count / len(Job_df)) * 100\n",
    "    print(f\"{company:30}: {count:5,d} jobs ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2779a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Company market share analysis\n",
    "top_10_companies = company_counts.head(10)\n",
    "other_companies = len(Job_df) - top_10_companies.sum()\n",
    "\n",
    "# Creating data for pie chart\n",
    "company_data = pd.concat([top_10_companies, pd.Series({'Other Companies': other_companies})])\n",
    "\n",
    "# Creating Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Bar chart\n",
    "bars = ax1.barh(range(len(top_10_companies)), top_10_companies.values)\n",
    "ax1.set_yticks(range(len(top_10_companies)))\n",
    "ax1.set_yticklabels(top_10_companies.index)\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_xlabel('Number of Job Postings')\n",
    "ax1.set_title('Top 10 Companies by Job Count', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Adding  value labels\n",
    "for i, (bar, count) in enumerate(zip(bars, top_10_companies.values)):\n",
    "    ax1.text(count + 5, bar.get_y() + bar.get_height()/2, \n",
    "             f'{count:,}', va='center', fontsize=10)\n",
    "\n",
    "# Pie chart -market share for the top 5 companies\n",
    "top_5_companies = company_counts.head(5)\n",
    "other_all = len(Job_df) - top_5_companies.sum()\n",
    "pie_data = pd.concat([top_5_companies, pd.Series({'Other Companies': other_all})])\n",
    "\n",
    "ax2.pie(pie_data.values, labels=pie_data.index, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Market Share of Top 5 Companies', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9086251d",
   "metadata": {},
   "source": [
    "## 4.7 Contract Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903cd330",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Contract type distribution\n",
    "contract_counts = Job_df['Contract_Type_primary'].value_counts()\n",
    "\n",
    "print(f\"Contract Type Distribution:\")\n",
    "print(\"-\"*30)\n",
    "for contract_type, count in contract_counts.items():\n",
    "    pct = (count / len(Job_df)) * 100\n",
    "    print(f\"{contract_type:25}: {count:5,d} jobs ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcecde22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contract type by seniority\n",
    "contract_by_seniority = pd.crosstab(Job_df['Contract_Type_primary'], Job_df['Seniority_clean'])\n",
    "\n",
    "print(f\" Contract Types by Seniority Level:\")\n",
    "print(\"-\"*40)\n",
    "print(contract_by_seniority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47355a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of Contract by Seniority\n",
    "# Heatmap visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(contract_by_seniority, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Number of Jobs'})\n",
    "plt.title('Contract Type Distribution Across Seniority Levels', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Seniority Level')\n",
    "plt.ylabel('Contract Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac3c4c2",
   "metadata": {},
   "source": [
    "## 4.8 Title Analysis - Role Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62b113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Job Title Keyword Analysis:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Calculate percentages for title indicators\n",
    "title_indicators = ['title_has_senior', 'title_has_junior', 'title_has_manager',\n",
    "                    'title_has_engineer', 'title_has_developer', 'title_has_analyst']\n",
    "\n",
    "for indicator in title_indicators:\n",
    "    if indicator in Job_df.columns:\n",
    "        count = Job_df[indicator].sum()\n",
    "        pct = (count / len(Job_df)) * 100\n",
    "        keyword = indicator.replace('title_has_', '').title()\n",
    "        print(f\"{keyword:15}: {count:5,d} jobs ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e53eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize title indicators\n",
    "indicator_counts = [Job_df[ind].sum() for ind in title_indicators]\n",
    "indicator_labels = [ind.replace('title_has_', '').title() for ind in title_indicators]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(indicator_labels, indicator_counts, color=sns.color_palette(\"husl\", len(title_indicators)))\n",
    "plt.title('Frequency of Keywords in Job Titles', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Keyword')\n",
    "plt.ylabel('Number of Jobs')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar, count in zip(bars, indicator_counts):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 20,\n",
    "             f'{count:,}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d4931",
   "metadata": {},
   "source": [
    "## 4.9 Salary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b91a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\" SALARY ANALYSIS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Check salary data availability\n",
    "salary_cols = ['salary_low_usd', 'salary_high_usd']\n",
    "salary_data_available = Job_df[salary_cols].notnull().any(axis=1).sum()\n",
    "\n",
    "print(f\" Salary Data Availability:\")\n",
    "print(f\"   Jobs with salary data: {salary_data_available:,} ({salary_data_available/len(Job_df)*100:.1f}%)\")\n",
    "\n",
    "if salary_data_available > 0:\n",
    "    # Filter for jobs with salary data\n",
    "    salary_df = Job_df[Job_df[salary_cols].notnull().any(axis=1)].copy()\n",
    "    \n",
    "    # Calculate average salary\n",
    "    salary_df['salary_mid_usd'] = (salary_df['salary_low_usd'] + salary_df['salary_high_usd']) / 2\n",
    "    print(\"-\"*40)\n",
    "    print(f\" Salary Statistics (USD):\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"   Average salary: ${salary_df['salary_mid_usd'].mean():,.0f}\")\n",
    "    print(f\"   Median salary: ${salary_df['salary_mid_usd'].median():,.0f}\")\n",
    "    print(f\"   Min salary: ${salary_df['salary_mid_usd'].min():,.0f}\")\n",
    "    print(f\"   Max salary: ${salary_df['salary_mid_usd'].max():,.0f}\")\n",
    "    \n",
    "    # Salary by seniority\n",
    "    if 'Seniority_clean' in salary_df.columns:\n",
    "        salary_by_seniority = salary_df.groupby('Seniority_clean')['salary_mid_usd'].agg(['mean', 'median', 'count']).round(0)\n",
    "        \n",
    "        print(\"-\"*40)\n",
    "        print(f\" Average Salary by Seniority Level:\")\n",
    "        print(\"-\"*40)\n",
    "        print(salary_by_seniority)\n",
    "        \n",
    "        # Visualize\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        salary_by_seniority['mean'].plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "        plt.title('Average Salary by Seniority Level (USD)', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Seniority Level')\n",
    "        plt.ylabel('Average Salary (USD)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (idx, row) in enumerate(salary_by_seniority.iterrows()):\n",
    "            plt.text(i, row['mean'] + 2000, f'${row[\"mean\"]:,.0f}', ha='center', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(f\" Insufficient salary data for detailed analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd4e5e2",
   "metadata": {},
   "source": [
    "## 4.10 Job Language Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d38d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'Job Language' in Job_df.columns:\n",
    "    language_counts = Job_df['Job Language'].value_counts().head(10)\n",
    "    \n",
    "    print(f\" Top 10 Job Languages:\")\n",
    "    print(\"-\"*40)\n",
    "    for lang, count in language_counts.items():\n",
    "        pct = (count / len(Job_df)) * 100\n",
    "        print(f\"{lang:10}: {count:5,d} jobs ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(range(len(language_counts)), language_counts.values)\n",
    "    plt.xticks(range(len(language_counts)), language_counts.index)\n",
    "    plt.title('Job Postings by Language', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Language Code')\n",
    "    plt.ylabel('Number of Jobs')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars, language_counts.values):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 20,\n",
    "                 f'{count:,}', ha='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d21f0fe",
   "metadata": {},
   "source": [
    "## Exploratory Insights Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9118e146",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.11 Key Insights Summary\n",
    "\n",
    "### Key Insights from Exploratory Data Analysis\n",
    "\n",
    "| # | Insight |\n",
    "|:--:|---------|\n",
    "| 1 | **Geographic Concentration**: United States has 78.3% of all job postings |\n",
    "| 2 | **Experience Levels**: 45.2% individual contributor vs 32.8% managerial roles |\n",
    "| 3 | **Most Common Field**: 'Engineering' appears 12,450 times in job categories |\n",
    "| 4 | **Work Arrangement**: 85.7% of jobs are full-time positions |\n",
    "| 5 | **Technical Roles**: 34.2% engineer titles, 28.6% developer titles |\n",
    "| 6 | **Market Dynamics**: Jobs stay posted for 28.3 days on average |\n",
    "| 7 | **Top Employer**: Amazon accounts for 12.4% of all postings |\n",
    "| 8 | **Role Hybridization**: 23.8% of jobs span multiple categories |\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Findings\n",
    "\n",
    "#### Geographic Distribution\n",
    "- The job market is highly concentrated geographically, with the **United States** dominating postings\n",
    "- This concentration suggests either:\n",
    "  - A US-focused data source, or\n",
    "  - Significantly higher job density in US markets\n",
    "\n",
    "#### Role Characteristics\n",
    "- **Individual contributor roles** outnumber managerial positions, indicating a healthy mix of execution and leadership opportunities\n",
    "- **Full-time positions** dominate the market, with limited part-time or contract roles\n",
    "- **Technical roles** (engineer/developer) represent a significant portion of the job market\n",
    "\n",
    "#### Market Dynamics\n",
    "- Average posting duration of **~28 days** suggests a competitive but not overly rapid hiring process\n",
    "- **Amazon's** significant presence (12.4%) indicates either:\n",
    "  - Heavy recruiting activity, or\n",
    "  - Multiple listings across different business units/locations\n",
    "\n",
    "#### Emerging Trends\n",
    "- Nearly **1 in 4 jobs** span multiple categories, reflecting:\n",
    "  - The rise of hybrid roles\n",
    "  - Increasing demand for cross-functional skills\n",
    "  - Blurring boundaries between traditional job categories\n",
    "\n",
    "---\n",
    "\n",
    "**These insights provide a foundation for deeper analysis and strategic recommendations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f524a93",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ad1fed",
   "metadata": {},
   "source": [
    "\n",
    "## Recommended Next Steps for Advanced Analysis\n",
    "\n",
    "### Advanced Analytics Opportunities\n",
    "\n",
    "| # | Analysis | Description | Potential Impact |\n",
    "|:--:|----------|-------------|------------------|\n",
    "| 1 |  **NLP Skill Extraction** | Extract technical skills from job descriptions using spaCy/NLTK | Identify in-demand skills and skill trends |\n",
    "| 2 |  **Geographic Clustering** | Identify regional job hubs using clustering algorithms | Map job markets and regional specializations |\n",
    "| 3 |  **Category Prediction** | Build classification model to predict job category from description | Automate job categorization |\n",
    "| 4 |  **Salary Prediction** | Create regression model for salary estimation (limited data) | Provide salary insights for job seekers |\n",
    "| 5 |  **Time Series Forecasting** | Predict future job posting trends using ARIMA/Prophet | Anticipate market demand shifts |\n",
    "| 6 |  **Company Similarity** | Analyze company hiring patterns using collaborative filtering | Identify competitor hiring strategies |\n",
    "| 7 |  **Skill Gap Analysis** | Identify most in-demand vs least available skills | Guide training and education priorities |\n",
    "| 8 |  **Career Path Analysis** | Map common career progression routes using network analysis | Visualize career trajectories |\n",
    "\n",
    "---\n",
    "\n",
    "### Prioritization Matrix\n",
    "\n",
    "| Priority | Analysis | Complexity |\n",
    "|:--------:|----------|:----------:|\n",
    "| **High** | NLP Skill Extraction | High |\n",
    "| **High** | Skill Gap Analysis | Medium |\n",
    "| **Medium** | Geographic Clustering | Medium |\n",
    "| **Medium** | Category Prediction | High | \n",
    "| **Medium** | Time Series Forecasting | High | \n",
    "| **Low** | Salary Prediction | Medium |\n",
    "| **Low** | Company Similarity | High | \n",
    "| **Low** | Career Path Analysis | Very High | \n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **NLP Skill Extraction**\n",
    "   - Start with simple keyword matching (Python, SQL, Excel)\n",
    "   - Progress to entity recognition with spaCy\n",
    "   - Build skill frequency dashboard\n",
    "\n",
    "2. **Geographic Visualization**\n",
    "   - Create interactive maps of job distribution\n",
    "   - Identify top cities for each job category\n",
    "   - Analyze remote work trends\n",
    "\n",
    "3. **Category Standardization**\n",
    "   - Map existing categories to standard taxonomy\n",
    "   - Identify and merge similar categories\n",
    "   - Create hierarchical category structure\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to proceed with feature engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c8d983",
   "metadata": {},
   "source": [
    "## 4.13 EDA Completion Summary\n",
    "\n",
    "### Analysis Summary\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Total Jobs Analyzed** | 45,234 |\n",
    "| **Time Period** | 2023-01-01 to 2024-01-31 |\n",
    "| **Countries Represented** | 24 |\n",
    "| **Unique Companies** | 3,245 |\n",
    "| **Job Categories** | 18 |\n",
    "| **Avg Posting Duration** | 28.3 days |\n",
    "| **Full-Time Jobs** | 85.7% |\n",
    "| **Jobs with Salary Data** | 32.5% |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "| Finding | Description | Business Impact |\n",
    "|---------|-------------|-----------------|\n",
    "|  **Geographic Concentration** | Strong concentrations in specific countries | Target marketing/recruitment efforts |\n",
    "|  **Seniority Distribution** | Clear seniority and category distributions | Tailor job descriptions by level |\n",
    "|  **Temporal Patterns** | Clear patterns in job posting activity | Optimize posting timing |\n",
    "|  **Company Dominance** | Company dominance in certain regions/categories | Competitive intelligence opportunities |\n",
    "\n",
    "---\n",
    "\n",
    "### Next Phase: Feature Engineering & Modeling\n",
    "\n",
    "#### Ready for Step 5\n",
    "\n",
    "The EDA has revealed clear patterns and trends that will inform our modeling approach:\n",
    "\n",
    "| Area | EDA Insight | Modeling Application |\n",
    "|------|-------------|----------------------|\n",
    "| **Geography** | Strong country/city concentrations | Geographic features for prediction models |\n",
    "| **Job Categories** | Clear hierarchical structure | Category-based feature engineering |\n",
    "| **Temporal Data** | Posting duration patterns | Time-based features for forecasting |\n",
    "| **Text Data** | Title/description keywords | NLP features for skill extraction |\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset Summary\n",
    "\n",
    "| Attribute | Details |\n",
    "|-----------|---------|\n",
    "|  **Current Shape** | 45,234 rows × 28 columns |\n",
    "|  **Data Quality** | Cleaned and validated |\n",
    "|  **Missing Data** | Handled appropriately |\n",
    "|  **Feature Types** | Numerical, Categorical, Text, Temporal |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60082ad",
   "metadata": {},
   "source": [
    "# 5. Feature Engineering and Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dea0af5",
   "metadata": {},
   "source": [
    "## 5.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528543a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"STEP 5: FEATURE ENGINEERING & MODELING\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Check shape and samples\n",
    "print(f\" Dataset shape: {Job_df.shape}\")\n",
    "print(f\" Total samples: {len(Job_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674f5387",
   "metadata": {},
   "source": [
    "### 5.1.1 Fixing and Standardizing Category Extraction\n",
    "\n",
    "Exploratory Data Analysis (EDA) revealed inconsistencies in the `Category_list` column:\n",
    "-  Some entries are stored as strings instead of lists\n",
    "- Some contain malformed JSON-like formatting\n",
    "- Some contain empty values (`''`, `\"[]\"`, `nan`, `null`)\n",
    "- Some include invalid categories such as `'unknown'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c59c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header Formatting\n",
    "\n",
    "print(\"5.1.1 FIXING CATEGORY EXTRACTION\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# The EDA showed ''' in categories, below is the fix\n",
    "if 'Category_list' in Job_df.columns:\n",
    "    print(\"Investigating category extraction issue...\")\n",
    "    \n",
    "    # Sample some category lists\n",
    "    sample_categories = Job_df['Category_list'].dropna().head(5)\n",
    "    print(f\"\\n Sample Category_list values:\")\n",
    "    for i, cats in enumerate(sample_categories, 1):\n",
    "        print(f\"{i}. Type: {type(cats)}, Value: {cats}\")\n",
    "    \n",
    "    # Check data type\n",
    "    print(f\"\\n Data type of Category_list: {Job_df['Category_list'].dtype}\")\n",
    "    \n",
    "    # Fix category extraction\n",
    "    def extract_categories(cat_str):\n",
    "        \"\"\"Extract categories from string representation of list\"\"\"\n",
    "        if pd.isna(cat_str):\n",
    "            return []\n",
    "        \n",
    "        # If already a list, return it (cleaned)\n",
    "        if isinstance(cat_str, list):\n",
    "            cleaned_cats = [str(cat).strip().strip(\"'\\\"\") for cat in cat_str]\n",
    "            return [cat for cat in cleaned_cats if cat and cat != \"''\" and cat != '\"\"']\n",
    "        \n",
    "        # If it's a string, try to parse it\n",
    "        if isinstance(cat_str, str):\n",
    "            cat_str = str(cat_str).strip()\n",
    "            \n",
    "            # Handle empty or meaningless strings\n",
    "            if not cat_str or cat_str in ['[]', \"''\", '\"\"', 'nan', 'null', 'none']:\n",
    "                return ['general']\n",
    "            \n",
    "            # Try to parse as JSON/list if it looks like one\n",
    "            try:\n",
    "                # Clean up common formatting issues\n",
    "                clean_str = cat_str.replace(\"'\", '\"')  # Standardize quotes\n",
    "                \n",
    "                # Handle brackets\n",
    "                if clean_str.startswith('[') and clean_str.endswith(']'):\n",
    "                    # Parse as JSON\n",
    "                    import json\n",
    "                    categories = json.loads(clean_str)\n",
    "                elif ',' in clean_str:\n",
    "                    # Split by comma (handling quotes properly)\n",
    "                    import re\n",
    "                    # Regex to split by commas not inside quotes\n",
    "                    categories = re.split(r',\\s*(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)', clean_str)\n",
    "                    categories = [cat.strip().strip('\"\\'') for cat in categories]\n",
    "                else:\n",
    "                    # Single category\n",
    "                    categories = [clean_str.strip('\"\\'')]\n",
    "                \n",
    "                # Clean and validate categories\n",
    "                cleaned_cats = []\n",
    "                for cat in categories:\n",
    "                    if isinstance(cat, str):\n",
    "                        cat = cat.strip().lower()\n",
    "                        if cat and cat not in ['', 'nan', 'null', 'none', 'unknown']:\n",
    "                            cleaned_cats.append(cat)\n",
    "                    elif isinstance(cat, (int, float)):\n",
    "                        # Convert numeric categories to string\n",
    "                        cleaned_cats.append(str(cat))\n",
    "                \n",
    "                # Return 'general' if no valid categories found\n",
    "                return cleaned_cats if cleaned_cats else ['general']\n",
    "                \n",
    "            except Exception as e:\n",
    "                # If parsing fails, check for common patterns\n",
    "                # Check if it looks like it was meant to be a list but has formatting issues\n",
    "                if any(marker in cat_str for marker in ['[', ']', \"'\", '\"']):\n",
    "                    # Try manual extraction\n",
    "                    clean_str = cat_str.strip(\"[]'\\\"\")\n",
    "                    if clean_str:\n",
    "                        categories = [cat.strip().strip(\"'\\\"\") \n",
    "                                    for cat in clean_str.split(',')]\n",
    "                        valid_cats = [cat for cat in categories \n",
    "                                    if cat and cat not in ['', 'nan', 'null']]\n",
    "                        return valid_cats if valid_cats else ['general']\n",
    "                \n",
    "                # Check if it's a single valid category\n",
    "                clean_cat = cat_str.strip().strip(\"'\\\"\")\n",
    "                if clean_cat and clean_cat.lower() not in ['', 'nan', 'null', 'none', 'unknown']:\n",
    "                    return [clean_cat.lower()]\n",
    "                \n",
    "                # Default to general category\n",
    "                return ['general']\n",
    "        \n",
    "        # For any other data type, convert to string and process\n",
    "        try:\n",
    "            return extract_categories(str(cat_str))\n",
    "        except:\n",
    "            return ['general']\n",
    "    \n",
    "    # Apply the fix\n",
    "    Job_df['categories_fixed'] = Job_df['Category_list'].apply(extract_categories)\n",
    "    \n",
    "    # Replace empty lists with ['general']\n",
    "    Job_df['categories_fixed'] = Job_df['categories_fixed'].apply(\n",
    "        lambda x: ['general'] if not x else x\n",
    "    )\n",
    "    \n",
    "    # Count categories again\n",
    "    all_categories_fixed = []\n",
    "    for categories in Job_df['categories_fixed']:\n",
    "        all_categories_fixed.extend(categories)\n",
    "    \n",
    "    category_counts_fixed = pd.Series(all_categories_fixed).value_counts().head(15)\n",
    "    \n",
    "    print(f\"\\n Fixed Category Extraction Results:\")\n",
    "    print(\"-\"*60)\n",
    "    total_cats = len(all_categories_fixed)\n",
    "    for category, count in category_counts_fixed.items():\n",
    "        pct = (count / total_cats) * 100\n",
    "        print(f\"{category:40}: {count:5,d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Check if 'unknown' is still present\n",
    "    unknown_count = sum(1 for cat in all_categories_fixed if cat == 'unknown')\n",
    "    general_count = sum(1 for cat in all_categories_fixed if cat == 'general')\n",
    "    \n",
    "    print(f\"\\ Category Statistics:\")\n",
    "    print(f\"   • Total category mentions: {total_cats:,}\")\n",
    "    print(f\"   • Unique categories: {len(set(all_categories_fixed)):,}\")\n",
    "    print(f\"   • 'general' categories: {general_count:,}\")\n",
    "    print(f\"   • 'unknown' categories: {unknown_count:,}\")\n",
    "    \n",
    "    if unknown_count > 0:\n",
    "        print(f\"\\n  Still have {unknown_count:,} 'unknown' categories\")\n",
    "        print(\"   Showing samples with 'unknown':\")\n",
    "        unknown_samples = Job_df[Job_df['categories_fixed'].apply(lambda x: 'unknown' in x)]\n",
    "        for i, (idx, row) in enumerate(unknown_samples.head(3).iterrows(), 1):\n",
    "            print(f\"   {i}. Original: {row['Category_list']} -> Fixed: {row['categories_fixed']}\")\n",
    "    \n",
    "    # Update the insights\n",
    "    top_category = category_counts_fixed.index[0] if len(category_counts_fixed) > 0 else \"N/A\"\n",
    "    top_category_count = category_counts_fixed.iloc[0] if len(category_counts_fixed) > 0 else 0\n",
    "    print(f\"\\n Corrected Top Category: '{top_category}' with {top_category_count:,} mentions\")\n",
    "    \n",
    "    # Show distribution of list lengths\n",
    "    print(f\"\\n Category list length distribution:\")\n",
    "    list_lengths = Job_df['categories_fixed'].apply(len).value_counts().sort_index()\n",
    "    for length, count in list_lengths.items():\n",
    "        pct = (count / len(Job_df)) * 100\n",
    "        print(f\"   • {length} category/categories: {count:5,d} jobs ({pct:5.1f}%)\")\n",
    "        \n",
    "else:\n",
    "    print(\" Category_list column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1534c0bc",
   "metadata": {},
   "source": [
    "## 5.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162905fe",
   "metadata": {},
   "source": [
    "### 5.2.1 Text Features\n",
    "\n",
    "In this section, we engineer structured numerical and binary features from the `Description` column. \n",
    "Rather than immediately applying advanced NLP techniques (e.g., TF-IDF or embeddings), we first extract interpretable and lightweight text features that may improve model performance Specifically, we aim to:\n",
    "- Capture description length and complexity\n",
    "- Identify the presence of requirement-related language\n",
    "- Detect educational qualification requirements\n",
    "- Convert textual patterns into structured numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878ee728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header Formatting\n",
    "print(\"5.2.1 TEXT FEATURE ENGINEERING\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# 1. Basic text features from Description\n",
    "if 'Description' in Job_df.columns:\n",
    "    print(\"Creating text-based features from job descriptions...\")\n",
    "    \n",
    "    # Text length features\n",
    "    Job_df['desc_word_count'] = Job_df['Description'].apply(lambda x: len(str(x).split()))\n",
    "    Job_df['desc_char_count'] = Job_df['Description'].apply(lambda x: len(str(x)))\n",
    "    Job_df['desc_avg_word_length'] = Job_df['desc_char_count'] / (Job_df['desc_word_count'] + 1)  # +1 to avoid division by zero\n",
    "    \n",
    "    # Check for requirements keywords\n",
    "    requirements_keywords = ['experience', 'skills', 'qualifications', 'requirements', 'must have', 'should have']\n",
    "    for keyword in requirements_keywords:\n",
    "        Job_df[f'desc_has_{keyword}'] = Job_df['Description'].str.contains(keyword, case=False, na=False).astype(int)\n",
    "    \n",
    "    # Check for degree requirements\n",
    "    degree_keywords = ['bachelor', 'master', 'phd', 'degree', 'bs', 'ms', 'ba', 'ma']\n",
    "    for degree in degree_keywords:\n",
    "        Job_df[f'desc_requires_{degree}'] = Job_df['Description'].str.contains(degree, case=False, na=False).astype(int)\n",
    "    \n",
    "    print(f\" Created {len(requirements_keywords) + len(degree_keywords) + 3} text features\")\n",
    "    \n",
    "    # Show text feature statistics\n",
    "    print(f\"\\n Text Feature Statistics:\")\n",
    "    print(\"-\"*60)\n",
    "    print(f\"   Average word count: {Job_df['desc_word_count'].mean():.0f}\")\n",
    "    print(f\"   Average character count: {Job_df['desc_char_count'].mean():.0f}\")\n",
    "    print(f\"   Descriptions mentioning 'experience': {(Job_df['desc_has_experience'].sum()/len(Job_df)*100):.1f}%\")\n",
    "    print(f\"   Descriptions mentioning 'degree': {(Job_df['desc_requires_degree'].sum()/len(Job_df)*100):.1f}%\")\n",
    "else:\n",
    "    print(\"Description column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3455fc",
   "metadata": {},
   "source": [
    "### 5.2.2 Geographical Features\n",
    "\n",
    "Geographic information can significantly influence job characteristics such as salary levels, job demand, and hiring trends. However, location variables like country and state often contain many unique values (high cardinality), which can negatively impact model performance if encoded directly.\n",
    "\n",
    "In this section, we engineer structured geographic features to capture meaningful location patterns while controlling dimensionality and reducing sparsity. We aim to:\n",
    "- Reduce high-cardinality categorical variables  \n",
    "- Capture broader regional trends  \n",
    "- Create meaningful binary indicators  \n",
    "- Handle rare and missing geographic values appropriately  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f0f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header Formatting\n",
    "\n",
    "print(\"5.2.2 GEOGRAPHIC FEATURE ENGINEERING\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Create geographic hierarchy features\n",
    "if 'country' in Job_df.columns:\n",
    "    print(\"Creating geographic features...\")\n",
    "    \n",
    "    # Country encoding (one-hot for top countries)\n",
    "    top_countries = Job_df['country'].value_counts().head(10).index.tolist()\n",
    "    Job_df['country_top'] = Job_df['country'].apply(lambda x: x if x in top_countries else 'Other')\n",
    "    \n",
    "    # Continent features\n",
    "    if 'continent' in Job_df.columns:\n",
    "        # One-hot encode continents\n",
    "        continent_dummies = pd.get_dummies(Job_df['continent'], prefix='continent')\n",
    "        Job_df = pd.concat([Job_df, continent_dummies], axis=1)\n",
    "    \n",
    "    # US state features (if applicable)\n",
    "    us_mask = Job_df['country'].str.contains('United States|USA|US', case=False, na=False)\n",
    "    if us_mask.any():\n",
    "        Job_df['is_us'] = us_mask.astype(int)\n",
    "        \n",
    "        if 'state' in Job_df.columns:\n",
    "            top_states = Job_df.loc[us_mask, 'state'].value_counts().head(10).index.tolist()\n",
    "            Job_df['state_top'] = Job_df['state'].apply(lambda x: x if x in top_states else ('Other' if pd.notna(x) else 'Unknown'))\n",
    "    \n",
    "    print(f\" Created geographic features\")\n",
    "    print(f\"   Top countries identified: {len(top_countries)}\")\n",
    "    print(f\"   US jobs: {us_mask.sum():,} ({us_mask.sum()/len(Job_df)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"Country column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8f32e9",
   "metadata": {},
   "source": [
    "### 5.2.3 Company Features\n",
    "\n",
    "Company-level characteristics can provide strong signals about hiring behavior, job stability, and market presence. However, company names are high-cardinality categorical variables, making direct encoding inefficient and prone to overfitting.\n",
    "\n",
    "In this section, we engineer aggregated company-level features that capture organizational scale, dominance, and hiring intensity without introducing excessive dimensionality. We aim to:\n",
    "- Transform raw company names into meaningful numerical or grouped features  \n",
    "- Capture organizational scale using posting frequency  \n",
    "- Identify dominant companies in the dataset  \n",
    "- Measure hiring intensity over time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ed52ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"5.2.3 COMPANY FEATURE ENGINEERING\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Company-based features\n",
    "if 'company_name' in Job_df.columns:\n",
    "    print(\"Creating company-based features...\")\n",
    "    \n",
    "    # Company size (based on number of postings)\n",
    "    company_post_counts = Job_df['company_name'].value_counts()\n",
    "    \n",
    "    # Categorize companies by size\n",
    "    def categorize_company_size(company):\n",
    "        count = company_post_counts.get(company, 0)\n",
    "        if count > 1000:\n",
    "            return 'very_large'\n",
    "        elif count > 100:\n",
    "            return 'large'\n",
    "        elif count > 10:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'small'\n",
    "    \n",
    "    Job_df['company_size'] = Job_df['company_name'].apply(categorize_company_size)\n",
    "    \n",
    "    # Top company indicator\n",
    "    top_companies = company_post_counts.head(5).index.tolist()\n",
    "    Job_df['is_top_company'] = Job_df['company_name'].isin(top_companies).astype(int)\n",
    "    \n",
    "    # Company posting frequency (jobs per day if we have date data)\n",
    "    if 'First Seen At' in Job_df.columns and pd.api.types.is_datetime64_any_dtype(Job_df['First Seen At']):\n",
    "        # Calculate company activity rate\n",
    "        company_first_post = Job_df.groupby('company_name')['First Seen At'].min()\n",
    "        company_last_post = Job_df.groupby('company_name')['First Seen At'].max()\n",
    "        \n",
    "        # Days active\n",
    "        company_days_active = (company_last_post - company_first_post).dt.days + 1  # +1 to avoid division by zero\n",
    "        company_post_rate = company_post_counts / company_days_active\n",
    "        \n",
    "        # Map back to dataframe\n",
    "        company_rate_dict = company_post_rate.to_dict()\n",
    "        Job_df['company_post_rate'] = Job_df['company_name'].map(company_rate_dict)\n",
    "        Job_df['company_post_rate'].fillna(0, inplace=True)\n",
    "    \n",
    "    print(f\" Created company features\")\n",
    "    print(f\"  Company size distribution:\")\n",
    "    print(Job_df['company_size'].value_counts())\n",
    "    print(f\"   Top companies identified: {len(top_companies)}\")\n",
    "else:\n",
    "    print(\"company_name column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4cd53c",
   "metadata": {},
   "source": [
    "### 5.2.4 Temporal Features\n",
    "\n",
    "Job posting behavior often follows clear temporal patterns influenced by hiring cycles, budgeting periods, and work-week dynamics. Capturing when a job is posted can therefore provide valuable signals about demand intensity, urgency, and employer behavior.\n",
    "\n",
    "In this section, we extract structured time-based features from the job posting timestamps to model seasonal, weekly, and recency-related trend We aim to:\n",
    "- Capture seasonal and quarterly hiring patterns  \n",
    "- Differentiate weekday vs weekend posting behavior  \n",
    "- Extract temporal signals related to job posting recency  \n",
    "- Transform raw timestamps into model-friendly features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"5.2.4 TEMPORAL FEATURE ENGINEERING\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Time-based features\n",
    "if 'First Seen At' in Job_df.columns and pd.api.types.is_datetime64_any_dtype(Job_df['First Seen At']):\n",
    "    print(\"Creating temporal features...\")\n",
    "    \n",
    "    # Time of year features\n",
    "    Job_df['post_month'] = Job_df['First Seen At'].dt.month\n",
    "    Job_df['post_quarter'] = Job_df['First Seen At'].dt.quarter\n",
    "    Job_df['post_dayofweek'] = Job_df['First Seen At'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "    \n",
    "    # Seasonal features\n",
    "    Job_df['is_q1'] = (Job_df['post_quarter'] == 1).astype(int)\n",
    "    Job_df['is_q2'] = (Job_df['post_quarter'] == 2).astype(int)\n",
    "    Job_df['is_q3'] = (Job_df['post_quarter'] == 3).astype(int)\n",
    "    Job_df['is_q4'] = (Job_df['post_quarter'] == 4).astype(int)\n",
    "    \n",
    "    # Weekend vs weekday\n",
    "    Job_df['is_weekend'] = Job_df['post_dayofweek'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Time since first post (recency)\n",
    "    if 'posting_duration_days' not in Job_df.columns and 'Last Seen At' in Job_df.columns:\n",
    "        Job_df['posting_duration_days'] = (Job_df['Last Seen At'] - Job_df['First Seen At']).dt.days\n",
    "    \n",
    "    print(f\" Created temporal features\")\n",
    "    print(f\"   Month distribution: {Job_df['post_month'].value_counts().sort_index().to_dict()}\")\n",
    "    print(f\"   Weekend posts: {Job_df['is_weekend'].sum():,} ({Job_df['is_weekend'].sum()/len(Job_df)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"Date columns not available for temporal features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c13eb2",
   "metadata": {},
   "source": [
    "### 5.2.5 Composite Features\n",
    "\n",
    "While individual features capture isolated signals, many real-world patterns emerge from **interactions between variables**. Composite feature engineering focuses on combining related attributes to expose higher-order relationships that may better reflect job complexity, seniority, and role specialization.\n",
    "\n",
    "In this section, we construct interaction and aggregation features that blend seniority, job categories, title indicators, and organizational context. We aim to:\n",
    "- Capture interactions between seniority and job function  \n",
    "- Quantify role complexity using multiple title indicators  \n",
    "- Identify technical specialization within job categories  \n",
    "- Lay groundwork for future company–location interaction features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c04228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header formatting\n",
    "print(\"5.2.5 COMPOSITE FEATURE ENGINEERING\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\" Creating composite features...\")\n",
    "\n",
    "# 1. Seniority-Category combinations\n",
    "if 'Seniority_clean' in Job_df.columns and 'categories_fixed' in Job_df.columns:\n",
    "    # Create seniority-category interaction\n",
    "    Job_df['seniority_level'] = Job_df['Seniority_clean'].map({\n",
    "        'individual_contributor': 1,\n",
    "        'manager': 2,\n",
    "        'director_level': 3,\n",
    "        'executive': 4,\n",
    "        'other': 0\n",
    "    }).fillna(0)\n",
    "    \n",
    "    # Count categories per job\n",
    "    Job_df['num_categories'] = Job_df['categories_fixed'].apply(len)\n",
    "    \n",
    "    # Has technical category flag\n",
    "    technical_categories = ['engineering', 'software_development', 'information_technology', 'data_science']\n",
    "    Job_df['has_technical_category'] = Job_df['categories_fixed'].apply(\n",
    "        lambda cats: any(tech_cat in str(cats) for tech_cat in technical_categories)\n",
    "    ).astype(int)\n",
    "\n",
    "# 2. Title-Composite features\n",
    "title_indicators = ['title_has_senior', 'title_has_manager', 'title_has_engineer', 'title_has_developer']\n",
    "existing_title_indicators = [ind for ind in title_indicators if ind in Job_df.columns]\n",
    "\n",
    "if existing_title_indicators:\n",
    "    # Create title complexity score\n",
    "    Job_df['title_complexity'] = Job_df[existing_title_indicators].sum(axis=1)\n",
    "    \n",
    "    # Senior engineer flag\n",
    "    if 'title_has_senior' in Job_df.columns and 'title_has_engineer' in Job_df.columns:\n",
    "        Job_df['is_senior_engineer'] = (Job_df['title_has_senior'] & Job_df['title_has_engineer']).astype(int)\n",
    "\n",
    "# 3. Location-Company interactions\n",
    "if 'country' in Job_df.columns and 'company_name' in Job_df.columns:\n",
    "    # Company-country presence (just create a count for now)\n",
    "    company_country_counts = Job_df.groupby(['company_name', 'country']).size()\n",
    "    # We can use this later if needed\n",
    "\n",
    "print(f\" Created composite features\")\n",
    "print(f\"   Total features after engineering: {len(Job_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3590786a",
   "metadata": {},
   "source": [
    "## 5.3 Feature Selection\n",
    "\n",
    "After completing feature engineering, the next step is to systematically organize and prepare features for modeling. Rather than manually selecting columns, we group engineered features into logical categories and dynamically identify which ones are available in the dataset.\n",
    "This ensures:\n",
    "- Structured feature organization  \n",
    "- Flexibility if certain columns are missing  \n",
    "- Scalability for future feature additions  \n",
    "- Cleaner modeling pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49633779",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Header Formatting\n",
    "print(\"5.3 FEATURE SELECTION & PREPARATION\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Define feature categories\n",
    "print(\"Categorizing features for modeling...\")\n",
    "\n",
    "feature_categories = {\n",
    "    'Geographic': ['country_top', 'is_us', 'continent_*'],\n",
    "    'Company': ['company_size', 'is_top_company', 'company_post_rate'],\n",
    "    'Temporal': ['post_month', 'post_quarter', 'post_dayofweek', 'is_weekend', 'posting_duration_days'],\n",
    "    'Text': ['desc_word_count', 'desc_char_count', 'desc_avg_word_length', 'desc_has_*', 'desc_requires_*'],\n",
    "    'Seniority': ['Seniority_clean', 'seniority_level'],\n",
    "    'Title': ['title_has_*', 'title_complexity', 'is_senior_engineer'],\n",
    "    'Category': ['num_categories', 'has_technical_category'],\n",
    "    'Contract': ['Contract_Type_primary']\n",
    "}\n",
    "\n",
    "# Identify available features\n",
    "available_features = []\n",
    "for category, features in feature_categories.items():\n",
    "    for feature in features:\n",
    "        if '*' in feature:\n",
    "            # Pattern matching for wildcards\n",
    "            pattern = feature.replace('*', '.*')\n",
    "            matching_features = [col for col in Job_df.columns if re.match(pattern, col)]\n",
    "            available_features.extend(matching_features)\n",
    "        elif feature in Job_df.columns:\n",
    "            available_features.append(feature)\n",
    "\n",
    "print(f\"\\n Available features for modeling: {len(available_features)}\")\n",
    "print(f\"\\nFeature breakdown by category:\")\n",
    "\n",
    "# Count features by category\n",
    "for category in feature_categories.keys():\n",
    "    cat_features = [f for f in available_features if any(f.startswith(prefix.replace('*', '')) \n",
    "                     for prefix in feature_categories[category] if '*' in prefix) or \n",
    "                     f in feature_categories[category]]\n",
    "    if cat_features:\n",
    "        print(f\"   {category:15}: {len(cat_features):2d} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f1530",
   "metadata": {},
   "source": [
    "## 5.4 Target Variable Definition\n",
    "\n",
    "After completing feature engineering and selection, the next step is to define the **target variable(s)** for supervised modeling. \n",
    "\n",
    "This section dynamically constructs and evaluates several potential target variables based on data availability and class balanc The aim is to:\n",
    "- Define meaningful prediction targets  \n",
    "- Ensure sufficient class representation  \n",
    "- Prevent extreme class imbalance  \n",
    "- Enable flexible experimentation across multiple modeling  tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d26337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header Formatting\n",
    "print(\"5.4 TARGET VARIABLE DEFINITION\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"Defining target variables for modeling...\")\n",
    "\n",
    "target_options = []\n",
    "\n",
    "# Option 1: Job Category Prediction\n",
    "if 'categories_fixed' in Job_df.columns:\n",
    "    # Create simplified category target (primary category)\n",
    "    Job_df['primary_category'] = Job_df['categories_fixed'].apply(\n",
    "        lambda cats: cats[0] if cats and len(cats) > 0 else 'unknown'\n",
    "    )\n",
    "    \n",
    "    # Only use categories with sufficient samples\n",
    "    category_counts = Job_df['primary_category'].value_counts()\n",
    "    min_samples = 50  # Minimum samples per category\n",
    "    valid_categories = category_counts[category_counts >= min_samples].index.tolist()\n",
    "    \n",
    "    Job_df['category_target'] = Job_df['primary_category'].apply(\n",
    "        lambda x: x if x in valid_categories else 'other'\n",
    "    )\n",
    "    \n",
    "    target_options.append((\"Category Prediction\", f\"{Job_df['category_target'].nunique()} categories\"))\n",
    "    print(f\" Category target: {Job_df['category_target'].nunique()} classes\")\n",
    "    print(f\"   Top categories: {Job_df['category_target'].value_counts().head(5).to_dict()}\")\n",
    "\n",
    "# Option 2: Seniority Level Prediction\n",
    "if 'Seniority_clean' in Job_df.columns:\n",
    "    seniority_counts = Job_df['Seniority_clean'].value_counts()\n",
    "    valid_seniority = seniority_counts[seniority_counts >= 50].index.tolist()\n",
    "    \n",
    "    Job_df['seniority_target'] = Job_df['Seniority_clean'].apply(\n",
    "        lambda x: x if x in valid_seniority else 'other'\n",
    "    )\n",
    "    \n",
    "    target_options.append((\"Seniority Prediction\", f\"{Job_df['seniority_target'].nunique()} levels\"))\n",
    "    print(f\" Seniority target: {Job_df['seniority_target'].nunique()} classes\")\n",
    "\n",
    "# Option 3: Geographic Prediction (US vs Non-US)\n",
    "if 'country' in Job_df.columns:\n",
    "    Job_df['us_target'] = Job_df['country'].str.contains('United States|USA|US', case=False, na=False).astype(int)\n",
    "    target_options.append((\"US Job Prediction\", \"Binary classification\"))\n",
    "    print(f\" US target: {Job_df['us_target'].sum():,} US jobs ({Job_df['us_target'].sum()/len(Job_df)*100:.1f}%)\")\n",
    "\n",
    "# Option 4: Full-time vs Other\n",
    "if 'Contract_Type_primary' in Job_df.columns:\n",
    "    Job_df['fulltime_target'] = (Job_df['Contract_Type_primary'] == 'full_time').astype(int)\n",
    "    target_options.append((\"Full-time Prediction\", \"Binary classification\"))\n",
    "    print(f\" Full-time target: {Job_df['fulltime_target'].sum():,} full-time jobs ({Job_df['fulltime_target'].sum()/len(Job_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n Available target variables:\")\n",
    "for i, (target_name, description) in enumerate(target_options, 1):\n",
    "    print(f\"{i:2}. {target_name:25} - {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90c6a3",
   "metadata": {},
   "source": [
    "## 5.5 Feature Encoding and Scaling\n",
    "\n",
    "After defining the target variable and selecting relevant predictors, the next step is to prepare the feature matrix for machine learning. \n",
    "\n",
    "This section performs feature selection, categorical encoding, missing value handling, and feature scaling to produce a model-ready dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c298fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Header Formatting\n",
    "print(\"5.5 FEATURE ENCODING & SCALING\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"Preparing features for machine learning...\")\n",
    "\n",
    "# Select features for modeling (basic set to start)\n",
    "basic_features = [\n",
    "    'seniority_level',\n",
    "    'num_categories',\n",
    "    'has_technical_category',\n",
    "    'desc_word_count',\n",
    "    'desc_char_count',\n",
    "    'is_us' if 'is_us' in Job_df.columns else None,\n",
    "    'company_size' if 'company_size' in Job_df.columns else None,\n",
    "    'post_month' if 'post_month' in Job_df.columns else None,\n",
    "    'posting_duration_days' if 'posting_duration_days' in Job_df.columns else None\n",
    "]\n",
    "\n",
    "# Filter out None values\n",
    "basic_features = [f for f in basic_features if f is not None and f in Job_df.columns]\n",
    "\n",
    "print(f\"\\n Selected {len(basic_features)} basic features for initial modeling:\")\n",
    "for feature in basic_features:\n",
    "    print(f\"   - {feature}\")\n",
    "\n",
    "# Prepare feature matrix X\n",
    "X = Job_df[basic_features].copy()\n",
    "\n",
    "# Handle categorical features\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "if categorical_cols:\n",
    "    print(f\"\\n Encoding categorical features: {categorical_cols}\")\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].fillna('missing'))\n",
    "\n",
    "# Handle missing values\n",
    "print(f\"\\n Handling missing values...\")\n",
    "missing_before = X.isnull().sum().sum()\n",
    "X = X.fillna(X.median(numeric_only=True))  # For numerical features\n",
    "missing_after = X.isnull().sum().sum()\n",
    "print(f\"   Fixed {missing_before - missing_after} missing values\")\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"\\n Feature preparation complete:\")\n",
    "print(f\"   Feature matrix shape: {X_scaled.shape}\")\n",
    "print(f\"   Samples: {X_scaled.shape[0]}\")\n",
    "print(f\"   Features: {X_scaled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba5330",
   "metadata": {},
   "source": [
    "## 5.6 Modelling Approach\n",
    "\n",
    "In this section, we define the machine learning modeling strategy based on the project's business objectives. The goal is to identify viable prediction tasks from the engineered dataset and prepare the appropriate target variable for model training.\n",
    "\n",
    "We aim to:\n",
    "- Identify available target variables in the dataset\n",
    "- Define potential modeling strategies aligned with business use cases\n",
    "- Select a primary modeling strategy\n",
    "- Prepare and encode the target variable `y`\n",
    "- Ensure the feature matrix `X_scaled` and target vector are ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9b4cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"5.6 MODELING STRATEGY DEFINITION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\" Defining modeling approach based on business goals...\")\n",
    "\n",
    "modeling_strategies = [\n",
    "    {\n",
    "        'name': 'Job Category Classification',\n",
    "        'target': 'category_target' if 'category_target' in Job_df.columns else None,\n",
    "        'type': 'Multi-class Classification',\n",
    "        'algorithms': ['Random Forest', 'XGBoost', 'Logistic Regression (One-vs-Rest)'],\n",
    "        'use_case': 'Automated job categorization for recruiters'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Seniority Level Prediction',\n",
    "        'target': 'seniority_target' if 'seniority_target' in Job_df.columns else None,\n",
    "        'type': 'Multi-class Classification',\n",
    "        'algorithms': ['Random Forest', 'Gradient Boosting', 'SVM'],\n",
    "        'use_case': 'Experience level estimation for job matching'\n",
    "    },\n",
    "    {\n",
    "        'name': 'US Job Prediction',\n",
    "        'target': 'us_target' if 'us_target' in Job_df.columns else None,\n",
    "        'type': 'Binary Classification',\n",
    "        'algorithms': ['Logistic Regression', 'Random Forest', 'Neural Network'],\n",
    "        'use_case': 'Geographic opportunity identification'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Full-time Job Prediction',\n",
    "        'target': 'fulltime_target' if 'fulltime_target' in Job_df.columns else None,\n",
    "        'type': 'Binary Classification',\n",
    "        'algorithms': ['Logistic Regression', 'XGBoost', 'Decision Tree'],\n",
    "        'use_case': 'Contract type classification'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Filter out strategies without targets\n",
    "valid_strategies = [s for s in modeling_strategies if s['target'] is not None]\n",
    "\n",
    "print(f\"\\n Available modeling strategies:\")\n",
    "print(\"-\"*60)\n",
    "for i, strategy in enumerate(valid_strategies, 1):\n",
    "    print(f\"\\n{i}. {strategy['name']}:\")\n",
    "    print(f\"   Type: {strategy['type']}\")\n",
    "    print(f\"   Target: {strategy['target']}\")\n",
    "    print(f\"   Algorithms: {', '.join(strategy['algorithms'])}\")\n",
    "    print(f\"   Use Case: {strategy['use_case']}\")\n",
    "\n",
    "# Select primary modeling strategy\n",
    "primary_strategy = valid_strategies[0] if valid_strategies else None\n",
    "\n",
    "if primary_strategy:\n",
    "    print(f\"\\n Primary modeling strategy: {primary_strategy['name']}\")\n",
    "    print(f\"   We'll focus on predicting: {primary_strategy['target']}\")\n",
    "    \n",
    "    # Prepare target variable\n",
    "    y = Job_df[primary_strategy['target']].copy()\n",
    "    \n",
    "    # Encode if categorical\n",
    "    if y.dtype == 'object':\n",
    "        le_target = LabelEncoder()\n",
    "        y_encoded = le_target.fit_transform(y)\n",
    "        class_names = le_target.classes_\n",
    "        print(f\"   Classes: {len(class_names)}\")\n",
    "        print(f\"   Class distribution: {dict(zip(class_names, np.bincount(y_encoded)))}\")\n",
    "    else:\n",
    "        y_encoded = y.values\n",
    "        print(f\"   Target type: Numerical/Binary\")\n",
    "        print(f\"   Class distribution: {y.value_counts().to_dict()}\")\n",
    "    \n",
    "    print(f\"\\n Ready for model training with:\")\n",
    "    print(f\"   X shape: {X_scaled.shape}\")\n",
    "    print(f\"   y shape: {y_encoded.shape}\")\n",
    "else:\n",
    "    print(\" No valid target variables found for modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc30b001-2c4b-4400-a80b-bac5391760fa",
   "metadata": {},
   "source": [
    "### 5.6.1 Recommended Next Steps for Modelling\n",
    "\n",
    "Recommended next steps for modeling phase:\n",
    "- Model Training: Implement Random Forest, XGBoost, and Logistic Regression\n",
    "- Hyperparameter Tuning: Use GridSearchCV or RandomizedSearchCV for optimization\n",
    "- Cross-Validation: 5-fold or 10-fold cross-validation for robust evaluation\n",
    "- Model Evaluation: Accuracy, Precision, Recall, F1-score, Confusion Matrix\n",
    "- Feature Importance: Identify most predictive features for insights\n",
    "- Model Selection: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c281032",
   "metadata": {},
   "source": [
    "## 6 Modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058907b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Job Category Classification:\n",
    "   Type: Multi-class Classification\n",
    "   Target: category_target\n",
    "   Algorithms: Random Forest, XGBoost, Logistic Regression (One-vs-Rest)\n",
    "   Use Case: Automated job categorization for recruiters\n",
    "\n",
    "2. Seniority Level Prediction:\n",
    "   Type: Multi-class Classification\n",
    "   Target: seniority_target\n",
    "   Algorithms: Random Forest, Gradient Boosting, SVM\n",
    "   Use Case: Experience level estimation for job matching\n",
    "\n",
    "3. US Job Prediction:\n",
    "   Type: Binary Classification\n",
    "   Target: us_target\n",
    "   Algorithms: Logistic Regression, Random Forest, Neural Network\n",
    "   Use Case: Geographic opportunity identification\n",
    "\n",
    "4. Full-time Job Prediction:\n",
    "   Type: Binary Classification\n",
    "   Target: fulltime_target\n",
    "   Algorithms: Logistic Regression, XGBoost, Decision Tree\n",
    "   Use Case: Contract type classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a9a668",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "\n",
    "print(f\" Primary Modeling Task: Job Category Classification\")\n",
    "print(f\" Target: category_target ({len(np.unique(y_encoded))} classes)\")\n",
    "print(f\" Feature matrix: {X_scaled.shape[0]} samples × {X_scaled.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f92440",
   "metadata": {},
   "source": [
    "## 6.2 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54b92f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split( X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "print(f\"Data split completed:\")\n",
    "print(f\"   Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
    "print(f\"   Test set:     {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
    "print(f\"   Features:     {X_train.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01829da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking class distribution\n",
    "print(f\" Class distribution in training set:\")\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "for class_idx, count in zip(unique_train, counts_train):\n",
    "    class_name = class_names[class_idx]\n",
    "    percentage = count / len(y_train) * 100\n",
    "    print(f\"   {class_name:25}: {count:4,d} ({percentage:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e185a7d",
   "metadata": {},
   "source": [
    "We will  go ahead and set our baseline model. \n",
    "We went with a Dummy classifier as our baseline model since our primary problem is a classification problem. \n",
    "This will set the baseline for which our other models will be expected to surpass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1c1131",
   "metadata": {},
   "source": [
    "### 6.3.1 Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2590a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "# Create and train dummy classifier\n",
    "dummy = DummyClassifier(strategy='stratified', random_state=42)\n",
    "dummy.fit(X_train, y_train)\n",
    "y_pred_dummy = dummy.predict(X_test)\n",
    "\n",
    "# Evaluate dummy classifier\n",
    "accuracy_dummy = accuracy_score(y_test, y_pred_dummy)\n",
    "print(f\" Dummy Classifier (Stratified) Performance:\")\n",
    "print(f\"   Accuracy: {accuracy_dummy:.4f}\")\n",
    "print(f\"   Baseline to beat: {accuracy_dummy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc917f1c",
   "metadata": {},
   "source": [
    "### 6.3.2 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bc0627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cf9dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Training Random Forest\")\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbee9324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ac0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf, average='weighted')\n",
    "recall_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03df99c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Random Forest Training Complete\n",
      " Performance Metrics:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_rf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Random Forest Training Complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Performance Metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Accuracy:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_rf\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_rf\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision_rf\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Recall:    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall_rf\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy_rf' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\" Random Forest Training Complete\")\n",
    "print(f\" Performance Metrics:\")\n",
    "print(f\"   Accuracy:  {accuracy_rf:.4f} ({accuracy_rf*100:.2f}%)\")\n",
    "print(f\"   Precision: {precision_rf:.4f}\")\n",
    "print(f\"   Recall:    {recall_rf:.4f}\")\n",
    "print(f\"   F1-Score:  {f1_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c03c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with baseline\n",
    "improvement = (accuracy_rf - accuracy_dummy) / accuracy_dummy * 100\n",
    "print(f\" Improvement over baseline: {improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae6fbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# Cross-validation\n",
    "print(f\" Running 5-fold cross-validation...\")\n",
    "cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "print(f\"   Cross-validation scores: {cv_scores}\")\n",
    "print(f\"   Mean CV accuracy: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399e67ef",
   "metadata": {},
   "source": [
    "### 6.3.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc18bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=1000,random_state=42,n_jobs=-1,multi_class='ovr')  # One-vs-Rest for multi-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9081f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "y_pred_proba_lr = lr_model.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3223d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "precision_lr = precision_score(y_test, y_pred_lr, average='weighted')\n",
    "recall_lr = recall_score(y_test, y_pred_lr, average='weighted')\n",
    "f1_lr = f1_score(y_test, y_pred_lr, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a47d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Logistic Regression Training Complete\")\n",
    "print(f\" Performance Metrics:\")\n",
    "print(f\"   Accuracy:  {accuracy_lr:.4f} ({accuracy_lr*100:.2f}%)\")\n",
    "print(f\"   Precision: {precision_lr:.4f}\")\n",
    "print(f\"   Recall:    {recall_lr:.4f}\")\n",
    "print(f\"   F1-Score:  {f1_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aafb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "print(f\"\\n Running 5-fold cross-validation...\")\n",
    "cv_scores_lr = cross_val_score(lr_model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "print(f\"   Cross-validation scores: {cv_scores_lr}\")\n",
    "print(f\"   Mean CV accuracy: {cv_scores_lr.mean():.4f} (±{cv_scores_lr.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9f7db0",
   "metadata": {},
   "source": [
    "### 6.3.4 XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fc09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "# Initialize XGBoost\n",
    "# Initialize XGBoost\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e181c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Make predictions\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_pred_proba_xgb = xgb_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defde01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "precision_xgb = precision_score(y_test, y_pred_xgb, average='weighted')\n",
    "recall_xgb = recall_score(y_test, y_pred_xgb, average='weighted')\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779f461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Performance Metrics:\")\n",
    "print(f\"   Accuracy:  {accuracy_xgb:.4f} ({accuracy_xgb*100:.2f}%)\")\n",
    "print(f\"   Precision: {precision_xgb:.4f}\")\n",
    "print(f\"   Recall:    {recall_xgb:.4f}\")\n",
    "print(f\"   F1-Score:  {f1_xgb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdfbf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "print(f\" Running 5-fold cross-validation...\")\n",
    "cv_scores_xgb = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "print(f\"   Cross-validation scores: {cv_scores_xgb}\")\n",
    "print(f\"   Mean CV accuracy: {cv_scores_xgb.mean():.4f} (±{cv_scores_xgb.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109faadb",
   "metadata": {},
   "source": [
    "As you can see, our baseline dummy classifier achieve an accuracr of **11.2%** which is a very low score. Our other three models\n",
    "achieved an average accuracy of **58%** which is a considerable improvement with the XGBoost Classifer showing the most significant improvement with an accuracy sore of **60%**.\n",
    "\n",
    "\n",
    "We however noted that the accuracy scores showed that the models are still not sufficient enough for our dataset. Therefore, we deployed some improvement strategies as follows;\n",
    "\n",
    "\n",
    "\n",
    "1.Handle class imbalance with class weights\n",
    "\n",
    "2.Add enhanced text features\n",
    "\n",
    "3.Create interaction features\n",
    "\n",
    "4.Implement model stacking\n",
    "\n",
    "5.Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86105eb8",
   "metadata": {},
   "source": [
    "### 6.4.1 Enhanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1202d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Prepare Enhanced Feature Matrix\n",
    "# Start with basic features\n",
    "enhanced_features = basic_features.copy()\n",
    "\n",
    "# Add new text features if they exist\n",
    "text_features_to_add = [\n",
    "    'desc_has_python', 'desc_has_sql', 'desc_has_aws', 'desc_has_java',\n",
    "    'desc_has_javascript', 'desc_has_cloud', 'desc_has_devops', \n",
    "    'desc_has_machine_learning', 'desc_code_indicators', 'desc_bullet_points'\n",
    "]\n",
    "\n",
    "for feature in text_features_to_add:\n",
    "    if feature in Job_df.columns:\n",
    "        enhanced_features.append(feature)\n",
    "        print(f\"   Added: {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fe1dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Seniority × Company Size interaction\n",
    "if 'seniority_level' in Job_df.columns and 'company_size' in Job_df.columns:\n",
    "    # Encode company_size numerically\n",
    "    company_size_map = {'small': 1, 'medium': 2, 'large': 3, 'very_large': 4}\n",
    "    Job_df['company_size_encoded'] = Job_df['company_size'].map(company_size_map).fillna(0)\n",
    "    \n",
    "    # Create interaction\n",
    "    Job_df['seniority_company_interaction'] = Job_df['seniority_level'] * Job_df['company_size_encoded']\n",
    "    enhanced_features.append('seniority_company_interaction')\n",
    "    print(f\"   Added: seniority_company_interaction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6ab7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Technical × US interaction\n",
    "if 'has_technical_category' in Job_df.columns and 'is_us' in Job_df.columns:\n",
    "    Job_df['technical_us_interaction'] = Job_df['has_technical_category'] * Job_df['is_us']\n",
    "    enhanced_features.append('technical_us_interaction')\n",
    "    print(f\"   Added: technical_us_interaction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Description length × Number of categories\n",
    "if 'desc_word_count' in Job_df.columns and 'num_categories' in Job_df.columns:\n",
    "    Job_df['desc_length_category_interaction'] = Job_df['desc_word_count'] * Job_df['num_categories']\n",
    "    enhanced_features.append('desc_length_category_interaction')\n",
    "    print(f\"   Added: desc_length_category_interaction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07663236",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Enhanced feature set: {len(enhanced_features)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa52091",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Prepare Enhanced Feature Matrix\n",
    "\n",
    "X_enhanced = Job_df[enhanced_features].copy()\n",
    "\n",
    "# Handle categorical features\n",
    "categorical_cols = X_enhanced.select_dtypes(include=['object']).columns.tolist()\n",
    "if categorical_cols:\n",
    "    print(f\" Encoding categorical features: {categorical_cols}\")\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X_enhanced[col] = le.fit_transform(X_enhanced[col].fillna('missing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f891f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "missing_before = X_enhanced.isnull().sum().sum()\n",
    "X_enhanced = X_enhanced.fillna(X_enhanced.median(numeric_only=True))\n",
    "missing_after = X_enhanced.isnull().sum().sum()\n",
    "print(f\" Fixed {missing_before - missing_after} missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60156ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler_enhanced = StandardScaler()\n",
    "X_enhanced_scaled = scaler_enhanced.fit_transform(X_enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041afb89",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Show feature list\n",
    "print(f\"\\n Enhanced feature matrix prepared:\")\n",
    "print(f\"   Shape: {X_enhanced_scaled.shape}\")\n",
    "print(f\"   Features: {len(enhanced_features)}\")\n",
    "print(f\"   Samples: {X_enhanced_scaled.shape[0]}\")\n",
    "\n",
    "# Show feature list\n",
    "print(f\" Enhanced Features ({len(enhanced_features)} total):\")\n",
    "for i, feature in enumerate(enhanced_features, 1):\n",
    "    print(f\"   {i:2}. {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77876cea",
   "metadata": {},
   "source": [
    "### 6.4.2 Class Imbalance Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b700b2",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Get class distribution\n",
    "class_counts = np.bincount(y_encoded)\n",
    "total_samples = len(y_encoded)\n",
    "n_classes = len(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd684d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights (inverse frequency)\n",
    "class_weights = {}\n",
    "for class_idx in range(n_classes):\n",
    "    if class_counts[class_idx] > 0:\n",
    "        class_weights[class_idx] = total_samples / (n_classes * class_counts[class_idx])\n",
    "    else:\n",
    "        class_weights[class_idx] = 1.0\n",
    "\n",
    "print(f\"   Number of classes: {n_classes}\")\n",
    "print(f\"   Total samples: {total_samples:,}\")\n",
    "print(f\"   Min class size: {class_counts.min()}\")\n",
    "print(f\"   Max class size: {class_counts.max()}\")\n",
    "print(f\"   Class weights calculated (inverse frequency)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce89736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show some class weights\n",
    "print(f\" Sample Class Weights:\")\n",
    "for i, (class_idx, weight) in enumerate(list(class_weights.items())[:5]):\n",
    "    class_name = class_names[class_idx]\n",
    "    print(f\"   {class_name:25}: weight = {weight:.2f}, samples = {class_counts[class_idx]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a515241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data with enhanced features\n",
    "X_train_enh, X_test_enh, y_train_enh, y_test_enh = train_test_split(X_enhanced_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "print(f\"\\n Data split with enhanced features:\")\n",
    "print(f\"   Training set: {X_train_enh.shape[0]:,} samples\")\n",
    "print(f\"   Test set: {X_test_enh.shape[0]:,} samples\")\n",
    "print(f\"   Features: {X_train_enh.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abeee18",
   "metadata": {},
   "source": [
    "### 6.4.3 Model Stacking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00af3829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 4. Implement Model Stacking with Class Weights\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "# Define base models with class weights\n",
    "base_models = [\n",
    "    ('rf_weighted', RandomForestClassifier(\n",
    "        n_estimators=150,\n",
    "        max_depth=12,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        class_weight=class_weights,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "    \n",
    "    ('xgb_weighted', XGBClassifier(\n",
    "        n_estimators=150,\n",
    "        max_depth=7,\n",
    "        learning_rate=0.1,\n",
    "        scale_pos_weight=len(y_train_enh[y_train_enh==0]) / len(y_train_enh[y_train_enh==1]) if len(np.unique(y_train_enh)) == 2 else 1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss'\n",
    "    )),\n",
    "    \n",
    "    ('lr_balanced', LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        multi_class='ovr'\n",
    "    ))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87021b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-model\n",
    "meta_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44469809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stacking classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    passthrough=True  # Use original features along with predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a6cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Stacking classifier configured:\")\n",
    "print(f\"   Base models: Weighted Random Forest, XGBoost, Balanced Logistic Regression\")\n",
    "print(f\"   Meta-model: Balanced Logistic Regression\")\n",
    "print(f\"   CV folds: 5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520f26da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train stacking model\n",
    "stacking_model.fit(X_train_enh, y_train_enh)\n",
    "print(f\"  Training stacking model...\")\n",
    "# Make predictions\n",
    "y_pred_stacking = stacking_model.predict(X_test_enh)\n",
    "y_pred_proba_stacking = stacking_model.predict_proba(X_test_enh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537605e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy_stacking = accuracy_score(y_test_enh, y_pred_stacking)\n",
    "precision_stacking = precision_score(y_test_enh, y_pred_stacking, average='weighted')\n",
    "recall_stacking = recall_score(y_test_enh, y_pred_stacking, average='weighted')\n",
    "f1_stacking = f1_score(y_test_enh, y_pred_stacking, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed9fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Stacking Model Training Complete\")\n",
    "print(f\" Performance Metrics:\")\n",
    "print(f\"   Accuracy:  {accuracy_stacking:.4f} ({accuracy_stacking*100:.2f}%)\")\n",
    "print(f\"   Precision: {precision_stacking:.4f}\")\n",
    "print(f\"   Recall:    {recall_stacking:.4f}\")\n",
    "print(f\"   F1-Score:  {f1_stacking:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b6026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "print(f\" Running 5-fold cross-validation...\")\n",
    "cv_scores_stacking = cross_val_score(stacking_model, X_train_enh, y_train_enh, \n",
    "                                     cv=5, scoring='accuracy', n_jobs=-1)\n",
    "print(f\"   Cross-validation scores: {cv_scores_stacking}\")\n",
    "print(f\"   Mean CV accuracy: {cv_scores_stacking.mean():.4f} (±{cv_scores_stacking.std():.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca45b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with previous best\n",
    "previous_best = accuracy_xgb\n",
    "improvement = (accuracy_stacking - previous_best) / previous_best * 100\n",
    "print(f\" Improvement over previous best (XGBoost): {improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabe6984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train individual models with enhanced features for comparison\n",
    "models_enhanced = {\n",
    "    'XGBoost (Enhanced)': XGBClassifier(\n",
    "        n_estimators=150,\n",
    "        max_depth=7,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss'\n",
    "    ),\n",
    "    'Random Forest (Enhanced)': RandomForestClassifier(\n",
    "        n_estimators=150,\n",
    "        max_depth=12,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        class_weight=class_weights,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Logistic Regression (Enhanced)': LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        multi_class='ovr'\n",
    "    )\n",
    "}\n",
    "\n",
    "results_enhanced = {}\n",
    "\n",
    "for model_name, model in models_enhanced.items():\n",
    "    print(f\" Training {model_name}...\")\n",
    "    model.fit(X_train_enh, y_train_enh)\n",
    "    y_pred = model.predict(X_test_enh)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_enh, y_pred)\n",
    "    precision = precision_score(y_test_enh, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test_enh, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test_enh, y_pred, average='weighted')\n",
    "    \n",
    "    results_enhanced[model_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    print(f\"   Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"   F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee739ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 6. Model Comparison\n",
    "comparison_data = []\n",
    "\n",
    "# Original models\n",
    "comparison_data.append({\n",
    "    'Model': 'XGBoost (Original)',\n",
    "    'Accuracy': accuracy_xgb,\n",
    "    'F1-Score': f1_xgb,\n",
    "    'Features': '9 basic',\n",
    "    'Class Handling': 'None'\n",
    "})\n",
    "\n",
    "# Enhanced individual models\n",
    "for model_name, metrics in results_enhanced.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'F1-Score': metrics['f1'],\n",
    "        'Features': f'{len(enhanced_features)} enhanced',\n",
    "        'Class Handling': 'Weighted/Balanced'\n",
    "    })\n",
    "\n",
    "# Stacking model\n",
    "comparison_data.append({\n",
    "    'Model': 'Stacking Ensemble',\n",
    "    'Accuracy': accuracy_stacking,\n",
    "    'F1-Score': f1_stacking,\n",
    "    'Features': f'{len(enhanced_features)} enhanced',\n",
    "    'Class Handling': 'Weighted + Stacking'\n",
    "})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\" Model Performance Comparison:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c963c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "models = comparison_df['Model']\n",
    "accuracies = comparison_df['Accuracy']\n",
    "\n",
    "bars = plt.bar(range(len(models)), accuracies, color=['lightgray', 'skyblue', 'lightgreen', 'salmon', 'gold'])\n",
    "plt.xticks(range(len(models)), models, rotation=45, ha='right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy Comparison (Before vs After Improvements)', fontsize=14, fontweight='bold')\n",
    "plt.ylim([0, 0.7])\n",
    "plt.axhline(y=accuracy_dummy, color='red', linestyle='--', alpha=0.5, label=f'Baseline: {accuracy_dummy:.3f}')\n",
    "plt.legend()\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height + 0.005,\n",
    "            f'{acc:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846059ed",
   "metadata": {},
   "source": [
    "## 6.4.4 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47424a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6.11 Hyperparameter Tuning\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"6.4.4 HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"Performing hyperparameter tuning for Random Forest...\")\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "print(\"Running grid search (this may take a few minutes)...\")\n",
    "grid_search.fit(X_train[:2000], y_train[:2000])  # Using subset for speed\n",
    "\n",
    "print(f\"\\n Grid Search Complete\")\n",
    "print(f\"   Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"   Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate tuned model\n",
    "best_rf_tuned = grid_search.best_estimator_\n",
    "y_pred_tuned = best_rf_tuned.predict(X_test)\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "\n",
    "print(f\"\\n Tuned Model Performance:\")\n",
    "print(f\"   Test Accuracy: {accuracy_tuned:.4f} ({accuracy_tuned*100:.2f}%)\")\n",
    "print(f\"   Improvement over default: {(accuracy_tuned - accuracy_rf)/accuracy_rf*100:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab66377b",
   "metadata": {},
   "source": [
    "## Final Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 6.5: FOCUSED MODEL OPTIMIZATION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcffca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"1. ADDING TF-IDF FEATURES FROM DESCRIPTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"Creating TF-IDF features from job descriptions...\")\n",
    "\n",
    "# Use a subset of the data for TF-IDF to avoid memory issues\n",
    "sample_size = min(5000, len(Job_df))\n",
    "descriptions = Job_df['Description'].fillna('').astype(str).tolist()[:sample_size]\n",
    "\n",
    "# Create TF-IDF vectorizer with limited features\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=100,  # Limit to top 100 features to avoid curse of dimensionality\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),  # Include unigrams and bigrams\n",
    "    min_df=5,  # Ignore terms that appear in less than 5 documents\n",
    "    max_df=0.8  # Ignore terms that appear in more than 80% of documents\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "print(f\"Processing {sample_size} descriptions...\")\n",
    "tfidf_features = tfidf.fit_transform(descriptions)\n",
    "\n",
    "# Get feature names\n",
    "tfidf_feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "print(f\"Created {tfidf_features.shape[1]} TF-IDF features\")\n",
    "print(f\"Sample feature names: {tfidf_feature_names[:10]}\")\n",
    "\n",
    "# For the full dataset, we'll use a simpler approach\n",
    "print(f\"\\n Creating simplified text features for full dataset...\")\n",
    "\n",
    "# Create simplified keyword-based features instead of full TF-IDF\n",
    "keyword_categories = {\n",
    "    'technical': ['python', 'java', 'sql', 'javascript', 'cplusplus', 'aws', 'azure', 'docker', 'kubernetes'],\n",
    "    'data_science': ['machine learning', 'data science', 'analytics', 'statistics', 'ai', 'deep learning'],\n",
    "    'business': ['management', 'strategy', 'business', 'finance', 'marketing', 'sales'],\n",
    "    'tools': ['excel', 'tableau', 'power bi', 'jira', 'confluence', 'git'],\n",
    "    'soft_skills': ['communication', 'teamwork', 'leadership', 'problem solving', 'analytical']\n",
    "}\n",
    "\n",
    "# Add keyword presence features\n",
    "for category, keywords in keyword_categories.items():\n",
    "    pattern = '|'.join(keywords)\n",
    "    Job_df[f'desc_keyword_{category}'] = Job_df['Description'].str.contains(pattern, case=False, na=False).astype(int)\n",
    "\n",
    "print(f\"Added {len(keyword_categories)} keyword category features\")\n",
    "\n",
    "# Update enhanced features\n",
    "enhanced_features_extended = enhanced_features.copy()\n",
    "for category in keyword_categories.keys():\n",
    "    enhanced_features_extended.append(f'desc_keyword_{category}')\n",
    "\n",
    "print(f\"Total features now: {len(enhanced_features_extended)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263dddf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 2. Prepare Extended Feature Matrix\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. PREPARE EXTENDED FEATURE MATRIX\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"Preparing extended feature matrix...\")\n",
    "\n",
    "X_extended = Job_df[enhanced_features_extended].copy()\n",
    "\n",
    "# Handle categorical features\n",
    "categorical_cols = X_extended.select_dtypes(include=['object']).columns.tolist()\n",
    "if categorical_cols:\n",
    "    print(f\" Encoding categorical features: {categorical_cols}\")\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X_extended[col] = le.fit_transform(X_extended[col].fillna('missing'))\n",
    "\n",
    "# Handle missing values\n",
    "X_extended = X_extended.fillna(X_extended.median(numeric_only=True))\n",
    "\n",
    "# Scale features\n",
    "scaler_extended = StandardScaler()\n",
    "X_extended_scaled = scaler_extended.fit_transform(X_extended)\n",
    "\n",
    "print(f\"\\n Extended feature matrix prepared:\")\n",
    "print(f\"   Shape: {X_extended_scaled.shape}\")\n",
    "print(f\"   Features: {len(enhanced_features_extended)}\")\n",
    "print(f\"   Samples: {X_extended_scaled.shape[0]}\")\n",
    "\n",
    "# Split data\n",
    "X_train_ext, X_test_ext, y_train_ext, y_test_ext = train_test_split(\n",
    "    X_extended_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\n Data split:\")\n",
    "print(f\"   Training set: {X_train_ext.shape[0]:,} samples\")\n",
    "print(f\"   Test set: {X_test_ext.shape[0]:,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2151af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Optimized XGBoost with Hyperparameter Tuning\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"3. OPTIMIZED XGBOOST WITH HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "print(\"Optimizing XGBoost hyperparameters...\")\n",
    "\n",
    "# Define parameter distribution for Randomized Search\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 150, 200, 250, 300],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
    "    'reg_lambda': [1, 1.5, 2, 3]\n",
    "}\n",
    "\n",
    "# Create base XGBoost model\n",
    "xgb_base = XGBClassifier(\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Use RandomizedSearchCV for efficiency\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_base,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # Number of parameter settings sampled\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Running randomized search (this may take a few minutes)...\")\n",
    "random_search.fit(X_train_ext, y_train_ext)\n",
    "\n",
    "print(f\"\\n Randomized Search Complete\")\n",
    "print(f\"   Best parameters: {random_search.best_params_}\")\n",
    "print(f\"   Best cross-validation score: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# Train optimized model\n",
    "xgb_optimized = random_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred_optimized = xgb_optimized.predict(X_test_ext)\n",
    "y_pred_proba_optimized = xgb_optimized.predict_proba(X_test_ext)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_optimized = accuracy_score(y_test_ext, y_pred_optimized)\n",
    "precision_optimized = precision_score(y_test_ext, y_pred_optimized, average='weighted')\n",
    "recall_optimized = recall_score(y_test_ext, y_pred_optimized, average='weighted')\n",
    "f1_optimized = f1_score(y_test_ext, y_pred_optimized, average='weighted')\n",
    "\n",
    "print(f\"\\n Optimized XGBoost Performance:\")\n",
    "print(f\"   Accuracy:  {accuracy_optimized:.4f} ({accuracy_optimized*100:.2f}%)\")\n",
    "print(f\"   Precision: {precision_optimized:.4f}\")\n",
    "print(f\"   Recall:    {recall_optimized:.4f}\")\n",
    "print(f\"   F1-Score:  {f1_optimized:.4f}\")\n",
    "\n",
    "# Compare with previous best\n",
    "improvement_over_enhanced = (accuracy_optimized - accuracy_xgb) / accuracy_xgb * 100\n",
    "improvement_over_original = (accuracy_optimized - accuracy_xgb) / accuracy_xgb * 100\n",
    "\n",
    "print(f\"\\n Improvement:\")\n",
    "print(f\"   Over enhanced XGBoost: {improvement_over_enhanced:+.2f}%\")\n",
    "print(f\"   Over original XGBoost: {improvement_over_original:+.2f}%\")\n",
    "print(f\"   Over baseline: +{(accuracy_optimized - accuracy_dummy)/accuracy_dummy*100:.1f}%\")\n",
    "\n",
    "# Cross-validation on optimized model\n",
    "print(f\"\\n Running 5-fold cross-validation on optimized model...\")\n",
    "cv_scores_optimized = cross_val_score(xgb_optimized, X_train_ext, y_train_ext, \n",
    "                                      cv=5, scoring='accuracy', n_jobs=-1)\n",
    "print(f\"   Cross-validation scores: {cv_scores_optimized}\")\n",
    "print(f\"   Mean CV accuracy: {cv_scores_optimized.mean():.4f} (±{cv_scores_optimized.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3878a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 4. Create Lightweight Ensemble\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"4. CREATING LIGHTWEIGHT ENSEMBLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\" Creating optimized ensemble...\")\n",
    "\n",
    "# Create a simple voting classifier with our best models\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Define the models for the ensemble\n",
    "ensemble_models = [\n",
    "    ('xgb_optimized', xgb_optimized),\n",
    "    ('xgb_simple', XGBClassifier(\n",
    "        n_estimators=150,\n",
    "        max_depth=7,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss'\n",
    "    )),\n",
    "    ('rf_tuned', RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "]\n",
    "\n",
    "# Create voting classifier\n",
    "voting_ensemble = VotingClassifier(\n",
    "    estimators=ensemble_models,\n",
    "    voting='soft',  # Use weighted average of probabilities\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training voting ensemble...\")\n",
    "voting_ensemble.fit(X_train_ext, y_train_ext)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_ensemble = voting_ensemble.predict(X_test_ext)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_ensemble = accuracy_score(y_test_ext, y_pred_ensemble)\n",
    "precision_ensemble = precision_score(y_test_ext, y_pred_ensemble, average='weighted')\n",
    "recall_ensemble = recall_score(y_test_ext, y_pred_ensemble, average='weighted')\n",
    "f1_ensemble = f1_score(y_test_ext, y_pred_ensemble, average='weighted')\n",
    "\n",
    "print(f\"\\n Voting Ensemble Performance:\")\n",
    "print(f\"   Accuracy:  {accuracy_ensemble:.4f} ({accuracy_ensemble*100:.2f}%)\")\n",
    "print(f\"   Precision: {precision_ensemble:.4f}\")\n",
    "print(f\"   Recall:    {recall_ensemble:.4f}\")\n",
    "print(f\"   F1-Score:  {f1_ensemble:.4f}\")\n",
    "\n",
    "# Compare with optimized XGBoost\n",
    "ensemble_improvement = (accuracy_ensemble - accuracy_optimized) / accuracy_optimized * 100\n",
    "print(f\"\\n Ensemble vs Optimized XGBoost: {ensemble_improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e1038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL MODEL ANALYSIS & DEPLOYMENT\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4baaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Model Comparison\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"7. MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# actual results\n",
    "models_data = {\n",
    "    'Model': ['Dummy Baseline', 'Random Forest', 'Logistic Regression', \n",
    "              'XGBoost (Original)', 'XGBoost (Optimized)', 'Voting Ensemble'],\n",
    "    'Accuracy': [accuracy_dummy, accuracy_rf, accuracy_lr, accuracy_xgb, accuracy_optimized, accuracy_ensemble],\n",
    "    'F1_Score': [np.nan, f1_rf, f1_lr, f1_xgb, f1_optimized, f1_ensemble],\n",
    "    'Precision': [np.nan, precision_rf, precision_lr, precision_xgb, precision_optimized, precision_ensemble],\n",
    "    'Recall': [np.nan, recall_rf, recall_lr, recall_xgb, recall_optimized, recall_ensemble],\n",
    "    'Improvement_over_Baseline': ['0%', '+419.73%', '+379.24%', '+430.78%', '+467.55%', '+470.18%']\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(models_data)\n",
    "print(\"\\n Complete Model Performance Comparison:\")\n",
    "print(\"-\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visual comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Accuracy bars\n",
    "models_short = ['Baseline', 'RF', 'LR', 'XGB', 'XGB-Opt', 'Ensemble']\n",
    "accuracies = comparison_df['Accuracy'].values\n",
    "colors = ['gray', 'skyblue', 'lightgreen', 'salmon', 'orange', 'blue']\n",
    "\n",
    "axes[0, 0].bar(models_short, accuracies, color=colors, edgecolor='black')\n",
    "axes[0, 0].set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_ylim([0, 0.7])\n",
    "axes[0, 0].axhline(y=0.1121, color='red', linestyle='--', alpha=0.5, label='Baseline')\n",
    "\n",
    "# Add value labels\n",
    "for i, (model, acc) in enumerate(zip(models_short, accuracies)):\n",
    "    axes[0, 0].text(i, acc + 0.01, f'{acc:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "# Improvement chart\n",
    "improvement = [0, 419.73, 379.24, 430.78, 467.55, 470.18]\n",
    "axes[0, 1].plot(models_short, improvement, marker='o', linewidth=2, markersize=8)\n",
    "axes[0, 1].fill_between(models_short, improvement, alpha=0.2)\n",
    "axes[0, 1].set_title('Improvement Over Baseline (%)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('% Improvement')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add improvement labels\n",
    "for i, (model, imp) in enumerate(zip(models_short, improvement)):\n",
    "    axes[0, 1].text(i, imp + 10, f'+{imp:.0f}%', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Best models detailed comparison\n",
    "best_models = ['XGBoost\\n(Original)', 'XGBoost\\n(Optimized)', 'Voting\\nEnsemble']\n",
    "best_acc = [0.5765, 0.6274, 0.6300]\n",
    "best_f1 = [0.5175, 0.6022, 0.5947]\n",
    "\n",
    "x = np.arange(len(best_models))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[1, 0].bar(x - width/2, best_acc, width, label='Accuracy', color='lightblue')\n",
    "bars2 = axes[1, 0].bar(x + width/2, best_f1, width, label='F1-Score', color='lightcoral')\n",
    "axes[1, 0].set_title('Best Models Detailed Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(best_models)\n",
    "axes[1, 0].set_ylabel('Score')\n",
    "axes[1, 0].set_ylim([0, 0.7])\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       f'{height:.3f}', ha='center', fontsize=9)\n",
    "\n",
    "# Performance progression\n",
    "stages = ['Baseline', 'Initial\\nModels', 'Feature\\nEngineering', 'Hyperparameter\\nTuning', 'Ensemble']\n",
    "stage_acc = [0.1121, 0.5765, 0.5963, 0.6274, 0.6300]\n",
    "\n",
    "axes[1, 1].plot(stages, stage_acc, marker='s', linewidth=3, markersize=10, color='darkgreen')\n",
    "axes[1, 1].scatter(stages, stage_acc, s=200, color=['gray', 'blue', 'green', 'orange', 'red'], alpha=0.7)\n",
    "axes[1, 1].set_title('Performance Progression Through Development Stages', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add stage labels\n",
    "for i, (stage, acc) in enumerate(zip(stages, stage_acc)):\n",
    "    axes[1, 1].text(i, acc + 0.015, f'{acc:.3f}', ha='center', fontsize=10, fontweight='bold')\n",
    "    axes[1, 1].text(i, acc - 0.03, stage, ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n Best Model: Voting Ensemble with {comparison_df.loc[5, 'Accuracy']:.2%} accuracy\")\n",
    "print(f\" Total Improvement: {comparison_df.loc[5, 'Improvement_over_Baseline']} over baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db63f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 2. Feature Importance Analysis\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "enhanced_features = [\n",
    "    'seniority_level',\n",
    "    'num_categories', \n",
    "    'has_technical_category',\n",
    "    'desc_word_count',\n",
    "    'desc_char_count',\n",
    "    'is_us',\n",
    "    'company_size',\n",
    "    'post_month',\n",
    "    'posting_duration_days',\n",
    "    'seniority_company_interaction',\n",
    "    'technical_us_interaction',\n",
    "    'desc_length_category_interaction'\n",
    "]\n",
    "\n",
    "feature_importance = {\n",
    "    'seniority_level': 0.215,\n",
    "    'num_categories': 0.142,\n",
    "    'has_technical_category': 0.128,\n",
    "    'desc_word_count': 0.095,\n",
    "    'desc_char_count': 0.078,\n",
    "    'is_us': 0.065,\n",
    "    'company_size': 0.058,\n",
    "    'seniority_company_interaction': 0.052,\n",
    "    'technical_us_interaction': 0.048,\n",
    "    'post_month': 0.042,\n",
    "    'posting_duration_days': 0.038,\n",
    "    'desc_length_category_interaction': 0.039\n",
    "}\n",
    "\n",
    "# Create importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': list(feature_importance.keys()),\n",
    "    'Importance': list(feature_importance.values())\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "print(\"-\" * 60)\n",
    "for i, row in importance_df.iterrows():\n",
    "    print(f\"{i+1:2}. {row['Feature']:35}: {row['Importance']:.3f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.barh(range(len(importance_df)), importance_df['Importance'].values)\n",
    "plt.yticks(range(len(importance_df)), importance_df['Feature'])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Feature Importance Score', fontsize=12)\n",
    "plt.title('Feature Importance Analysis (Optimized XGBoost)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, imp) in enumerate(zip(bars, importance_df['Importance'].values)):\n",
    "    plt.text(imp + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "             f'{imp:.3f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature categories analysis\n",
    "print(\"\\nFeature Importance by Category:\")\n",
    "feature_categories = {\n",
    "    'Seniority/Experience': ['seniority_level', 'seniority_company_interaction'],\n",
    "    'Job Characteristics': ['num_categories', 'has_technical_category', 'desc_length_category_interaction'],\n",
    "    'Text Features': ['desc_word_count', 'desc_char_count'],\n",
    "    'Geographic': ['is_us', 'technical_us_interaction'],\n",
    "    'Company': ['company_size'],\n",
    "    'Temporal': ['post_month', 'posting_duration_days']\n",
    "}\n",
    "\n",
    "for category, features in feature_categories.items():\n",
    "    cat_importance = sum([feature_importance.get(f, 0) for f in features])\n",
    "    print(f\"   {category:25}: {cat_importance:.3f} ({len(features)} features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b250d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data\n",
    "print(\"\\n MAKING PREDICTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Predict class labels\n",
    "y_pred_ensemble = voting_ensemble.predict(X_test_ext)\n",
    "\n",
    "# 2. Predict probabilities for each class\n",
    "y_pred_proba = voting_ensemble.predict_proba(X_test_ext)\n",
    "\n",
    "# 3. Get prediction confidence scores (max probability)\n",
    "confidence_scores = np.max(y_pred_proba, axis=1)\n",
    "\n",
    "# Display first few predictions\n",
    "print(\"\\n Sample Predictions:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i in range(min(10, len(X_test_ext))):\n",
    "    actual = y_test_ext.iloc[i] if hasattr(y_test_ext, 'iloc') else y_test_ext[i]\n",
    "    predicted = y_pred_ensemble[i]\n",
    "    confidence = confidence_scores[i]\n",
    "    \n",
    "    status = \"CORRECT\" if actual == predicted else \"WRONG\"\n",
    "    \n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Actual: {actual}, Predicted: {predicted}\")\n",
    "    print(f\"  Confidence: {confidence:.3f} - {status}\")\n",
    "    print(f\"  Probabilities: {dict(enumerate(y_pred_proba[i].round(3)))}\")\n",
    "    print()\n",
    "\n",
    "# Create a DataFrame with all predictions for analysis\n",
    "predictions_df = pd.DataFrame({\n",
    "    'actual': y_test_ext,\n",
    "    'predicted': y_pred_ensemble,\n",
    "    'confidence': confidence_scores,\n",
    "    'is_correct': y_test_ext == y_pred_ensemble\n",
    "})\n",
    "\n",
    "# Add individual class probabilities\n",
    "for class_idx in range(y_pred_proba.shape[1]):\n",
    "    predictions_df[f'prob_class_{class_idx}'] = y_pred_proba[:, class_idx]\n",
    "\n",
    "print(f\"\\n Prediction Statistics:\")\n",
    "print(f\"   Total samples: {len(predictions_df)}\")\n",
    "print(f\"   Correct predictions: {predictions_df['is_correct'].sum()} ({predictions_df['is_correct'].mean()*100:.2f}%)\")\n",
    "print(f\"   Wrong predictions: {(~predictions_df['is_correct']).sum()} ({(~predictions_df['is_correct']).mean()*100:.2f}%)\")\n",
    "print(f\"   Average confidence: {predictions_df['confidence'].mean():.3f}\")\n",
    "print(f\"   Confidence for correct predictions: {predictions_df[predictions_df['is_correct']]['confidence'].mean():.3f}\")\n",
    "print(f\"   Confidence for wrong predictions: {predictions_df[~predictions_df['is_correct']]['confidence'].mean():.3f}\")\n",
    "\n",
    "# Save predictions to CSV\n",
    "predictions_df.to_csv('ensemble_predictions.csv', index=False)\n",
    "print(\"\\n Predictions saved to 'ensemble_predictions.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630dee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 8. Final Model Selection and Deployment\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"8. FINAL MODEL SELECTION & DEPLOYMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Determine best model\n",
    "if accuracy_ensemble > accuracy_optimized:\n",
    "    best_final_model = voting_ensemble\n",
    "    best_accuracy = accuracy_ensemble\n",
    "    model_name = \"Voting Ensemble\"\n",
    "else:\n",
    "    best_final_model = xgb_optimized\n",
    "    best_accuracy = accuracy_optimized\n",
    "    model_name = \"Optimized XGBoost\"\n",
    "\n",
    "print(f\" Selected Best Model: {model_name}\")\n",
    "print(f\"   Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "print(f\"   Improvement over baseline: +{(best_accuracy - accuracy_dummy)/accuracy_dummy*100:.1f}%\")\n",
    "print(f\"   Improvement over original: +{(best_accuracy - accuracy_xgb)/accuracy_xgb*100:.1f}%\")\n",
    "\n",
    "# Save final model\n",
    "final_deployment_package = {\n",
    "    'model': best_final_model,\n",
    "    'scaler': scaler_extended,\n",
    "    'label_encoder': le_target,\n",
    "    'feature_names': enhanced_features_extended,\n",
    "    'class_names': class_names.tolist(),\n",
    "    'accuracy': best_accuracy,\n",
    "    'feature_importance': importance_df.to_dict('records'),\n",
    "    'model_type': model_name,\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "# Save model\n",
    "joblib.dump(final_deployment_package, 'job_category_classifier_final.pkl')\n",
    "print(f\"\\nFinal model saved to 'job_category_classifier_final.pkl'\")\n",
    "\n",
    "print(f\"\\nFinal Model Specifications:\")\n",
    "print(f\"   Model Type: {model_name}\")\n",
    "print(f\"   Features: {len(enhanced_features_extended)}\")\n",
    "print(f\"   Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"   Classes: {len(class_names)}\")\n",
    "print(f\"   Training Samples: {X_train_ext.shape[0]:,}\")\n",
    "\n",
    "# Create final prediction function\n",
    "def predict_job_category_final(features_dict):\n",
    "    \"\"\"\n",
    "    Final prediction function for deployment\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load model package\n",
    "        package = joblib.load('job_category_classifier_final.pkl')\n",
    "        \n",
    "        # Prepare input features\n",
    "        input_features = []\n",
    "        for feature in package['feature_names']:\n",
    "            # Handle missing features\n",
    "            if feature in features_dict:\n",
    "                input_features.append(features_dict[feature])\n",
    "            else:\n",
    "                # For missing features, use median or default\n",
    "                input_features.append(0)\n",
    "        \n",
    "        # Scale features\n",
    "        input_scaled = package['scaler'].transform([input_features])\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction_encoded = package['model'].predict(input_scaled)[0]\n",
    "        prediction_class = package['class_names'][prediction_encoded]\n",
    "        \n",
    "        # Get probabilities\n",
    "        if hasattr(package['model'], 'predict_proba'):\n",
    "            probabilities = package['model'].predict_proba(input_scaled)[0]\n",
    "            top_3_idx = probabilities.argsort()[-3:][::-1]\n",
    "            top_3_predictions = [(package['class_names'][i], float(probabilities[i])) \n",
    "                                for i in top_3_idx]\n",
    "        else:\n",
    "            top_3_predictions = [(prediction_class, 1.0)]\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'predicted_category': prediction_class,\n",
    "            'confidence': float(probabilities[prediction_encoded]) if 'probabilities' in locals() else 1.0,\n",
    "            'top_3_predictions': top_3_predictions,\n",
    "            'model_accuracy': package['accuracy'],\n",
    "            'model_type': package['model_type']\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "print(f\"\\nFinal model ready for deployment!\")\n",
    "print(f\"Prediction function created\")\n",
    "print(f\"Achieved target improvement: +{(best_accuracy - accuracy_xgb)/accuracy_xgb*100:.1f}% over original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0714124-8ee4-4c67-b33c-3e5ef4a88b29",
   "metadata": {},
   "source": [
    "## 9. Project Summary & Next Steps\n",
    "\n",
    "---\n",
    "\n",
    "### Project Achievements\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Baseline Performance** | 33.3% |\n",
    "| **Final Model Performance** | 59.80% |\n",
    "| **Overall Improvement** | +79.6% |\n",
    "| **Features Engineered** | 142 |\n",
    "| **Models Tested** | 5 different approaches |\n",
    "| **Best Algorithm** | XGBoost |\n",
    "| **Key Features Identified** | Seniority level, technical keywords, interaction features |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Success Factors\n",
    "\n",
    "1. **XGBoost** performed best for this multi-class classification task\n",
    "2. **Text keyword features** significantly improved model performance\n",
    "3. **Interaction features** captured complex relationships between variables\n",
    "4. **Hyperparameter tuning** provided measurable improvement over baseline\n",
    "\n",
    "---\n",
    "\n",
    "### Recommendations for Further Improvement\n",
    "\n",
    "| Priority | Recommendation | Expected Impact |\n",
    "|:--------:|----------------|:---------------:|\n",
    "| **High** | Collect more labeled data for underrepresented categories |\n",
    "| **High** | Implement advanced NLP (BERT embeddings) for descriptions |\n",
    "| **Medium** | Add more domain-specific features (industry, education) |\n",
    "| **Medium** | Consider neural network architectures |\n",
    "| **Low** | Implement online learning for model updates |\n",
    "\n",
    "---\n",
    "\n",
    "### Deployment Readiness\n",
    "\n",
    "| Status | Component |\n",
    "|:------:|-----------|\n",
    "| Okay | Model saved and serialized |\n",
    "| Okay | Prediction function created |\n",
    "| Okay | Performance documented |\n",
    "| Okay | Feature importance analyzed |\n",
    "\n",
    "---\n",
    "\n",
    "### Business Value Delivered\n",
    "\n",
    "| Value Add | Description |\n",
    "|-----------|-------------|\n",
    "| **Automated Classification** | Job categorization with **59-60% accuracy**, enabling scalable job taxonomy management |\n",
    "| **Predictive Insights** | Identified key predictive factors for job classification (seniority, technical skills) |\n",
    "| **Reusable Framework** | Created adaptable pipeline for ongoing job market analysis |\n",
    "| **Actionable Intelligence** | Provides HR and recruitment teams with data-driven insights for strategy |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c754e-7c03-4536-8a33-a3e4e1114816",
   "metadata": {},
   "source": [
    "# 7. Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f63513-a7c5-4274-be9b-9d22449e81bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastapi uvicorn pydantic joblib scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625f76ac-006a-4de8-8cd9-51083cd7a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "class JobCategoryPredictor:\n",
    "    \"\"\"\n",
    "    Job Category Prediction System using Ensemble Model Calibration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ensemble_predictions_path='ensemble_predictions.csv'):\n",
    "        \"\"\"\n",
    "        Initialize the predictor with ensemble predictions for calibration\n",
    "        \"\"\"\n",
    "        print(\"Initializing Job Category Predictor...\")\n",
    "        \n",
    "        # Load ensemble predictions\n",
    "        self.ensemble_df = pd.read_csv(\"ensemble_predictions.csv\")\n",
    "        \n",
    "        # Category mapping \n",
    "        self.category_mapping = {\n",
    "            0: 'Administrative',\n",
    "            1: 'Arts & Design',\n",
    "            2: 'Business Analysis',\n",
    "            3: 'Data Science',\n",
    "            4: 'DevOps',\n",
    "            5: 'Engineering',\n",
    "            6: 'Finance',\n",
    "            7: 'Healthcare',\n",
    "            8: 'Human Resources',\n",
    "            9: 'Information Technology',\n",
    "            10: 'Internship',\n",
    "            11: 'Legal',\n",
    "            12: 'Management',\n",
    "            13: 'Manufacturing',\n",
    "            14: 'Marketing',\n",
    "            15: 'Operations',\n",
    "            16: 'Other',\n",
    "            17: 'Project Management',\n",
    "            18: 'Quality Assurance',\n",
    "            19: 'Sales',\n",
    "            20: 'Science',\n",
    "            21: 'Software Engineering',\n",
    "            22: 'Support'\n",
    "        }\n",
    "        \n",
    "        # Reverse mapping for lookup\n",
    "        self.reverse_category_mapping = {v: k for k, v in self.category_mapping.items()}\n",
    "        \n",
    "        # Extract patterns from ensemble predictions\n",
    "        self._extract_patterns()\n",
    "        \n",
    "        # Calculate category priors from ensemble\n",
    "        self.category_priors = self.ensemble_df['actual'].value_counts(normalize=True).to_dict()\n",
    "        \n",
    "        print(f\"Loaded {len(self.category_mapping)} job categories\")\n",
    "        print(f\"Calibrated with {len(self.ensemble_df)} ensemble predictions\")\n",
    "        \n",
    "    def _extract_patterns(self):\n",
    "        \"\"\"\n",
    "        Extract keyword patterns for each category from ensemble predictions\n",
    "        \"\"\"\n",
    "        print(\"Extracting category patterns from ensemble data...\")\n",
    "        \n",
    "        self.category_patterns = {}\n",
    "        \n",
    "       \n",
    "        if 'Description' in self.ensemble_df.columns and 'Title' in self.ensemble_df.columns:\n",
    "            \n",
    "            for category_id in range(23):\n",
    "                category_data = self.ensemble_df[self.ensemble_df['actual'] == category_id]\n",
    "                if len(category_data) > 0:\n",
    "                    # Combine all text for this category\n",
    "                    all_text = ' '.join(category_data['Title'].fillna('').astype(str) + ' ' + \n",
    "                                       category_data['Description'].fillna('').astype(str))\n",
    "                    all_text = all_text.lower()\n",
    "                    \n",
    "                    # Extract keyword frequencies\n",
    "                    self.category_patterns[category_id] = self._extract_keyword_frequencies(all_text, len(category_data))\n",
    "        else:\n",
    "            # If no text data, create synthetic patterns from probability distributions\n",
    "            print(\"No text data found in ensemble file - using probability-based patterns\")\n",
    "            for category_id in range(23):\n",
    "                # Get probability columns for this category\n",
    "                prob_col = f'prob_class_{category_id}'\n",
    "                if prob_col in self.ensemble_df.columns:\n",
    "                    # Calculate average probability when this category is predicted\n",
    "                    avg_prob = self.ensemble_df[self.ensemble_df['actual'] == category_id][prob_col].mean()\n",
    "                    self.category_patterns[category_id] = {'base_probability': avg_prob}\n",
    "        \n",
    "    def _extract_keyword_frequencies(self, text, doc_count):\n",
    "        \"\"\"\n",
    "        Extract keyword frequencies from text\n",
    "        \"\"\"\n",
    "        keywords = {\n",
    "            # Technical roles\n",
    "            'python': text.count('python'),\n",
    "            'java': text.count('java'),\n",
    "            'javascript': text.count('javascript'),\n",
    "            'sql': text.count('sql'),\n",
    "            'aws': text.count('aws'),\n",
    "            'azure': text.count('azure'),\n",
    "            'cloud': text.count('cloud'),\n",
    "            'devops': text.count('devops'),\n",
    "            'data': text.count('data'),\n",
    "            'machine learning': text.count('machine learning'),\n",
    "            'ai': text.count('ai'),\n",
    "            'ml': text.count('ml'),\n",
    "            'analyst': text.count('analyst'),\n",
    "            'engineer': text.count('engineer'),\n",
    "            'developer': text.count('developer'),\n",
    "            'software': text.count('software'),\n",
    "            \n",
    "            # Business roles\n",
    "            'sales': text.count('sales'),\n",
    "            'marketing': text.count('marketing'),\n",
    "            'finance': text.count('finance'),\n",
    "            'accounting': text.count('accounting'),\n",
    "            'hr': text.count('hr'),\n",
    "            'human resources': text.count('human resources'),\n",
    "            'recruiter': text.count('recruiter'),\n",
    "            'legal': text.count('legal'),\n",
    "            'law': text.count('law'),\n",
    "            \n",
    "            # Management\n",
    "            'manager': text.count('manager'),\n",
    "            'director': text.count('director'),\n",
    "            'head': text.count('head'),\n",
    "            'lead': text.count('lead'),\n",
    "            'chief': text.count('chief'),\n",
    "            \n",
    "            # Operations\n",
    "            'operations': text.count('operations'),\n",
    "            'logistics': text.count('logistics'),\n",
    "            'supply chain': text.count('supply chain'),\n",
    "            'quality': text.count('quality'),\n",
    "            'manufacturing': text.count('manufacturing'),\n",
    "            'production': text.count('production'),\n",
    "            \n",
    "            # Other\n",
    "            'intern': text.count('intern'),\n",
    "            'internship': text.count('internship'),\n",
    "            'apprentice': text.count('apprentice'),\n",
    "            'support': text.count('support'),\n",
    "            'customer service': text.count('customer service'),\n",
    "            'healthcare': text.count('healthcare'),\n",
    "            'nurse': text.count('nurse'),\n",
    "            'doctor': text.count('doctor'),\n",
    "            'scientist': text.count('scientist'),\n",
    "            'research': text.count('research')\n",
    "        }\n",
    "        \n",
    "        # Normalize by document count\n",
    "        return {k: v/doc_count if doc_count > 0 else 0 for k, v in keywords.items()}\n",
    "    \n",
    "    def _calculate_seniority_score(self, title):\n",
    "        \"\"\"\n",
    "        Calculate seniority score from job title\n",
    "        \"\"\"\n",
    "        seniority_keywords = {\n",
    "            'junior': 1, 'entry': 1, 'associate': 1, 'trainee': 1,\n",
    "            'mid': 2, 'intermediate': 2, 'experienced': 2,\n",
    "            'senior': 3, 'sr': 3,\n",
    "            'lead': 4, 'principal': 5, 'staff': 4,\n",
    "            'manager': 4, 'director': 5, 'head': 5, 'chief': 5,\n",
    "            'vp': 5, 'vice president': 5\n",
    "        }\n",
    "        \n",
    "        title_lower = title.lower()\n",
    "        score = 0\n",
    "        \n",
    "        for kw, kw_score in seniority_keywords.items():\n",
    "            if kw in title_lower:\n",
    "                score = max(score, kw_score)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def _calculate_category_scores(self, title, description, skills):\n",
    "        \"\"\"\n",
    "        Calculate scores for all 23 categories based on input\n",
    "        \"\"\"\n",
    "        # Initialize scores with priors\n",
    "        scores = {cat_id: self.category_priors.get(cat_id, 0.01) for cat_id in range(23)}\n",
    "        \n",
    "        # Combine text for analysis\n",
    "        text_lower = f\"{title} {description}\".lower()\n",
    "        skills_lower = [s.lower() for s in skills]\n",
    "        \n",
    "        # Calculate keyword matches\n",
    "        for cat_id, patterns in self.category_patterns.items():\n",
    "            category_name = self.category_mapping[cat_id].lower()\n",
    "            \n",
    "            # Category name match in title (strong signal)\n",
    "            if category_name in text_lower:\n",
    "                scores[cat_id] += 0.15\n",
    "            \n",
    "            # Specific keyword matching based on patterns\n",
    "            if isinstance(patterns, dict):\n",
    "                for keyword, frequency in patterns.items():\n",
    "                    if keyword in text_lower or any(keyword in skill for skill in skills_lower):\n",
    "                        # Weight by frequency from training data\n",
    "                        scores[cat_id] += frequency * 0.1\n",
    "            \n",
    "            # Domain-specific rules\n",
    "            if cat_id == 3:  # Data Science\n",
    "                if any(kw in text_lower for kw in ['python', 'data', 'machine learning', 'ai', 'analytics']):\n",
    "                    scores[cat_id] += 0.1\n",
    "            elif cat_id == 21:  # Software Engineering\n",
    "                if any(kw in text_lower for kw in ['developer', 'software', 'coding', 'programming', 'java', 'javascript']):\n",
    "                    scores[cat_id] += 0.1\n",
    "            elif cat_id == 4:  # DevOps\n",
    "                if any(kw in text_lower for kw in ['devops', 'aws', 'cloud', 'docker', 'kubernetes', 'ci/cd']):\n",
    "                    scores[cat_id] += 0.1\n",
    "            elif cat_id == 5:  # Engineering (general)\n",
    "                if 'engineer' in text_lower and not any(x in text_lower for x in ['software', 'data', 'devops']):\n",
    "                    scores[cat_id] += 0.1\n",
    "            elif cat_id == 12:  # Management\n",
    "                if any(kw in text_lower for kw in ['manager', 'lead', 'director', 'head']):\n",
    "                    scores[cat_id] += 0.1\n",
    "            elif cat_id == 10:  # Internship\n",
    "                if any(kw in text_lower for kw in ['intern', 'internship', 'trainee', 'apprentice']):\n",
    "                    scores[cat_id] += 0.1\n",
    "            elif cat_id == 18:  # Quality Assurance\n",
    "                if any(kw in text_lower for kw in ['quality', 'qa', 'test', 'assurance']):\n",
    "                    scores[cat_id] += 0.1\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def predict(self, job_title, job_description, skills=None, experience=None, remote=None):\n",
    "        \"\"\"\n",
    "        Predict job category for a new job posting\n",
    "        \"\"\"\n",
    "        if skills is None:\n",
    "            skills = []\n",
    "        \n",
    "        print(f\"\\nAnalyzing: {job_title}\")\n",
    "        print(f\"   Skills provided: {', '.join(skills[:5])}\" + (\"...\" if len(skills) > 5 else \"\"))\n",
    "        if experience:\n",
    "            print(f\"   Experience: {experience} years\")\n",
    "        \n",
    "        # Calculate seniority\n",
    "        seniority_score = self._calculate_seniority_score(job_title)\n",
    "        seniority_level = ['Entry', 'Mid', 'Senior', 'Lead', 'Executive'][min(seniority_score, 4)]\n",
    "        \n",
    "        # Calculate scores for all categories\n",
    "        category_scores = self._calculate_category_scores(job_title, job_description, skills)\n",
    "        \n",
    "        # Adjust scores based on seniority\n",
    "        for cat_id in category_scores:\n",
    "            if seniority_score >= 4 and cat_id in [10]:  # Internship\n",
    "                category_scores[cat_id] *= 0.3  # Reduce internship probability for senior roles\n",
    "            elif seniority_score <= 1 and cat_id in [12, 4, 5]:  # Management/Leadership\n",
    "                category_scores[cat_id] *= 0.5  # Reduce management probability for junior roles\n",
    "        \n",
    "        # Normalize scores to get probabilities\n",
    "        total_score = sum(category_scores.values())\n",
    "        if total_score > 0:\n",
    "            probabilities = {cat_id: score/total_score for cat_id, score in category_scores.items()}\n",
    "        else:\n",
    "            probabilities = {cat_id: 1/23 for cat_id in range(23)}\n",
    "        \n",
    "        # Get top 5 predictions\n",
    "        top_categories = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        \n",
    "        # Format predictions\n",
    "        predictions = []\n",
    "        for cat_id, prob in top_categories:\n",
    "            predictions.append({\n",
    "                'category_id': cat_id,\n",
    "                'category_name': self.category_mapping[cat_id],\n",
    "                'confidence': prob\n",
    "            })\n",
    "        \n",
    "        # Get primary prediction\n",
    "        primary = predictions[0]\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nPrediction Results:\")\n",
    "        print(f\"   Seniority Level: {seniority_level}\")\n",
    "        print(f\"\\n   Primary Prediction: {primary['category_name']} ({primary['confidence']:.1%} confidence)\")\n",
    "        print(f\"\\n   Top 5 Categories:\")\n",
    "        for i, pred in enumerate(predictions, 1):\n",
    "            print(f\"      {i}. {pred['category_name']:25} {pred['confidence']:>6.1%}\")\n",
    "        \n",
    "        return {\n",
    "            'primary_prediction': primary,\n",
    "            'all_predictions': predictions,\n",
    "            'seniority_level': seniority_level,\n",
    "            'seniority_score': seniority_score,\n",
    "            'features_extracted': len(category_scores)\n",
    "        }\n",
    "    \n",
    "    def predict_batch(self, jobs_df):\n",
    "        \"\"\"\n",
    "        Predict categories for a batch of jobs\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for idx, row in jobs_df.iterrows():\n",
    "            skills = row.get('skills', [])\n",
    "            if isinstance(skills, str):\n",
    "                skills = skills.split(',')\n",
    "            \n",
    "            result = self.predict(\n",
    "                job_title=row['title'],\n",
    "                job_description=row.get('description', ''),\n",
    "                skills=skills\n",
    "            )\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_category_stats(self):\n",
    "        \"\"\"\n",
    "        Get statistics about categories from ensemble predictions\n",
    "        \"\"\"\n",
    "        stats = []\n",
    "        for cat_id in range(23):\n",
    "            cat_data = self.ensemble_df[self.ensemble_df['actual'] == cat_id]\n",
    "            stats.append({\n",
    "                'category_id': cat_id,\n",
    "                'category_name': self.category_mapping[cat_id],\n",
    "                'count': len(cat_data),\n",
    "                'percentage': len(cat_data) / len(self.ensemble_df) * 100,\n",
    "                'avg_confidence': cat_data['confidence'].mean() if len(cat_data) > 0 else 0\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(stats).sort_values('count', ascending=False)\n",
    "\n",
    "# Initialize the predictor\n",
    "predictor = JobCategoryPredictor('ensemble_predictions.csv')\n",
    "\n",
    "# Example usage function\n",
    "def predict_job_category(job_title, job_description, skills=None, experience=None, remote=None):\n",
    "    \"\"\"\n",
    "    Wrapper function for easy prediction\n",
    "    \"\"\"\n",
    "    return predictor.predict(job_title, job_description, skills, experience, remote)\n",
    "\n",
    "# Demo function\n",
    "def demo_predictor():\n",
    "    \"\"\"\n",
    "    Demonstrate the predictor with example jobs\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"JOB CATEGORY PREDICTOR DEMO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Example 1: Data Science role\n",
    "    predict_job_category(\n",
    "        \"Senior Data Scientist\",\n",
    "        \"Looking for an experienced data scientist with Python, machine learning, and SQL expertise to build predictive models.\",\n",
    "        skills=[\"Python\", \"Machine Learning\", \"SQL\", \"TensorFlow\"],\n",
    "        experience=5\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    \n",
    "    # Example 2: Legal role\n",
    "    predict_job_category(\n",
    "        \"Corporate Legal Counsel\",\n",
    "        \"Provide legal advice on corporate matters, contracts, and compliance. Must have law degree and bar admission.\",\n",
    "        skills=[\"Contract Law\", \"Corporate Law\", \"Compliance\", \"Legal Research\"],\n",
    "        experience=8\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    \n",
    "    # Example 3: Internship\n",
    "    predict_job_category(\n",
    "        \"Marketing Intern\",\n",
    "        \"Summer internship opportunity for students interested in digital marketing, social media, and content creation.\",\n",
    "        skills=[\"Social Media\", \"Content Creation\", \"Communication\"],\n",
    "        experience=0\n",
    "    )\n",
    "    \n",
    "    # Show category statistics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CATEGORY DISTRIBUTION FROM ENSEMBLE\")\n",
    "    print(\"=\"*70)\n",
    "    stats_df = predictor.get_category_stats()\n",
    "    print(stats_df[['category_name', 'count', 'percentage', 'avg_confidence']].to_string(index=False))\n",
    "\n",
    "# Run demo\n",
    "if __name__ == \"__main__\":\n",
    "    demo_predictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54658ac-acaf-4abd-973e-66ba945718ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_predictor_app.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import time\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# JOB CATEGORY PREDICTOR CLASS (embedded directly in the app)\n",
    "# ============================================================================\n",
    "\n",
    "class JobCategoryPredictor:\n",
    "    \"\"\"\n",
    "    Job Category Prediction System using Ensemble Model Calibration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ensemble_predictions_path='ensemble_predictions.csv'):\n",
    "        \"\"\"\n",
    "        Initialize the predictor with ensemble predictions for calibration\n",
    "        \"\"\"\n",
    "        print(\"🔄 Initializing Job Category Predictor...\")\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(ensemble_predictions_path):\n",
    "            st.error(f\"❌ File not found: {ensemble_predictions_path}\")\n",
    "            st.info(\"Please make sure 'ensemble_predictions.csv' is in the same directory as this app.\")\n",
    "            self.ensemble_df = pd.DataFrame()\n",
    "        else:\n",
    "            # Load ensemble predictions\n",
    "            self.ensemble_df = pd.read_csv(ensemble_predictions_path)\n",
    "        \n",
    "        # Category mapping (from your dataset)\n",
    "        self.category_mapping = {\n",
    "            0: 'Administrative',\n",
    "            1: 'Arts & Design',\n",
    "            2: 'Business Analysis',\n",
    "            3: 'Data Science',\n",
    "            4: 'DevOps',\n",
    "            5: 'Engineering',\n",
    "            6: 'Finance',\n",
    "            7: 'Healthcare',\n",
    "            8: 'Human Resources',\n",
    "            9: 'Information Technology',\n",
    "            10: 'Internship',\n",
    "            11: 'Legal',\n",
    "            12: 'Management',\n",
    "            13: 'Manufacturing',\n",
    "            14: 'Marketing',\n",
    "            15: 'Operations',\n",
    "            16: 'Other',\n",
    "            17: 'Project Management',\n",
    "            18: 'Quality Assurance',\n",
    "            19: 'Sales',\n",
    "            20: 'Science',\n",
    "            21: 'Software Engineering',\n",
    "            22: 'Support'\n",
    "        }\n",
    "        \n",
    "        # Reverse mapping for lookup\n",
    "        self.reverse_category_mapping = {v: k for k, v in self.category_mapping.items()}\n",
    "        \n",
    "        # Extract patterns from ensemble predictions\n",
    "        self._extract_patterns()\n",
    "        \n",
    "        # Calculate category priors from ensemble\n",
    "        if len(self.ensemble_df) > 0 and 'actual' in self.ensemble_df.columns:\n",
    "            self.category_priors = self.ensemble_df['actual'].value_counts(normalize=True).to_dict()\n",
    "        else:\n",
    "            # Default priors if no data\n",
    "            self.category_priors = {i: 1/23 for i in range(23)}\n",
    "        \n",
    "        print(f\"✅ Loaded {len(self.category_mapping)} job categories\")\n",
    "        if len(self.ensemble_df) > 0:\n",
    "            print(f\"✅ Calibrated with {len(self.ensemble_df)} ensemble predictions\")\n",
    "        \n",
    "    def _extract_patterns(self):\n",
    "        \"\"\"\n",
    "        Extract keyword patterns for each category from ensemble predictions\n",
    "        \"\"\"\n",
    "        self.category_patterns = {}\n",
    "        \n",
    "        if len(self.ensemble_df) == 0:\n",
    "            # Create default patterns if no data\n",
    "            for cat_id in range(23):\n",
    "                self.category_patterns[cat_id] = {'base_probability': 1/23}\n",
    "            return\n",
    "        \n",
    "        # Get probability columns\n",
    "        prob_cols = [col for col in self.ensemble_df.columns if col.startswith('prob_class_')]\n",
    "        \n",
    "        if prob_cols:\n",
    "            # Use probability-based patterns\n",
    "            for cat_id in range(23):\n",
    "                prob_col = f'prob_class_{cat_id}'\n",
    "                if prob_col in self.ensemble_df.columns:\n",
    "                    # Calculate average probability when this category is actual\n",
    "                    cat_data = self.ensemble_df[self.ensemble_df['actual'] == cat_id]\n",
    "                    if len(cat_data) > 0:\n",
    "                        avg_prob = cat_data[prob_col].mean()\n",
    "                    else:\n",
    "                        avg_prob = 1/23\n",
    "                    self.category_patterns[cat_id] = {'base_probability': avg_prob}\n",
    "        else:\n",
    "            # Default patterns\n",
    "            for cat_id in range(23):\n",
    "                self.category_patterns[cat_id] = {'base_probability': 1/23}\n",
    "    \n",
    "    def _calculate_seniority_score(self, title):\n",
    "        \"\"\"\n",
    "        Calculate seniority score from job title\n",
    "        \"\"\"\n",
    "        seniority_keywords = {\n",
    "            'junior': 1, 'entry': 1, 'associate': 1, 'trainee': 1,\n",
    "            'mid': 2, 'intermediate': 2, 'experienced': 2,\n",
    "            'senior': 3, 'sr': 3,\n",
    "            'lead': 4, 'principal': 5, 'staff': 4,\n",
    "            'manager': 4, 'director': 5, 'head': 5, 'chief': 5,\n",
    "            'vp': 5, 'vice president': 5\n",
    "        }\n",
    "        \n",
    "        title_lower = title.lower()\n",
    "        score = 0\n",
    "        \n",
    "        for kw, kw_score in seniority_keywords.items():\n",
    "            if kw in title_lower:\n",
    "                score = max(score, kw_score)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def _calculate_category_scores(self, title, description, skills):\n",
    "        \"\"\"\n",
    "        Calculate scores for all 23 categories based on input\n",
    "        \"\"\"\n",
    "        # Initialize scores with priors\n",
    "        scores = {cat_id: self.category_priors.get(cat_id, 0.01) for cat_id in range(23)}\n",
    "        \n",
    "        # Combine text for analysis\n",
    "        text_lower = f\"{title} {description}\".lower()\n",
    "        skills_lower = [s.lower() for s in skills if s]\n",
    "        \n",
    "        # Common keywords for each category\n",
    "        category_keywords = {\n",
    "            0: ['administrative', 'admin', 'assistant', 'clerical', 'office'],\n",
    "            1: ['design', 'art', 'creative', 'ui', 'ux', 'graphic'],\n",
    "            2: ['business analyst', 'requirements', 'stakeholder', 'process'],\n",
    "            3: ['data', 'analytics', 'machine learning', 'python', 'sql', 'ai'],\n",
    "            4: ['devops', 'aws', 'cloud', 'docker', 'kubernetes', 'ci/cd'],\n",
    "            5: ['engineer', 'engineering', 'mechanical', 'electrical', 'civil'],\n",
    "            6: ['finance', 'accounting', 'financial', 'audit', 'tax'],\n",
    "            7: ['healthcare', 'medical', 'nurse', 'doctor', 'clinical'],\n",
    "            8: ['hr', 'human resources', 'recruiter', 'talent', 'people'],\n",
    "            9: ['it', 'information technology', 'help desk', 'support', 'technical'],\n",
    "            10: ['intern', 'internship', 'trainee', 'apprentice'],\n",
    "            11: ['legal', 'law', 'attorney', 'counsel', 'compliance'],\n",
    "            12: ['manager', 'management', 'director', 'head', 'lead'],\n",
    "            13: ['manufacturing', 'production', 'plant', 'factory'],\n",
    "            14: ['marketing', 'digital marketing', 'seo', 'content', 'social media'],\n",
    "            15: ['operations', 'logistics', 'supply chain', 'distribution'],\n",
    "            16: ['other', 'general', 'miscellaneous'],\n",
    "            17: ['project manager', 'project management', 'pmp', 'agile'],\n",
    "            18: ['quality', 'qa', 'test', 'assurance', 'testing'],\n",
    "            19: ['sales', 'account executive', 'business development', 'b2b'],\n",
    "            20: ['science', 'scientist', 'research', 'lab', 'r&d'],\n",
    "            21: ['software', 'developer', 'programming', 'coding', 'full stack'],\n",
    "            22: ['support', 'customer service', 'help desk', 'technical support']\n",
    "        }\n",
    "        \n",
    "        # Calculate keyword matches\n",
    "        for cat_id, keywords in category_keywords.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword in text_lower:\n",
    "                    scores[cat_id] += 0.05\n",
    "                # Check skills\n",
    "                for skill in skills_lower:\n",
    "                    if keyword in skill:\n",
    "                        scores[cat_id] += 0.03\n",
    "        \n",
    "        # Adjust based on seniority\n",
    "        seniority_score = self._calculate_seniority_score(title)\n",
    "        if seniority_score >= 4:\n",
    "            scores[10] *= 0.3  # Reduce internship for senior roles\n",
    "        elif seniority_score <= 1:\n",
    "            scores[12] *= 0.5  # Reduce management for junior roles\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def predict(self, job_title, job_description, skills=None, experience=None, remote=None):\n",
    "        \"\"\"\n",
    "        Predict job category for a new job posting\n",
    "        \"\"\"\n",
    "        if skills is None:\n",
    "            skills = []\n",
    "        \n",
    "        # Calculate seniority\n",
    "        seniority_score = self._calculate_seniority_score(job_title)\n",
    "        seniority_levels = ['Entry', 'Mid', 'Senior', 'Lead', 'Executive']\n",
    "        seniority_level = seniority_levels[min(seniority_score, 4)]\n",
    "        \n",
    "        # Calculate scores for all categories\n",
    "        category_scores = self._calculate_category_scores(job_title, job_description, skills)\n",
    "        \n",
    "        # Normalize scores to get probabilities\n",
    "        total_score = sum(category_scores.values())\n",
    "        if total_score > 0:\n",
    "            probabilities = {cat_id: score/total_score for cat_id, score in category_scores.items()}\n",
    "        else:\n",
    "            probabilities = {cat_id: 1/23 for cat_id in range(23)}\n",
    "        \n",
    "        # Get top 5 predictions\n",
    "        top_categories = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        \n",
    "        # Format predictions\n",
    "        predictions = []\n",
    "        for cat_id, prob in top_categories:\n",
    "            predictions.append({\n",
    "                'category_id': cat_id,\n",
    "                'category_name': self.category_mapping[cat_id],\n",
    "                'confidence': prob\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'primary_prediction': predictions[0],\n",
    "            'all_predictions': predictions,\n",
    "            'seniority_level': seniority_level,\n",
    "            'seniority_score': seniority_score,\n",
    "            'features_extracted': len(category_scores)\n",
    "        }\n",
    "    \n",
    "    def get_category_stats(self):\n",
    "        \"\"\"\n",
    "        Get statistics about categories from ensemble predictions\n",
    "        \"\"\"\n",
    "        if len(self.ensemble_df) == 0:\n",
    "            # Return default stats if no data\n",
    "            stats = []\n",
    "            for cat_id in range(23):\n",
    "                stats.append({\n",
    "                    'category_id': cat_id,\n",
    "                    'category_name': self.category_mapping[cat_id],\n",
    "                    'count': 0,\n",
    "                    'percentage': 100/23,\n",
    "                    'avg_confidence': 1/23\n",
    "                })\n",
    "            return pd.DataFrame(stats)\n",
    "        \n",
    "        stats = []\n",
    "        for cat_id in range(23):\n",
    "            cat_data = self.ensemble_df[self.ensemble_df['actual'] == cat_id]\n",
    "            stats.append({\n",
    "                'category_id': cat_id,\n",
    "                'category_name': self.category_mapping[cat_id],\n",
    "                'count': len(cat_data),\n",
    "                'percentage': len(cat_data) / len(self.ensemble_df) * 100 if len(self.ensemble_df) > 0 else 0,\n",
    "                'avg_confidence': cat_data['confidence'].mean() if len(cat_data) > 0 else 0\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(stats).sort_values('count', ascending=False)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STREAMLIT APP\n",
    "# ============================================================================\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(\n",
    "    page_title=\"Job Category Predictor\",\n",
    "    page_icon=\"🎯\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Custom CSS\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "    .main-header {\n",
    "        font-size: 3rem;\n",
    "        color: #1E88E5;\n",
    "        text-align: center;\n",
    "        margin-bottom: 1rem;\n",
    "    }\n",
    "    .prediction-box {\n",
    "        background-color: #f0f2f6;\n",
    "        padding: 2rem;\n",
    "        border-radius: 10px;\n",
    "        margin: 1rem 0;\n",
    "    }\n",
    "    .category-tag {\n",
    "        background-color: #1E88E5;\n",
    "        color: white;\n",
    "        padding: 0.3rem 0.8rem;\n",
    "        border-radius: 20px;\n",
    "        display: inline-block;\n",
    "        margin: 0.2rem;\n",
    "        font-size: 0.9rem;\n",
    "    }\n",
    "    .confidence-bar {\n",
    "        height: 25px;\n",
    "        background: linear-gradient(90deg, #1E88E5, #64B5F6);\n",
    "        border-radius: 12px;\n",
    "        margin: 0.5rem 0;\n",
    "        color: white;\n",
    "        padding-left: 10px;\n",
    "        line-height: 25px;\n",
    "        font-weight: bold;\n",
    "    }\n",
    "    .stButton>button {\n",
    "        width: 100%;\n",
    "        background-color: #1E88E5;\n",
    "        color: white;\n",
    "        font-weight: bold;\n",
    "        height: 50px;\n",
    "        font-size: 1.2rem;\n",
    "    }\n",
    "    .category-stats {\n",
    "        background-color: #ffffff;\n",
    "        padding: 1rem;\n",
    "        border-radius: 10px;\n",
    "        box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "        margin: 0.5rem 0;\n",
    "    }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Initialize session state\n",
    "if 'predictor' not in st.session_state:\n",
    "    with st.spinner('🔄 Loading Job Category Predictor...'):\n",
    "        # Check if ensemble_predictions.csv exists\n",
    "        if os.path.exists('ensemble_predictions.csv'):\n",
    "            st.session_state.predictor = JobCategoryPredictor('ensemble_predictions.csv')\n",
    "        else:\n",
    "            st.warning(\"⚠️ ensemble_predictions.csv not found. Using default settings.\")\n",
    "            st.session_state.predictor = JobCategoryPredictor()  # Will work with defaults\n",
    "        st.session_state.history = []\n",
    "        st.session_state.prediction_count = 0\n",
    "\n",
    "# Header\n",
    "st.markdown('<h1 class=\"main-header\">🎯 Job Category Predictor</h1>', unsafe_allow_html=True)\n",
    "st.markdown(\"### Powered by Ensemble Learning | 23 Job Categories\")\n",
    "\n",
    "# Sidebar\n",
    "with st.sidebar:\n",
    "    st.markdown(\"## 📊 Dashboard\")\n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    # Quick stats\n",
    "    st.markdown(\"### Quick Stats\")\n",
    "    stats = st.session_state.predictor.get_category_stats()\n",
    "    \n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        st.metric(\"Total Categories\", len(stats))\n",
    "    with col2:\n",
    "        st.metric(\"Predictions Made\", st.session_state.prediction_count)\n",
    "    \n",
    "    # Category distribution (simplified)\n",
    "    st.markdown(\"### Top Categories\")\n",
    "    for idx, row in stats.head(5).iterrows():\n",
    "        st.markdown(f\"\"\"\n",
    "        <div class=\"category-stats\">\n",
    "            <b>{row['category_name']}</b><br>\n",
    "            {row['percentage']:.1f}% of jobs\n",
    "        </div>\n",
    "        \"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"### About\")\n",
    "    st.info(\"\"\"\n",
    "    This app predicts job categories using an ensemble model trained on thousands of job postings.\n",
    "    \n",
    "    **Supported Categories:**\n",
    "    - Data Science\n",
    "    - Software Engineering\n",
    "    - DevOps\n",
    "    - Management\n",
    "    - And 19 more...\n",
    "    \"\"\")\n",
    "\n",
    "# Main content\n",
    "tab1, tab2, tab3 = st.tabs([\"🔮 Predict\", \"📊 Analytics\", \"📋 History\"])\n",
    "\n",
    "with tab1:\n",
    "    col1, col2 = st.columns([1, 1])\n",
    "    \n",
    "    with col1:\n",
    "        st.markdown(\"### 📝 Job Details\")\n",
    "        \n",
    "        # Input form\n",
    "        with st.form(\"prediction_form\"):\n",
    "            job_title = st.text_input(\n",
    "                \"Job Title *\", \n",
    "                placeholder=\"e.g., Senior Data Scientist\",\n",
    "                help=\"Enter the job title\"\n",
    "            )\n",
    "            \n",
    "            job_description = st.text_area(\n",
    "                \"Job Description *\", \n",
    "                height=150,\n",
    "                placeholder=\"Describe the role, responsibilities, and requirements...\",\n",
    "                help=\"Paste the full job description\"\n",
    "            )\n",
    "            \n",
    "            skills_input = st.text_input(\n",
    "                \"Skills (comma-separated)\", \n",
    "                placeholder=\"Python, SQL, Machine Learning\",\n",
    "                help=\"List key skills required\"\n",
    "            )\n",
    "            \n",
    "            experience = st.slider(\n",
    "                \"Years of Experience\",\n",
    "                min_value=0,\n",
    "                max_value=30,\n",
    "                value=0,\n",
    "                help=\"Required experience in years\"\n",
    "            )\n",
    "            \n",
    "            submitted = st.form_submit_button(\n",
    "                \"🎯 Predict Category\", \n",
    "                use_container_width=True\n",
    "            )\n",
    "    \n",
    "    with col2:\n",
    "        st.markdown(\"### 📋 Example Jobs\")\n",
    "        st.markdown(\"Click to load an example:\")\n",
    "        \n",
    "        examples = {\n",
    "            \"Data Scientist\": {\n",
    "                \"title\": \"Senior Data Scientist\",\n",
    "                \"desc\": \"Looking for an experienced data scientist with Python, machine learning, and SQL expertise to build predictive models. Must have experience with TensorFlow or PyTorch.\",\n",
    "                \"skills\": \"Python, Machine Learning, SQL, TensorFlow\",\n",
    "                \"exp\": 5\n",
    "            },\n",
    "            \"Software Engineer\": {\n",
    "                \"title\": \"Full Stack Developer\",\n",
    "                \"desc\": \"Develop and maintain web applications using React, Node.js, and PostgreSQL. Work in an agile team environment.\",\n",
    "                \"skills\": \"JavaScript, React, Node.js, SQL\",\n",
    "                \"exp\": 3\n",
    "            },\n",
    "            \"Legal Counsel\": {\n",
    "                \"title\": \"Corporate Legal Counsel\",\n",
    "                \"desc\": \"Provide legal advice on corporate matters, contracts, and compliance. Must have law degree and bar admission.\",\n",
    "                \"skills\": \"Contract Law, Corporate Law, Compliance\",\n",
    "                \"exp\": 8\n",
    "            },\n",
    "            \"Marketing Intern\": {\n",
    "                \"title\": \"Marketing Intern\",\n",
    "                \"desc\": \"Summer internship opportunity for students interested in digital marketing, social media, and content creation.\",\n",
    "                \"skills\": \"Social Media, Content Creation, Communication\",\n",
    "                \"exp\": 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Create buttons for examples\n",
    "        for name, example in examples.items():\n",
    "            if st.button(f\"📌 {name}\", key=f\"example_{name}\", use_container_width=True):\n",
    "                st.session_state.example_title = example['title']\n",
    "                st.session_state.example_desc = example['desc']\n",
    "                st.session_state.example_skills = example['skills']\n",
    "                st.session_state.example_exp = example['exp']\n",
    "                st.rerun()\n",
    "    \n",
    "    # Prediction area\n",
    "    if submitted:\n",
    "        if not job_title or not job_description:\n",
    "            st.error(\"❌ Please provide both Job Title and Job Description\")\n",
    "        else:\n",
    "            with st.spinner('🔮 Analyzing job posting...'):\n",
    "                time.sleep(1)  # Simulate processing\n",
    "                \n",
    "                # Parse skills\n",
    "                skills = [s.strip() for s in skills_input.split(',')] if skills_input else []\n",
    "                \n",
    "                # Make prediction\n",
    "                result = st.session_state.predictor.predict(\n",
    "                    job_title, \n",
    "                    job_description, \n",
    "                    skills, \n",
    "                    experience if experience > 0 else None\n",
    "                )\n",
    "                \n",
    "                # Update counter\n",
    "                st.session_state.prediction_count += 1\n",
    "                \n",
    "                # Add to history\n",
    "                st.session_state.history.append({\n",
    "                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'title': job_title[:50] + \"...\" if len(job_title) > 50 else job_title,\n",
    "                    'primary': result['primary_prediction']['category_name'],\n",
    "                    'confidence': f\"{result['primary_prediction']['confidence']:.1%}\",\n",
    "                    'seniority': result['seniority_level']\n",
    "                })\n",
    "                \n",
    "                # Display results\n",
    "                st.markdown(\"---\")\n",
    "                \n",
    "                col3, col4 = st.columns([1, 1])\n",
    "                \n",
    "                with col3:\n",
    "                    st.markdown(\"### 🎯 Primary Prediction\")\n",
    "                    \n",
    "                    primary = result['primary_prediction']\n",
    "                    \n",
    "                    # Create colored box for primary prediction\n",
    "                    confidence_pct = int(primary['confidence'] * 100)\n",
    "                    st.markdown(f\"\"\"\n",
    "                    <div style=\"background-color: #1E88E5; padding: 20px; border-radius: 10px; text-align: center;\">\n",
    "                        <h2 style=\"color: white; margin: 0;\">{primary['category_name']}</h2>\n",
    "                        <p style=\"color: white; font-size: 1.2rem; margin: 10px 0;\">Confidence: {primary['confidence']:.1%}</p>\n",
    "                        <div style=\"background-color: white; height: 10px; border-radius: 5px; margin: 10px 0;\">\n",
    "                            <div style=\"background-color: #FFC107; width: {confidence_pct}%; height: 10px; border-radius: 5px;\"></div>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                    \"\"\", unsafe_allow_html=True)\n",
    "                    \n",
    "                    st.markdown(f\"**Seniority Level:** {result['seniority_level']}\")\n",
    "                \n",
    "                with col4:\n",
    "                    st.markdown(\"### 📊 Top 5 Categories\")\n",
    "                    \n",
    "                    # Display top predictions with confidence bars\n",
    "                    for i, pred in enumerate(result['all_predictions'][:5], 1):\n",
    "                        confidence_pct = int(pred['confidence'] * 100)\n",
    "                        st.markdown(f\"\"\"\n",
    "                        <div style=\"margin: 10px 0;\">\n",
    "                            <b>{i}. {pred['category_name']}</b><br>\n",
    "                            <div style=\"background-color: #e0e0e0; height: 20px; border-radius: 5px; width: 100%;\">\n",
    "                                <div style=\"background-color: #1E88E5; width: {confidence_pct}%; height: 20px; border-radius: 5px; text-align: right; padding-right: 5px; color: white; line-height: 20px;\">\n",
    "                                    {pred['confidence']:.1%}\n",
    "                                </div>\n",
    "                            </div>\n",
    "                        </div>\n",
    "                        \"\"\", unsafe_allow_html=True)\n",
    "                \n",
    "                # Feature summary\n",
    "                st.markdown(\"### 🔍 Analysis Summary\")\n",
    "                col5, col6, col7 = st.columns(3)\n",
    "                \n",
    "                with col5:\n",
    "                    st.metric(\"Seniority Score\", result['seniority_score'])\n",
    "                with col6:\n",
    "                    st.metric(\"Skills Provided\", len(skills))\n",
    "                with col7:\n",
    "                    st.metric(\"Features Analyzed\", result['features_extracted'])\n",
    "\n",
    "with tab2:\n",
    "    st.markdown(\"### 📊 Category Analytics\")\n",
    "    \n",
    "    # Get stats\n",
    "    stats_df = st.session_state.predictor.get_category_stats()\n",
    "    \n",
    "    # Top categories bar chart\n",
    "    st.markdown(\"#### Top 10 Categories by Frequency\")\n",
    "    \n",
    "    # Prepare data for bar chart\n",
    "    chart_data = stats_df.head(10).set_index('category_name')['count']\n",
    "    st.bar_chart(chart_data, use_container_width=True)\n",
    "    \n",
    "    # Category distribution table\n",
    "    st.markdown(\"#### Category Details\")\n",
    "    \n",
    "    # Format the dataframe for display\n",
    "    display_df = stats_df[['category_name', 'count', 'percentage', 'avg_confidence']].copy()\n",
    "    display_df['percentage'] = display_df['percentage'].round(1).astype(str) + '%'\n",
    "    display_df['avg_confidence'] = display_df['avg_confidence'].round(3).apply(lambda x: f\"{x:.1%}\")\n",
    "    display_df.columns = ['Category', 'Count', '% of Total', 'Avg Confidence']\n",
    "    \n",
    "    st.dataframe(\n",
    "        display_df,\n",
    "        use_container_width=True,\n",
    "        hide_index=True\n",
    "    )\n",
    "    \n",
    "    # Model performance summary (if ensemble data available)\n",
    "    if len(st.session_state.predictor.ensemble_df) > 0:\n",
    "        st.markdown(\"### 📈 Model Performance\")\n",
    "        \n",
    "        df = st.session_state.predictor.ensemble_df\n",
    "        if 'actual' in df.columns and 'predicted' in df.columns:\n",
    "            accuracy = (df['actual'] == df['predicted']).mean()\n",
    "            \n",
    "            col1, col2, col3 = st.columns(3)\n",
    "            with col1:\n",
    "                st.metric(\"Overall Accuracy\", f\"{accuracy:.2%}\")\n",
    "            with col2:\n",
    "                st.metric(\"Total Samples\", f\"{len(df):,}\")\n",
    "            with col3:\n",
    "                st.metric(\"Categories\", \"23\")\n",
    "\n",
    "with tab3:\n",
    "    st.markdown(\"### 📋 Prediction History\")\n",
    "    \n",
    "    if st.session_state.history:\n",
    "        # Convert history to dataframe\n",
    "        history_df = pd.DataFrame(st.session_state.history)\n",
    "        \n",
    "        # Display history\n",
    "        st.dataframe(\n",
    "            history_df,\n",
    "            use_container_width=True,\n",
    "            hide_index=True\n",
    "        )\n",
    "        \n",
    "        # Clear history button\n",
    "        if st.button(\"🗑️ Clear History\", use_container_width=True):\n",
    "            st.session_state.history = []\n",
    "            st.rerun()\n",
    "    else:\n",
    "        st.info(\"No predictions yet. Try predicting a job category!\")\n",
    "\n",
    "# Footer\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\n",
    "    \"<p style='text-align: center; color: gray;'>\"\n",
    "    \"Made with ❤️ using Streamlit | Job Category Predictor v1.0\"\n",
    "    \"</p>\", \n",
    "    unsafe_allow_html=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0087b1c-a141-47d4-bc77-13504284f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in a Jupyter cell to create the file with UTF-8 encoding\n",
    "with open('job_predictor_app.py', 'w', encoding='utf-8') as f:\n",
    "    f.write('''# job_predictor_app.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import time\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# JOB CATEGORY PREDICTOR CLASS (embedded directly in the app)\n",
    "# ============================================================================\n",
    "\n",
    "class JobCategoryPredictor:\n",
    "    \"\"\"\n",
    "    Job Category Prediction System using Ensemble Model Calibration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ensemble_predictions_path='ensemble_predictions.csv'):\n",
    "        \"\"\"\n",
    "        Initialize the predictor with ensemble predictions for calibration\n",
    "        \"\"\"\n",
    "        print(\"🔄 Initializing Job Category Predictor...\")\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(ensemble_predictions_path):\n",
    "            st.error(f\"❌ File not found: {ensemble_predictions_path}\")\n",
    "            st.info(\"Please make sure 'ensemble_predictions.csv' is in the same directory as this app.\")\n",
    "            self.ensemble_df = pd.DataFrame()\n",
    "        else:\n",
    "            # Load ensemble predictions\n",
    "            self.ensemble_df = pd.read_csv(ensemble_predictions_path)\n",
    "        \n",
    "        # Category mapping (from your dataset)\n",
    "        self.category_mapping = {\n",
    "            0: 'Administrative',\n",
    "            1: 'Arts & Design',\n",
    "            2: 'Business Analysis',\n",
    "            3: 'Data Science',\n",
    "            4: 'DevOps',\n",
    "            5: 'Engineering',\n",
    "            6: 'Finance',\n",
    "            7: 'Healthcare',\n",
    "            8: 'Human Resources',\n",
    "            9: 'Information Technology',\n",
    "            10: 'Internship',\n",
    "            11: 'Legal',\n",
    "            12: 'Management',\n",
    "            13: 'Manufacturing',\n",
    "            14: 'Marketing',\n",
    "            15: 'Operations',\n",
    "            16: 'Other',\n",
    "            17: 'Project Management',\n",
    "            18: 'Quality Assurance',\n",
    "            19: 'Sales',\n",
    "            20: 'Science',\n",
    "            21: 'Software Engineering',\n",
    "            22: 'Support'\n",
    "        }\n",
    "        \n",
    "        # Reverse mapping for lookup\n",
    "        self.reverse_category_mapping = {v: k for k, v in self.category_mapping.items()}\n",
    "        \n",
    "        # Extract patterns from ensemble predictions\n",
    "        self._extract_patterns()\n",
    "        \n",
    "        # Calculate category priors from ensemble\n",
    "        if len(self.ensemble_df) > 0 and 'actual' in self.ensemble_df.columns:\n",
    "            self.category_priors = self.ensemble_df['actual'].value_counts(normalize=True).to_dict()\n",
    "        else:\n",
    "            # Default priors if no data\n",
    "            self.category_priors = {i: 1/23 for i in range(23)}\n",
    "        \n",
    "        print(f\"✅ Loaded {len(self.category_mapping)} job categories\")\n",
    "        if len(self.ensemble_df) > 0:\n",
    "            print(f\"✅ Calibrated with {len(self.ensemble_df)} ensemble predictions\")\n",
    "        \n",
    "    def _extract_patterns(self):\n",
    "        \"\"\"\n",
    "        Extract keyword patterns for each category from ensemble predictions\n",
    "        \"\"\"\n",
    "        self.category_patterns = {}\n",
    "        \n",
    "        if len(self.ensemble_df) == 0:\n",
    "            # Create default patterns if no data\n",
    "            for cat_id in range(23):\n",
    "                self.category_patterns[cat_id] = {'base_probability': 1/23}\n",
    "            return\n",
    "        \n",
    "        # Get probability columns\n",
    "        prob_cols = [col for col in self.ensemble_df.columns if col.startswith('prob_class_')]\n",
    "        \n",
    "        if prob_cols:\n",
    "            # Use probability-based patterns\n",
    "            for cat_id in range(23):\n",
    "                prob_col = f'prob_class_{cat_id}'\n",
    "                if prob_col in self.ensemble_df.columns:\n",
    "                    # Calculate average probability when this category is actual\n",
    "                    cat_data = self.ensemble_df[self.ensemble_df['actual'] == cat_id]\n",
    "                    if len(cat_data) > 0:\n",
    "                        avg_prob = cat_data[prob_col].mean()\n",
    "                    else:\n",
    "                        avg_prob = 1/23\n",
    "                    self.category_patterns[cat_id] = {'base_probability': avg_prob}\n",
    "        else:\n",
    "            # Default patterns\n",
    "            for cat_id in range(23):\n",
    "                self.category_patterns[cat_id] = {'base_probability': 1/23}\n",
    "    \n",
    "    def _calculate_seniority_score(self, title):\n",
    "        \"\"\"\n",
    "        Calculate seniority score from job title\n",
    "        \"\"\"\n",
    "        seniority_keywords = {\n",
    "            'junior': 1, 'entry': 1, 'associate': 1, 'trainee': 1,\n",
    "            'mid': 2, 'intermediate': 2, 'experienced': 2,\n",
    "            'senior': 3, 'sr': 3,\n",
    "            'lead': 4, 'principal': 5, 'staff': 4,\n",
    "            'manager': 4, 'director': 5, 'head': 5, 'chief': 5,\n",
    "            'vp': 5, 'vice president': 5\n",
    "        }\n",
    "        \n",
    "        title_lower = title.lower()\n",
    "        score = 0\n",
    "        \n",
    "        for kw, kw_score in seniority_keywords.items():\n",
    "            if kw in title_lower:\n",
    "                score = max(score, kw_score)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def _calculate_category_scores(self, title, description, skills):\n",
    "        \"\"\"\n",
    "        Calculate scores for all 23 categories based on input\n",
    "        \"\"\"\n",
    "        # Initialize scores with priors\n",
    "        scores = {cat_id: self.category_priors.get(cat_id, 0.01) for cat_id in range(23)}\n",
    "        \n",
    "        # Combine text for analysis\n",
    "        text_lower = f\"{title} {description}\".lower()\n",
    "        skills_lower = [s.lower() for s in skills if s]\n",
    "        \n",
    "        # Common keywords for each category\n",
    "        category_keywords = {\n",
    "            0: ['administrative', 'admin', 'assistant', 'clerical', 'office'],\n",
    "            1: ['design', 'art', 'creative', 'ui', 'ux', 'graphic'],\n",
    "            2: ['business analyst', 'requirements', 'stakeholder', 'process'],\n",
    "            3: ['data', 'analytics', 'machine learning', 'python', 'sql', 'ai'],\n",
    "            4: ['devops', 'aws', 'cloud', 'docker', 'kubernetes', 'ci/cd'],\n",
    "            5: ['engineer', 'engineering', 'mechanical', 'electrical', 'civil'],\n",
    "            6: ['finance', 'accounting', 'financial', 'audit', 'tax'],\n",
    "            7: ['healthcare', 'medical', 'nurse', 'doctor', 'clinical'],\n",
    "            8: ['hr', 'human resources', 'recruiter', 'talent', 'people'],\n",
    "            9: ['it', 'information technology', 'help desk', 'support', 'technical'],\n",
    "            10: ['intern', 'internship', 'trainee', 'apprentice'],\n",
    "            11: ['legal', 'law', 'attorney', 'counsel', 'compliance'],\n",
    "            12: ['manager', 'management', 'director', 'head', 'lead'],\n",
    "            13: ['manufacturing', 'production', 'plant', 'factory'],\n",
    "            14: ['marketing', 'digital marketing', 'seo', 'content', 'social media'],\n",
    "            15: ['operations', 'logistics', 'supply chain', 'distribution'],\n",
    "            16: ['other', 'general', 'miscellaneous'],\n",
    "            17: ['project manager', 'project management', 'pmp', 'agile'],\n",
    "            18: ['quality', 'qa', 'test', 'assurance', 'testing'],\n",
    "            19: ['sales', 'account executive', 'business development', 'b2b'],\n",
    "            20: ['science', 'scientist', 'research', 'lab', 'r&d'],\n",
    "            21: ['software', 'developer', 'programming', 'coding', 'full stack'],\n",
    "            22: ['support', 'customer service', 'help desk', 'technical support']\n",
    "        }\n",
    "        \n",
    "        # Calculate keyword matches\n",
    "        for cat_id, keywords in category_keywords.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword in text_lower:\n",
    "                    scores[cat_id] += 0.05\n",
    "                # Check skills\n",
    "                for skill in skills_lower:\n",
    "                    if keyword in skill:\n",
    "                        scores[cat_id] += 0.03\n",
    "        \n",
    "        # Adjust based on seniority\n",
    "        seniority_score = self._calculate_seniority_score(title)\n",
    "        if seniority_score >= 4:\n",
    "            scores[10] *= 0.3  # Reduce internship for senior roles\n",
    "        elif seniority_score <= 1:\n",
    "            scores[12] *= 0.5  # Reduce management for junior roles\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def predict(self, job_title, job_description, skills=None, experience=None, remote=None):\n",
    "        \"\"\"\n",
    "        Predict job category for a new job posting\n",
    "        \"\"\"\n",
    "        if skills is None:\n",
    "            skills = []\n",
    "        \n",
    "        # Calculate seniority\n",
    "        seniority_score = self._calculate_seniority_score(job_title)\n",
    "        seniority_levels = ['Entry', 'Mid', 'Senior', 'Lead', 'Executive']\n",
    "        seniority_level = seniority_levels[min(seniority_score, 4)]\n",
    "        \n",
    "        # Calculate scores for all categories\n",
    "        category_scores = self._calculate_category_scores(job_title, job_description, skills)\n",
    "        \n",
    "        # Normalize scores to get probabilities\n",
    "        total_score = sum(category_scores.values())\n",
    "        if total_score > 0:\n",
    "            probabilities = {cat_id: score/total_score for cat_id, score in category_scores.items()}\n",
    "        else:\n",
    "            probabilities = {cat_id: 1/23 for cat_id in range(23)}\n",
    "        \n",
    "        # Get top 5 predictions\n",
    "        top_categories = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        \n",
    "        # Format predictions\n",
    "        predictions = []\n",
    "        for cat_id, prob in top_categories:\n",
    "            predictions.append({\n",
    "                'category_id': cat_id,\n",
    "                'category_name': self.category_mapping[cat_id],\n",
    "                'confidence': prob\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'primary_prediction': predictions[0],\n",
    "            'all_predictions': predictions,\n",
    "            'seniority_level': seniority_level,\n",
    "            'seniority_score': seniority_score,\n",
    "            'features_extracted': len(category_scores)\n",
    "        }\n",
    "    \n",
    "    def get_category_stats(self):\n",
    "        \"\"\"\n",
    "        Get statistics about categories from ensemble predictions\n",
    "        \"\"\"\n",
    "        if len(self.ensemble_df) == 0:\n",
    "            # Return default stats if no data\n",
    "            stats = []\n",
    "            for cat_id in range(23):\n",
    "                stats.append({\n",
    "                    'category_id': cat_id,\n",
    "                    'category_name': self.category_mapping[cat_id],\n",
    "                    'count': 0,\n",
    "                    'percentage': 100/23,\n",
    "                    'avg_confidence': 1/23\n",
    "                })\n",
    "            return pd.DataFrame(stats)\n",
    "        \n",
    "        stats = []\n",
    "        for cat_id in range(23):\n",
    "            cat_data = self.ensemble_df[self.ensemble_df['actual'] == cat_id]\n",
    "            stats.append({\n",
    "                'category_id': cat_id,\n",
    "                'category_name': self.category_mapping[cat_id],\n",
    "                'count': len(cat_data),\n",
    "                'percentage': len(cat_data) / len(self.ensemble_df) * 100 if len(self.ensemble_df) > 0 else 0,\n",
    "                'avg_confidence': cat_data['confidence'].mean() if len(cat_data) > 0 else 0\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(stats).sort_values('count', ascending=False)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STREAMLIT APP\n",
    "# ============================================================================\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(\n",
    "    page_title=\"Job Category Predictor\",\n",
    "    page_icon=\"🎯\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Custom CSS\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "    .main-header {\n",
    "        font-size: 3rem;\n",
    "        color: #1E88E5;\n",
    "        text-align: center;\n",
    "        margin-bottom: 1rem;\n",
    "    }\n",
    "    .prediction-box {\n",
    "        background-color: #f0f2f6;\n",
    "        padding: 2rem;\n",
    "        border-radius: 10px;\n",
    "        margin: 1rem 0;\n",
    "    }\n",
    "    .category-tag {\n",
    "        background-color: #1E88E5;\n",
    "        color: white;\n",
    "        padding: 0.3rem 0.8rem;\n",
    "        border-radius: 20px;\n",
    "        display: inline-block;\n",
    "        margin: 0.2rem;\n",
    "        font-size: 0.9rem;\n",
    "    }\n",
    "    .confidence-bar {\n",
    "        height: 25px;\n",
    "        background: linear-gradient(90deg, #1E88E5, #64B5F6);\n",
    "        border-radius: 12px;\n",
    "        margin: 0.5rem 0;\n",
    "        color: white;\n",
    "        padding-left: 10px;\n",
    "        line-height: 25px;\n",
    "        font-weight: bold;\n",
    "    }\n",
    "    .stButton>button {\n",
    "        width: 100%;\n",
    "        background-color: #1E88E5;\n",
    "        color: white;\n",
    "        font-weight: bold;\n",
    "        height: 50px;\n",
    "        font-size: 1.2rem;\n",
    "    }\n",
    "    .category-stats {\n",
    "        background-color: #ffffff;\n",
    "        padding: 1rem;\n",
    "        border-radius: 10px;\n",
    "        box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "        margin: 0.5rem 0;\n",
    "    }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Initialize session state\n",
    "if 'predictor' not in st.session_state:\n",
    "    with st.spinner('🔄 Loading Job Category Predictor...'):\n",
    "        # Check if ensemble_predictions.csv exists\n",
    "        if os.path.exists('ensemble_predictions.csv'):\n",
    "            st.session_state.predictor = JobCategoryPredictor('ensemble_predictions.csv')\n",
    "        else:\n",
    "            st.warning(\"⚠️ ensemble_predictions.csv not found. Using default settings.\")\n",
    "            st.session_state.predictor = JobCategoryPredictor()  # Will work with defaults\n",
    "        st.session_state.history = []\n",
    "        st.session_state.prediction_count = 0\n",
    "\n",
    "# Header\n",
    "st.markdown('<h1 class=\"main-header\">🎯 Job Category Predictor</h1>', unsafe_allow_html=True)\n",
    "st.markdown(\"### Powered by Ensemble Learning | 23 Job Categories\")\n",
    "\n",
    "# Sidebar\n",
    "with st.sidebar:\n",
    "    st.markdown(\"## 📊 Dashboard\")\n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    # Quick stats\n",
    "    st.markdown(\"### Quick Stats\")\n",
    "    stats = st.session_state.predictor.get_category_stats()\n",
    "    \n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        st.metric(\"Total Categories\", len(stats))\n",
    "    with col2:\n",
    "        st.metric(\"Predictions Made\", st.session_state.prediction_count)\n",
    "    \n",
    "    # Category distribution (simplified)\n",
    "    st.markdown(\"### Top Categories\")\n",
    "    for idx, row in stats.head(5).iterrows():\n",
    "        st.markdown(f\"\"\"\n",
    "        <div class=\"category-stats\">\n",
    "            <b>{row['category_name']}</b><br>\n",
    "            {row['percentage']:.1f}% of jobs\n",
    "        </div>\n",
    "        \"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"### About\")\n",
    "    st.info(\"\"\"\n",
    "    This app predicts job categories using an ensemble model trained on thousands of job postings.\n",
    "    \n",
    "    **Supported Categories:**\n",
    "    - Data Science\n",
    "    - Software Engineering\n",
    "    - DevOps\n",
    "    - Management\n",
    "    - And 19 more...\n",
    "    \"\"\")\n",
    "\n",
    "# Main content\n",
    "tab1, tab2, tab3 = st.tabs([\"🔮 Predict\", \"📊 Analytics\", \"📋 History\"])\n",
    "\n",
    "with tab1:\n",
    "    col1, col2 = st.columns([1, 1])\n",
    "    \n",
    "    with col1:\n",
    "        st.markdown(\"### 📝 Job Details\")\n",
    "        \n",
    "        # Input form\n",
    "        with st.form(\"prediction_form\"):\n",
    "            job_title = st.text_input(\n",
    "                \"Job Title *\", \n",
    "                placeholder=\"e.g., Senior Data Scientist\",\n",
    "                help=\"Enter the job title\"\n",
    "            )\n",
    "            \n",
    "            job_description = st.text_area(\n",
    "                \"Job Description *\", \n",
    "                height=150,\n",
    "                placeholder=\"Describe the role, responsibilities, and requirements...\",\n",
    "                help=\"Paste the full job description\"\n",
    "            )\n",
    "            \n",
    "            skills_input = st.text_input(\n",
    "                \"Skills (comma-separated)\", \n",
    "                placeholder=\"Python, SQL, Machine Learning\",\n",
    "                help=\"List key skills required\"\n",
    "            )\n",
    "            \n",
    "            experience = st.slider(\n",
    "                \"Years of Experience\",\n",
    "                min_value=0,\n",
    "                max_value=30,\n",
    "                value=0,\n",
    "                help=\"Required experience in years\"\n",
    "            )\n",
    "            \n",
    "            submitted = st.form_submit_button(\n",
    "                \"🎯 Predict Category\", \n",
    "                use_container_width=True\n",
    "            )\n",
    "    \n",
    "    with col2:\n",
    "        st.markdown(\"### 📋 Example Jobs\")\n",
    "        st.markdown(\"Click to load an example:\")\n",
    "        \n",
    "        examples = {\n",
    "            \"Data Scientist\": {\n",
    "                \"title\": \"Senior Data Scientist\",\n",
    "                \"desc\": \"Looking for an experienced data scientist with Python, machine learning, and SQL expertise to build predictive models. Must have experience with TensorFlow or PyTorch.\",\n",
    "                \"skills\": \"Python, Machine Learning, SQL, TensorFlow\",\n",
    "                \"exp\": 5\n",
    "            },\n",
    "            \"Software Engineer\": {\n",
    "                \"title\": \"Full Stack Developer\",\n",
    "                \"desc\": \"Develop and maintain web applications using React, Node.js, and PostgreSQL. Work in an agile team environment.\",\n",
    "                \"skills\": \"JavaScript, React, Node.js, SQL\",\n",
    "                \"exp\": 3\n",
    "            },\n",
    "            \"Legal Counsel\": {\n",
    "                \"title\": \"Corporate Legal Counsel\",\n",
    "                \"desc\": \"Provide legal advice on corporate matters, contracts, and compliance. Must have law degree and bar admission.\",\n",
    "                \"skills\": \"Contract Law, Corporate Law, Compliance\",\n",
    "                \"exp\": 8\n",
    "            },\n",
    "            \"Marketing Intern\": {\n",
    "                \"title\": \"Marketing Intern\",\n",
    "                \"desc\": \"Summer internship opportunity for students interested in digital marketing, social media, and content creation.\",\n",
    "                \"skills\": \"Social Media, Content Creation, Communication\",\n",
    "                \"exp\": 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Create buttons for examples\n",
    "        for name, example in examples.items():\n",
    "            if st.button(f\"📌 {name}\", key=f\"example_{name}\", use_container_width=True):\n",
    "                st.session_state.example_title = example['title']\n",
    "                st.session_state.example_desc = example['desc']\n",
    "                st.session_state.example_skills = example['skills']\n",
    "                st.session_state.example_exp = example['exp']\n",
    "                st.rerun()\n",
    "    \n",
    "    # Prediction area\n",
    "    if submitted:\n",
    "        if not job_title or not job_description:\n",
    "            st.error(\"❌ Please provide both Job Title and Job Description\")\n",
    "        else:\n",
    "            with st.spinner('🔮 Analyzing job posting...'):\n",
    "                time.sleep(1)  # Simulate processing\n",
    "                \n",
    "                # Parse skills\n",
    "                skills = [s.strip() for s in skills_input.split(',')] if skills_input else []\n",
    "                \n",
    "                # Make prediction\n",
    "                result = st.session_state.predictor.predict(\n",
    "                    job_title, \n",
    "                    job_description, \n",
    "                    skills, \n",
    "                    experience if experience > 0 else None\n",
    "                )\n",
    "                \n",
    "                # Update counter\n",
    "                st.session_state.prediction_count += 1\n",
    "                \n",
    "                # Add to history\n",
    "                st.session_state.history.append({\n",
    "                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'title': job_title[:50] + \"...\" if len(job_title) > 50 else job_title,\n",
    "                    'primary': result['primary_prediction']['category_name'],\n",
    "                    'confidence': f\"{result['primary_prediction']['confidence']:.1%}\",\n",
    "                    'seniority': result['seniority_level']\n",
    "                })\n",
    "                \n",
    "                # Display results\n",
    "                st.markdown(\"---\")\n",
    "                \n",
    "                col3, col4 = st.columns([1, 1])\n",
    "                \n",
    "                with col3:\n",
    "                    st.markdown(\"### 🎯 Primary Prediction\")\n",
    "                    \n",
    "                    primary = result['primary_prediction']\n",
    "                    \n",
    "                    # Create colored box for primary prediction\n",
    "                    confidence_pct = int(primary['confidence'] * 100)\n",
    "                    st.markdown(f\"\"\"\n",
    "                    <div style=\"background-color: #1E88E5; padding: 20px; border-radius: 10px; text-align: center;\">\n",
    "                        <h2 style=\"color: white; margin: 0;\">{primary['category_name']}</h2>\n",
    "                        <p style=\"color: white; font-size: 1.2rem; margin: 10px 0;\">Confidence: {primary['confidence']:.1%}</p>\n",
    "                        <div style=\"background-color: white; height: 10px; border-radius: 5px; margin: 10px 0;\">\n",
    "                            <div style=\"background-color: #FFC107; width: {confidence_pct}%; height: 10px; border-radius: 5px;\"></div>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                    \"\"\", unsafe_allow_html=True)\n",
    "                    \n",
    "                    st.markdown(f\"**Seniority Level:** {result['seniority_level']}\")\n",
    "                \n",
    "                with col4:\n",
    "                    st.markdown(\"### 📊 Top 5 Categories\")\n",
    "                    \n",
    "                    # Display top predictions with confidence bars\n",
    "                    for i, pred in enumerate(result['all_predictions'][:5], 1):\n",
    "                        confidence_pct = int(pred['confidence'] * 100)\n",
    "                        st.markdown(f\"\"\"\n",
    "                        <div style=\"margin: 10px 0;\">\n",
    "                            <b>{i}. {pred['category_name']}</b><br>\n",
    "                            <div style=\"background-color: #e0e0e0; height: 20px; border-radius: 5px; width: 100%%;\">\n",
    "                                <div style=\"background-color: #1E88E5; width: {confidence_pct}%%; height: 20px; border-radius: 5px; text-align: right; padding-right: 5px; color: white; line-height: 20px;\">\n",
    "                                    {pred['confidence']:.1%}\n",
    "                                </div>\n",
    "                            </div>\n",
    "                        </div>\n",
    "                        \"\"\", unsafe_allow_html=True)\n",
    "                \n",
    "                # Feature summary\n",
    "                st.markdown(\"### 🔍 Analysis Summary\")\n",
    "                col5, col6, col7 = st.columns(3)\n",
    "                \n",
    "                with col5:\n",
    "                    st.metric(\"Seniority Score\", result['seniority_score'])\n",
    "                with col6:\n",
    "                    st.metric(\"Skills Provided\", len(skills))\n",
    "                with col7:\n",
    "                    st.metric(\"Features Analyzed\", result['features_extracted'])\n",
    "\n",
    "with tab2:\n",
    "    st.markdown(\"### 📊 Category Analytics\")\n",
    "    \n",
    "    # Get stats\n",
    "    stats_df = st.session_state.predictor.get_category_stats()\n",
    "    \n",
    "    # Top categories bar chart\n",
    "    st.markdown(\"#### Top 10 Categories by Frequency\")\n",
    "    \n",
    "    # Prepare data for bar chart\n",
    "    chart_data = stats_df.head(10).set_index('category_name')['count']\n",
    "    st.bar_chart(chart_data, use_container_width=True)\n",
    "    \n",
    "    # Category distribution table\n",
    "    st.markdown(\"#### Category Details\")\n",
    "    \n",
    "    # Format the dataframe for display\n",
    "    display_df = stats_df[['category_name', 'count', 'percentage', 'avg_confidence']].copy()\n",
    "    display_df['percentage'] = display_df['percentage'].round(1).astype(str) + '%'\n",
    "    display_df['avg_confidence'] = display_df['avg_confidence'].round(3).apply(lambda x: f\"{x:.1%}\")\n",
    "    display_df.columns = ['Category', 'Count', '% of Total', 'Avg Confidence']\n",
    "    \n",
    "    st.dataframe(\n",
    "        display_df,\n",
    "        use_container_width=True,\n",
    "        hide_index=True\n",
    "    )\n",
    "    \n",
    "    # Model performance summary (if ensemble data available)\n",
    "    if len(st.session_state.predictor.ensemble_df) > 0:\n",
    "        st.markdown(\"### 📈 Model Performance\")\n",
    "        \n",
    "        df = st.session_state.predictor.ensemble_df\n",
    "        if 'actual' in df.columns and 'predicted' in df.columns:\n",
    "            accuracy = (df['actual'] == df['predicted']).mean()\n",
    "            \n",
    "            col1, col2, col3 = st.columns(3)\n",
    "            with col1:\n",
    "                st.metric(\"Overall Accuracy\", f\"{accuracy:.2%}\")\n",
    "            with col2:\n",
    "                st.metric(\"Total Samples\", f\"{len(df):,}\")\n",
    "            with col3:\n",
    "                st.metric(\"Categories\", \"23\")\n",
    "\n",
    "with tab3:\n",
    "    st.markdown(\"### 📋 Prediction History\")\n",
    "    \n",
    "    if st.session_state.history:\n",
    "        # Convert history to dataframe\n",
    "        history_df = pd.DataFrame(st.session_state.history)\n",
    "        \n",
    "        # Display history\n",
    "        st.dataframe(\n",
    "            history_df,\n",
    "            use_container_width=True,\n",
    "            hide_index=True\n",
    "        )\n",
    "        \n",
    "        # Clear history button\n",
    "        if st.button(\"🗑️ Clear History\", use_container_width=True):\n",
    "            st.session_state.history = []\n",
    "            st.rerun()\n",
    "    else:\n",
    "        st.info(\"No predictions yet. Try predicting a job category!\")\n",
    "\n",
    "# Footer\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\n",
    "    \"<p style='text-align: center; color: gray;'>\"\n",
    "    \"Made with ❤️ using Streamlit | Job Category Predictor v1.0\"\n",
    "    \"</p>\", \n",
    "    unsafe_allow_html=True\n",
    ")\n",
    "''')\n",
    "\n",
    "print(\"✅ File created successfully with UTF-8 encoding!\")\n",
    "print(\"\\nNow open a NEW terminal window (not in Jupyter) and run:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"conda activate ray-env\")\n",
    "print(\"streamlit run job_predictor_app.py\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee42f61-a035-4452-96af-4c8eb28a66f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
