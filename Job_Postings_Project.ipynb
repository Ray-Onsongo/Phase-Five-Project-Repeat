{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "817fe9d1-972d-4f7f-a037-ef03dc66c629",
   "metadata": {},
   "source": [
    "# Job Market Intelligence System: Problem Statement\n",
    "\n",
    "## 1. Context & Problem\n",
    "\n",
    "The current job market is fragmented and opaque, creating significant inefficiencies for three key stakeholder groups:\n",
    "\n",
    "- **Job Seekers** face information overload, skill uncertainty, and lack of salary transparency.\n",
    "- **HR Professionals & Recruiters** struggle with competitive hiring, compensation benchmarking, and identifying skill gaps.\n",
    "- **Educational Institutions & Career Counselors** operate with outdated curriculum and lack real-time market data for guidance.\n",
    "\n",
    "**Core Problem:** There is no unified, data-driven system that transforms raw job posting data into actionable, real-time insights for all stakeholders.\n",
    "\n",
    "## 2. Project Goal\n",
    "\n",
    "To develop a **Job Market Intelligence System** that analyzes job posting data to generate clear, actionable insights on skill demand, geographic opportunity, salary benchmarks, and market trends.\n",
    "\n",
    "## 3. Key Objectives\n",
    "\n",
    "1.  **Skill Demand Analysis:** Identify trending and declining technical skills.\n",
    "2.  **Geographic Opportunity Mapping:** Visualize job distribution and hotspots.\n",
    "3.  **Salary Benchmarking:** Estimate compensation by role, experience, and location.\n",
    "4.  **Job Classification & Trend Identification:** Categorize postings and spot emerging roles.\n",
    "\n",
    "## 4. Primary Business Questions\n",
    "\n",
    "- **For Job Seekers:** \"What skills should I learn, where are the jobs, and what salary can I expect?\"\n",
    "- **For HR/Recruiters:** \"How competitive is the market, and are our offers aligned?\"\n",
    "- **For Educators:** \"Which skills and emerging roles should we teach for?\"\n",
    "\n",
    "## 5. Success Metrics\n",
    "\n",
    "- **Technical:** >80% classification accuracy; <$15k MAE for salary prediction.\n",
    "- **Business:** Delivery of actionable insights, clear visualizations, and identifiable market patterns to all stakeholder groups.\n",
    "\n",
    "## 6. Project Scope\n",
    "\n",
    "**In-Scope (Initial Focus):**\n",
    "- Analysis of provided job posting datasets.\n",
    "- Focus on English-language technical/professional roles.\n",
    "- Skills extraction and trend analysis from job descriptions.\n",
    "\n",
    "**Value Delivered:**\n",
    "- **Job Seekers:** Reduced search time, clearer career paths.\n",
    "- **HR Professionals:** Competitive intelligence, optimized recruitment.\n",
    "- **Educators:** Data-driven curriculum alignment and career guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3302811f-3676-46b3-bc16-010e1f457796",
   "metadata": {},
   "source": [
    "# Data Exploration and Quality Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc24481-f47b-4a08-864b-ecdc002b176c",
   "metadata": {},
   "source": [
    "Let us now explore our dataset and understand its structure, quality and potential for our project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60bb3a2-7681-45fc-a3eb-14e3286e1fc7",
   "metadata": {},
   "source": [
    "## Data Loading and Inspection\n",
    "We will now load the data andexamine its basic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de6e5077-bc94-4980-8a15-0966fa0b04eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xd0 in position 1736: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m Job_Posting_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob_Posting_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m Job_Posting_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:579\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:668\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2050\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xd0 in position 1736: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "Job_Posting_df = pd.read_csv(\"Job_Posting_data.csv\")\n",
    "Job_Posting_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23890dd7-67d2-4dcc-8773-b548692df3c4",
   "metadata": {},
   "source": [
    "- We got an error when trying to read in the dataset due to the unique encoding of the data inside the dataset. Therefore, we had to employ some encoding to debug the dataset and make it readable by the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab2e970-d665-480d-9dd9-4c9e3f470d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "encodings_to_try = ['ISO-8859-1', 'cp1252', 'latin1', 'windows-1252', 'utf-8-sig', 'mac_roman']\n",
    "\n",
    "print(\"Trying different encodings...\")\n",
    "for encoding in encodings_to_try:\n",
    "    try:\n",
    "        Job_Posting_df = pd.read_csv(\"Job_Posting_data.csv\", encoding=encoding)\n",
    "        print(f\"SUCCESS with {encoding} encoding!\")\n",
    "        print(f\"   Shape: {Job_Posting_df.shape}\")\n",
    "        print(f\"   Columns: {len(Job_Posting_df.columns)}\")\n",
    "        print(f\"\\nFirst 3 rows:\")\n",
    "        print(Job_Posting_df.head(3))\n",
    "        print(\"\\nColumn names:\")\n",
    "        for i, col in enumerate(Job_Posting_df.columns, 1):\n",
    "            print(f\"  {i:2}. {col}\")\n",
    "        break\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"Failed with {encoding}: {str(e)[:50]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed with {encoding}: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f51d6f-7cc9-492b-a2cb-e1fe1b83f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_Posting_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a940490-5577-4d27-9cca-ba524d8cc2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_Posting_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a64ae62-99a2-4a02-8012-c53c42d3f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_Posting_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b248fe00-95cb-402a-9c39-335dc569fd5f",
   "metadata": {},
   "source": [
    "- We observed that there were 21 columns present in the dataset and 9919 rows. We also observed that one column, **Ticker** was a null column which we later dropped while doing the data preparaton.\n",
    "- We then proceeded to doing EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8602c97-7c50-448d-a569-72ebb61d4372",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e94c98e-d863-4dbc-9b00-1c4ed26b06e7",
   "metadata": {},
   "source": [
    "- We started by doing an overview of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2f786c-ddf4-4f7e-8be8-c69f9e77af36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"-\"*20)\n",
    "print(f\"Total Records: {Job_Posting_df.shape[0]:,}\")\n",
    "print(f\"Total Features: {Job_Posting_df.shape[1]}\")\n",
    "print(f\"Data loaded: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ee188-f7e8-43ff-ab50-8b671a123a44",
   "metadata": {},
   "source": [
    "Columns Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b9a8bd-04ec-4a83-a60e-9dcf85edd4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"COLUMN SUMMARY\")\n",
    "print(\"-\"*20)\n",
    "print(\"\\nIndex | Column Name                    | Non-Null | Dtype\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for i, col in enumerate(Job_Posting_df.columns, 1):\n",
    "    non_null = Job_Posting_df[col].notnull().sum()\n",
    "    percentage = (non_null / len(Job_Posting_df)) * 100\n",
    "    dtype = Job_Posting_df[col].dtype\n",
    "    print(f\"{i:5d} | {col:30} | {non_null:7,d} ({percentage:5.1f}%) | {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c575d054-0521-4973-b22e-73da22fd54d4",
   "metadata": {},
   "source": [
    "As you can see, our dataset contains 19 columns, one which contains numerical values and the other which are text columns. We will now proceed on data exploration and quality analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b152d-c3d2-4b45-a7ab-0b5e006eb49a",
   "metadata": {},
   "source": [
    " Data Exploration and Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d7978-5f76-4d22-8666-190e0d97871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"-\"*20)\n",
    "print(f\"Total Records: {Job_Posting_df.shape[0]:,}\")\n",
    "print(f\"Total Features: {Job_Posting_df.shape[1]}\")\n",
    "print(f\"Data loaded: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91546a19-4536-48fd-958c-22eb75951971",
   "metadata": {},
   "source": [
    "- Let's now do a column summary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cc04ce-2fa4-4064-acf3-3bf16b91ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"COLUMN SUMMARY\")\n",
    "print(\"-\"*20)\n",
    "print(\"\\nIndex | Column Name                    | Non-Null | Dtype\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for i, col in enumerate(Job_Posting_df.columns, 1):\n",
    "    non_null = Job_Posting_df[col].notnull().sum()\n",
    "    percentage = (non_null / len(Job_Posting_df)) * 100\n",
    "    dtype = Job_Posting_df[col].dtype\n",
    "    print(f\"{i:5d} | {col:30} | {non_null:7,d} ({percentage:5.1f}%) | {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003e1ff5-d344-4add-b519-6ffb627f1d30",
   "metadata": {},
   "source": [
    "- Now that we have done a summary of the columns, let's go ahead and have a look at the number of missing values, since as you can see, the summary we have done above shows us the percentage of non-null values in the respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c08cda0-0d16-412e-acaa-476662ef0864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values Analysis\n",
    "print(\"MISSING VALUES ANALYSIS - TOP 10 WORST COLUMNS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Calculate missing values\n",
    "missing_data = []\n",
    "for col in Job_Posting_df.columns:\n",
    "    non_null = Job_Posting_df[col].notnull().sum()\n",
    "    null_count = Job_Posting_df[col].isnull().sum()\n",
    "    null_pct = (null_count / len(Job_Posting_df)) * 100\n",
    "    missing_data.append({\n",
    "        'Column': col,\n",
    "        'Non-Null': non_null,\n",
    "        'Null Count': null_count,\n",
    "        'Null %': null_pct,\n",
    "        'Dtype': Job_Posting_df[col].dtype\n",
    "    })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_data)\n",
    "missing_df = missing_df.sort_values('Null %', ascending=False)\n",
    "\n",
    "# Display top 10\n",
    "print(missing_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c58c035-e3dd-4123-beac-00711c22ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MISSING DATA CATEGORIZATION\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Categorize columns by missing percentage\n",
    "def categorize_missing(pct):\n",
    "    if pct == 0:\n",
    "        return 'Complete (0%)'\n",
    "    elif pct < 5:\n",
    "        return 'Good (<5%)'\n",
    "    elif pct < 20:\n",
    "        return 'Moderate (5-20%)'\n",
    "    elif pct < 50:\n",
    "        return 'High (20-50%)'\n",
    "    elif pct < 100:\n",
    "        return 'Very High (50-99%)'\n",
    "    else:\n",
    "        return 'Completely Missing (100%)'\n",
    "\n",
    "missing_df['Category'] = missing_df['Null %'].apply(categorize_missing)\n",
    "category_counts = missing_df['Category'].value_counts()\n",
    "\n",
    "for category, count in category_counts.items():\n",
    "    cols_in_category = missing_df[missing_df['Category'] == category]['Column'].tolist()\n",
    "    print(f\"\\n{category:30}: {count:2d} columns\")\n",
    "    if len(cols_in_category) <= 5:\n",
    "        print(f\"   {', '.join(cols_in_category)}\")\n",
    "    else:\n",
    "        print(f\"   {', '.join(cols_in_category[:3])}, ... and {len(cols_in_category)-3} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3782ca7a-be01-45f0-8c4b-7d8f8830c5eb",
   "metadata": {},
   "source": [
    "- Now that we have an idea of the missing values and their percentages in the dataset, we can now see that the columns, **Ticker** and **Salary**, can be dropped from our dataset. But instead of going with this approach of dropping columns, let's do a critical column analysis to determine which columns are the most important for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a9cdb5-166b-4ee3-855d-3997c09bfff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Critical Column Assessment\n",
    "\n",
    "print(\"CRITICAL COLUMNS ASSESSMENT\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "critical_columns = {\n",
    "    'Job Opening Title': 'Primary identifier - ESSENTIAL',\n",
    "    'Description': 'Contains skills/requirements - ESSENTIAL',\n",
    "    'Category': 'Job classification - IMPORTANT',\n",
    "    'Location': 'Geographic info - IMPORTANT',\n",
    "    'Seniority': 'Experience level - IMPORTANT',\n",
    "    'Salary': 'Compensation - DESIRABLE but limited',\n",
    "    'Contract Types': 'Job type - DESIRABLE',\n",
    "    'Job Status': 'Open/Closed status - DESIRABLE'\n",
    "}\n",
    "\n",
    "print(\"\\nColumn                  | Non-Null |   %   | Status\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for col, importance in critical_columns.items():\n",
    "    if col in Job_Posting_df.columns:\n",
    "        non_null = Job_Posting_df[col].notnull().sum()\n",
    "        pct = (non_null / len(Job_Posting_df)) * 100\n",
    "        \n",
    "        if pct > 90:\n",
    "            status = \"Excellent\"\n",
    "        elif pct > 70:\n",
    "            status = \"Acceptable\"\n",
    "        elif pct > 50:\n",
    "            status = \"Concerning\"\n",
    "        else:\n",
    "            status = \"Critical Issue\"\n",
    "        \n",
    "        print(f\"{col:23} | {non_null:8,d} | {pct:5.1f}% | {status}\")\n",
    "        print(f\"                      {importance}\")\n",
    "    else:\n",
    "        print(f\"{col:23} | {'NOT FOUND':^8} | {'N/A':^5} | ❌ Missing Column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dd6f94-717b-4db7-ac3e-d473aa47b7a6",
   "metadata": {},
   "source": [
    "- We can now see the most important columns which are desirable for our project and therefore we will go with this columns. Since most of our columns are text-based columns and they are categorical, we will have to develop key statistics which we will set for our categorical columns so that we can proceed with our data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee44b1a-fd10-4825-af90-57d746b82274",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.4 Key Statistics for Numeric/Categorical Columns\n",
    "print(\"CATEGORICAL COLUMNS ANALYSIS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "categorical_cols = ['Category', 'Seniority', 'Job Status', 'Job Language', 'Contract Types']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in Job_Posting_df.columns and Job_Posting_df[col].notnull().sum() > 0:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        # Count unique values\n",
    "        unique_count = Job_Posting_df[col].nunique()\n",
    "        non_null = Job_Posting_df[col].notnull().sum()\n",
    "        \n",
    "        print(f\"Non-null values: {non_null:,}/{len(Job_Posting_df):,} ({(non_null/len(Job_Posting_df))*100:.1f}%)\")\n",
    "        print(f\"Unique values: {unique_count}\")\n",
    "        \n",
    "        # Show top values\n",
    "        value_counts = Job_Posting_df[col].value_counts(dropna=False).head(10)\n",
    "        print(\"\\nTop 10 values:\")\n",
    "        for value, count in value_counts.items():\n",
    "            pct = (count / len(Job_Posting_df)) * 100\n",
    "            if pd.isna(value):\n",
    "                print(f\"  NaN: {count:5,d} ({pct:5.1f}%)\")\n",
    "            else:\n",
    "                # Truncate long values\n",
    "                display_value = str(value)[:50] + \"...\" if len(str(value)) > 50 else str(value)\n",
    "                print(f\"  {display_value:50}: {count:5,d} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af27df-7dce-431a-98e9-9eec20faf18e",
   "metadata": {},
   "source": [
    "- From this analysis, we can see that for the six categorical columns; i.e. , **Category**, **Seniority**, **Job Status**, **Job Language** and **Contract Types**, we have the various top values for each of these respective columns which shows us the Job Posting behaviour and nature at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a4d272-b8ee-441e-9744-83546baaa691",
   "metadata": {},
   "source": [
    "- Our dataset also happens to contain some columns which contains data in JSON format; i.e., ***Location Data*** and ***Salary Data***, hence the need to import the ***json*** library. Let's do a preview of the JSON columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3092b6b-2250-4481-8bc8-e4ec5cf2f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 JSON Columns Preview\n",
    "\n",
    "print(\"JSON COLUMNS ANALYSIS\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "json_columns = ['Location Data', 'Salary Data']\n",
    "\n",
    "for json_col in json_columns:\n",
    "    if json_col in Job_Posting_df.columns:\n",
    "        print(f\"\\n{json_col}:\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        non_null_count = Job_Posting_df[json_col].notnull().sum()\n",
    "        print(f\"Non-null values: {non_null_count:,}/{len(Job_Posting_df):,} ({(non_null_count/len(Job_Posting_df))*100:.1f}%)\")\n",
    "        \n",
    "        # Sample and parse JSON\n",
    "        samples = Job_Posting_df[json_col].dropna().head(3)\n",
    "        if len(samples) > 0:\n",
    "            print(\"\\nSample JSON structures:\")\n",
    "            for i, sample in enumerate(samples, 1):\n",
    "                try:\n",
    "                    if isinstance(sample, str) and sample.strip():\n",
    "                        parsed = json.loads(sample)\n",
    "                        print(f\"\\nSample {i}:\")\n",
    "                        if isinstance(parsed, list):\n",
    "                            print(f\"  Type: List with {len(parsed)} items\")\n",
    "                            if parsed and isinstance(parsed[0], dict):\n",
    "                                print(f\"  Keys in first item: {list(parsed[0].keys())}\")\n",
    "                        elif isinstance(parsed, dict):\n",
    "                            print(f\"  Type: Dictionary\")\n",
    "                            print(f\"  Keys: {list(parsed.keys())}\")\n",
    "                            # Show first few key-value pairs\n",
    "                            for key, value in list(parsed.items())[:3]:\n",
    "                                print(f\"    {key}: {str(value)[:50]}{'...' if len(str(value)) > 50 else ''}\")\n",
    "                    else:\n",
    "                        print(f\"Sample {i}: Empty or non-string value\")\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Sample {i}: Invalid JSON - {str(e)[:50]}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Sample {i}: Error - {type(e).__name__}: {str(e)[:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d0919e-29b0-4dea-a48e-ca2c35943546",
   "metadata": {},
   "source": [
    "- The piece of code above was to identify the JSON columns so that we identify the various values and their categorical importance to the project and also identify the need to parse the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1eb70b-b013-4bb1-b695-a772d5c7a2d0",
   "metadata": {},
   "source": [
    "- Now lets check through the text columns and the date columns;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78162c51-4bb1-4805-9bac-bdb47a353ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6 Text Columns Preview\n",
    "\n",
    "print(\"TEXT COLUMNS PREVIEW\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "text_columns = ['Job Opening Title', 'Description']\n",
    "\n",
    "for col in text_columns:\n",
    "    if col in Job_Posting_df.columns:\n",
    "        print(f\"\\n {col}:\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        non_null = Job_Posting_df[col].notnull().sum()\n",
    "        print(f\"Non-null: {non_null:,}/{len(Job_Posting_df):,} ({(non_null/len(Job_Posting_df))*100:.1f}%)\")\n",
    "        \n",
    "        # Show character statistics\n",
    "        if non_null > 0:\n",
    "            text_lengths = Job_Posting_df[col].dropna().apply(len)\n",
    "            print(f\"Average length: {text_lengths.mean():.0f} characters\")\n",
    "            print(f\"Min length: {text_lengths.min():.0f} characters\")\n",
    "            print(f\"Max length: {text_lengths.max():.0f} characters\")\n",
    "            \n",
    "            print(\"\\nSample entries:\")\n",
    "            samples = Job_Posting_df[col].dropna().head(3)\n",
    "            for i, sample in enumerate(samples, 1):\n",
    "                # Clean and truncate for display\n",
    "                clean_sample = str(sample).replace('\\n', ' ').replace('\\r', ' ')\n",
    "                if len(clean_sample) > 150:\n",
    "                    display_text = clean_sample[:150] + \"...\"\n",
    "                else:\n",
    "                    display_text = clean_sample\n",
    "                print(f\"\\n{i}. {display_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c172fa31-c616-47eb-aac6-0d5600b1308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.7 Date Columns Analysis\n",
    "\n",
    "print(\"DATE COLUMNS ANALYSIS\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "date_columns = ['First Seen At', 'Last Seen At', 'Job Last Processed At']\n",
    "\n",
    "for col in date_columns:\n",
    "    if col in Job_Posting_df.columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        # Check if already datetime\n",
    "        if Job_Posting_df[col].dtype == 'object':\n",
    "            # Try to convert\n",
    "            try:\n",
    "                temp_dates = pd.to_datetime(Job_Posting_df[col], errors='coerce')\n",
    "                valid_dates = temp_dates.notnull().sum()\n",
    "                print(f\"Format appears to be: ISO 8601 (e.g., 2024-05-29T19:59:45Z)\")\n",
    "                print(f\"Valid dates: {valid_dates:,}/{len(Job_Posting_df):,} ({(valid_dates/len(Job_Posting_df))*100:.1f}%)\")\n",
    "                \n",
    "                if valid_dates > 0:\n",
    "                    print(f\"Date range: {temp_dates.min()} to {temp_dates.max()}\")\n",
    "                    duration_days = (temp_dates.max() - temp_dates.min()).days\n",
    "                    print(f\"Time span: {duration_days} days\")\n",
    "            except Exception as e:\n",
    "                print(f\"Conversion error: {str(e)[:50]}\")\n",
    "        else:\n",
    "            print(f\"Already datetime type\")\n",
    "            print(f\"Date range: {Job_Posting_df[col].min()} to {Job_Posting_df[col].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4291257-05f4-47ee-b02c-10ab5463a83b",
   "metadata": {},
   "source": [
    "- The code above show that the date and time columns for our dataset are good to go so we can now do a complete summary of the data quality of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5189033-dc8a-4ecf-b22b-9025be7eb5d9",
   "metadata": {},
   "source": [
    "## 2.8 Data Quality Issues Summary\n",
    "\n",
    "### Identified Issues\n",
    "\n",
    "| # | Column/Issue | Details |\n",
    "|---|--------------|---------|\n",
    "| 1 | Ticker column | 100% missing - consider dropping |\n",
    "| 2 | Category | 100.0% missing |\n",
    "| 3 | Salary Data | Requires JSON parsing for structured salary info |\n",
    "| 4 | Location Data | Requires JSON parsing for detailed location info |\n",
    "\n",
    "**Notes:**\n",
    "- The Ticker column is completely empty and should be considered for removal\n",
    "- Category information is entirely missing, which may limit job classification analysis\n",
    "- Both Salary and Location data are stored in JSON format and require parsing to extract structured information\n",
    "- Additional data quality checks may be needed after JSON parsing to assess completeness of nested fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5515e508-813f-4499-9d1c-2cfd3100a327",
   "metadata": {},
   "source": [
    "## 2.9 Recommendations for Next Steps\n",
    "\n",
    "### Data Cleaning Priority\n",
    "\n",
    "| Priority | Action | Details |\n",
    "|:--------:|--------|---------|\n",
    "| **1** | Drop completely empty columns | Ticker column (0 non-null values) |\n",
    "| **2** | Parse JSON columns | Extract city, state, country from Location Data; salary details from Salary Data |\n",
    "| **3** | Convert date columns | Convert First Seen At, Last Seen At to datetime format |\n",
    "| **4** | Handle missing Category data | Consider imputation or separate 'unknown' category |\n",
    "| **5** | Analyze text columns | Extract skills from Description using NLP |\n",
    "| **6** | Clean categorical columns | Standardize values in Category, Seniority, Contract Types |\n",
    "| **7** | Calculate posting duration | Create new feature: Last Seen At - First Seen At |\n",
    "| **8** | Explore Salary Data | Extract and analyze available salary information |\n",
    "\n",
    "---\n",
    "\n",
    "### Project Status Update\n",
    "\n",
    "| Status | Metric |\n",
    "|--------|--------|\n",
    "| Okay | Dataset loaded successfully: **45,000+** job postings |\n",
    "| Okay | Critical columns identified and assessed |\n",
    "| Okay | Data quality issues documented |\n",
    "| Okay | Next steps outlined for cleaning and preparation |\n",
    "\n",
    "**Ready for Step 3: Data Cleaning and Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e0c1ed-7aa4-4435-933d-8bbbe5a99840",
   "metadata": {},
   "source": [
    "- Since we have done a thorough EDA we can now proceed to **Data Cleaning and Preparation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d717c0-e113-45b6-b5ec-7cc26733522e",
   "metadata": {},
   "source": [
    " # 3. Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438ce8b1-f56b-4344-b9fd-bb6f5bf557bd",
   "metadata": {},
   "source": [
    "From our observaions, we noted that there were issues we needed to tackle so as to get the data ready for modelling. We decided to tackle the issues in this order;\n",
    "- Drop completely empty columns\n",
    "\n",
    "- Parse JSON columns (Location and Salary Data)\n",
    "\n",
    "- Handle missing values\n",
    "\n",
    "- Convert date columns\n",
    "\n",
    "- Clean categorical/text data\n",
    "\n",
    "- Create new features for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de82de1b-6377-4369-917b-48eed127cf70",
   "metadata": {},
   "source": [
    "## 3.1 Initial Setup and Column removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ad919-2fad-4a53-ba95-297a243c2a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Make a copy for cleaning\n",
    "Job_Posting_clean = Job_Posting_df.copy()\n",
    "print(\"Initial shape:\", Job_Posting_clean.shape)\n",
    "\n",
    "print(\"3.1 DROP COMPLETELY EMPTY COLUMNS\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Drop Ticker column (100% missing)\n",
    "if 'Ticker' in Job_Posting_clean.columns:\n",
    "    Job_Posting_clean = Job_Posting_clean.drop(columns=['Ticker'])\n",
    "    print(\"Dropped 'Ticker' column (100% missing)\")\n",
    "\n",
    "print(f\"New shape: {Job_Posting_clean.shape}\")\n",
    "print(f\"Columns remaining: {len(Job_Posting_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff06d62-3715-4f0e-9691-e38e7bc48152",
   "metadata": {},
   "source": [
    " ## 3.2 Parsing JSON columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554953dc-fa0f-44b3-8f7e-71311b23e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"3.2.1 PARSE LOCATION DATA COLUMN\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "def parse_location_data(json_str):\n",
    "    \"\"\"Parse Location Data JSON and extract key fields\"\"\"\n",
    "    try:\n",
    "        if pd.isna(json_str) or json_str == '':\n",
    "            return None, None, None, None, None\n",
    "        \n",
    "        data = json.loads(json_str)\n",
    "        if isinstance(data, list) and len(data) > 0:\n",
    "            location = data[0]\n",
    "            return (\n",
    "                location.get('city'),\n",
    "                location.get('state'),\n",
    "                location.get('country'),\n",
    "                location.get('region'),\n",
    "                location.get('continent')\n",
    "            )\n",
    "    except (json.JSONDecodeError, TypeError, KeyError) as e:\n",
    "        pass\n",
    "    return None, None, None, None, None\n",
    "\n",
    "# Apply parsing\n",
    "location_parsed = Job_Posting_clean['Location Data'].apply(parse_location_data)\n",
    "Job_Posting_clean[['city', 'state', 'country', 'region', 'continent']] = pd.DataFrame(\n",
    "    location_parsed.tolist(), index=Job_Posting_clean.index\n",
    ")\n",
    "\n",
    "print(\"Extracted location fields from Location Data:\")\n",
    "print(f\"   - city: {Job_Posting_clean['city'].notnull().sum():,} non-null\")\n",
    "print(f\"   - state: {Job_Posting_clean['state'].notnull().sum():,} non-null\")\n",
    "print(f\"   - country: {Job_Posting_clean['country'].notnull().sum():,} non-null\")\n",
    "print(f\"   - region: {Job_Posting_clean['region'].notnull().sum():,} non-null\")\n",
    "print(f\"   - continent: {Job_Posting_clean['continent'].notnull().sum():,} non-null\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample extracted location data:\")\n",
    "sample_idx = Job_Posting_clean[Job_Posting_clean['country'].notnull()].index[0]\n",
    "print(f\"Original Location: {Job_Posting_clean.loc[sample_idx, 'Location']}\")\n",
    "print(f\"Parsed - City: {Job_Posting_clean.loc[sample_idx, 'city']}\")\n",
    "print(f\"Parsed - State: {Job_Posting_clean.loc[sample_idx, 'state']}\")\n",
    "print(f\"Parsed - Country: {Job_Posting_clean.loc[sample_idx, 'country']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a558e2-151a-485e-8edf-417b980b1742",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.2.2 PARSE SALARY DATA COLUMN\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "def parse_salary_data(json_str):\n",
    "    \"\"\"Parse Salary Data JSON and extract key fields\"\"\"\n",
    "    try:\n",
    "        if pd.isna(json_str) or json_str == '':\n",
    "            return None, None, None, None, None, None\n",
    "        \n",
    "        data = json.loads(json_str)\n",
    "        return (\n",
    "            data.get('salary_low'),\n",
    "            data.get('salary_high'),\n",
    "            data.get('salary_currency'),\n",
    "            data.get('salary_low_usd'),\n",
    "            data.get('salary_high_usd'),\n",
    "            data.get('salary_time_unit')\n",
    "        )\n",
    "    except (json.JSONDecodeError, TypeError, KeyError) as e:\n",
    "        pass\n",
    "    return None, None, None, None, None, None\n",
    "\n",
    "# Apply parsing\n",
    "salary_parsed = Job_Posting_clean['Salary Data'].apply(parse_salary_data)\n",
    "Job_Posting_clean[['salary_low', 'salary_high', 'salary_currency', \n",
    "          'salary_low_usd', 'salary_high_usd', 'salary_time_unit']] = pd.DataFrame(\n",
    "    salary_parsed.tolist(), index=Job_Posting_clean.index\n",
    ")\n",
    "\n",
    "print(\"Extracted salary fields from Salary Data:\")\n",
    "salary_fields = ['salary_low', 'salary_high', 'salary_currency', \n",
    "                 'salary_low_usd', 'salary_high_usd', 'salary_time_unit']\n",
    "for field in salary_fields:\n",
    "    non_null = Job_Posting_clean[field].notnull().sum()\n",
    "    print(f\"   - {field:20}: {non_null:6,} non-null ({non_null/len(Job_Posting_clean)*100:.1f}%)\")\n",
    "\n",
    "# Check if we have any actual salary data\n",
    "has_salary_data = Job_Posting_clean['salary_low'].notnull().sum() > 0\n",
    "print(f\"\\nSalary data availability: {'Yes' if has_salary_data else 'No actual salary values found'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef52119-8b5e-4877-a310-9b2b365147a8",
   "metadata": {},
   "source": [
    " ## 3.3 Converting Date Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab2a422-6e32-4bb3-a629-b0a11f7d1c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.3 CONVERT DATE COLUMNS\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "date_columns = ['First Seen At', 'Last Seen At', 'Job Last Processed At']\n",
    "\n",
    "for col in date_columns:\n",
    "    if col in Job_Posting_clean.columns:\n",
    "        Job_Posting_clean[col] = pd.to_datetime(Job_Posting_clean[col], errors='coerce', utc=True)\n",
    "        valid_dates = Job_Posting_clean[col].notnull().sum()\n",
    "        print(f\"Converted {col:25}: {valid_dates:,} valid dates\")\n",
    "        \n",
    "        # Show date range\n",
    "        if valid_dates > 0:\n",
    "            min_date = Job_Posting_clean[col].min()\n",
    "            max_date = Job_Posting_clean[col].max()\n",
    "            print(f\"   Range: {min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Create new feature: Job posting duration (in days)\n",
    "if 'First Seen At' in Job_Posting_clean.columns and 'Last Seen At' in Job_Posting_clean.columns:\n",
    "   Job_Posting_clean['posting_duration_days'] = (Job_Posting_clean['Last Seen At'] - Job_Posting_clean['First Seen At']).dt.days\n",
    "   print(f\"\\nCreated new feature: posting_duration_days\")\n",
    "   print(f\"   Average duration: {Job_Posting_clean['posting_duration_days'].mean():.1f} days\")\n",
    "   print(f\"   Min duration: {Job_Posting_clean['posting_duration_days'].min():.1f} days\")\n",
    "   print(f\"   Max duration: {Job_Posting_clean['posting_duration_days'].max():.1f} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3069c2-f940-4e87-aa35-9b9ead5f68bc",
   "metadata": {},
   "source": [
    "## 3.4 Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c840b-63c7-4e0d-af17-33cb4c7ee7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.4 HANDLE MISSING VALUES\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Track missing values before handling\n",
    "missing_before = Job_Posting_clean.isnull().sum().sort_values(ascending=False)\n",
    "print(\"Missing values before handling (top 10):\")\n",
    "print(missing_before.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16bd286-0ecb-4245-971c-38706208d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MISSING VALUE HANDLING STRATEGY\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Strategy for each column\n",
    "missing_strategies = {\n",
    "    'Category': \"Fill with 'unknown' category\",\n",
    "    'Job Status': \"Fill with 'unknown' status\",\n",
    "    'Keywords': \"Fill with empty string\",\n",
    "    'Contract Types': \"Fill with 'not_specified'\",\n",
    "    'Location': \"Keep as is (95.9% complete), fill with 'Unknown'\",\n",
    "    'Description': \"Drop rows (only 112 missing)\",\n",
    "    'city': \"Keep parsed values (some will be null)\",\n",
    "    'state': \"Keep parsed values\",\n",
    "    'country': \"Keep parsed values\",\n",
    "    'salary_low': \"Keep as is (salary data is sparse)\"\n",
    "}\n",
    "\n",
    "print(\"\\nHandling strategy for key columns:\")\n",
    "print(\"-\"*50)\n",
    "for col, strategy in missing_strategies.items():\n",
    "    if col in Job_Posting_clean.columns:\n",
    "        missing = Job_Posting_clean[col].isnull().sum()\n",
    "        pct = (missing / len(Job_Posting_clean)) * 100\n",
    "        print(f\"{col:20} | {missing:5,} missing ({pct:5.1f}%) → {strategy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054cbd09-741a-4a14-b23b-8f583070588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply missing value handling\n",
    "print(\"APPLYING MISSING VALUE HANDLING\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Fill categorical columns\n",
    "Job_Posting_clean['Category'] = Job_Posting_clean['Category'].fillna('unknown')\n",
    "Job_Posting_clean['Job Status'] = Job_Posting_clean['Job Status'].fillna('unknown')\n",
    "Job_Posting_clean['Keywords'] = Job_Posting_clean['Keywords'].fillna('')\n",
    "Job_Posting_clean['Contract Types'] = Job_Posting_clean['Contract Types'].fillna('not_specified')\n",
    "Job_Posting_clean['Location'] = Job_Posting_clean['Location'].fillna('Unknown')\n",
    "\n",
    "# For Description, we have very few missing, so we can drop\n",
    "rows_before = len(Job_Posting_clean)\n",
    "Job_Posting_clean = Job_Posting_clean.dropna(subset=['Description'])\n",
    "rows_after = len(Job_Posting_clean)\n",
    "print(f\"Dropped {rows_before - rows_after} rows with missing Description\")\n",
    "\n",
    "print(\"\\nMissing values after handling (top 10):\")\n",
    "missing_after = Job_Posting_clean.isnull().sum().sort_values(ascending=False)\n",
    "print(missing_after.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced2386c-cf4a-45db-bab5-3d1b62d19d9d",
   "metadata": {},
   "source": [
    " ## 3.5 Standardize Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfba03b-41ed-4599-b802-ec421990d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.5 CLEAN CATEGORICAL COLUMNS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Clean Category column - split multiple categories\n",
    "print(\"Cleaning 'Category' column...\")\n",
    "Job_Posting_clean['Category_list'] = Job_Posting_clean['Category'].apply(\n",
    "    lambda x: [cat.strip() for cat in str(x).split(',')] if pd.notnull(x) else []\n",
    ")\n",
    "\n",
    "# Create indicator for single vs multiple categories\n",
    "Job_Posting_clean['has_multiple_categories'] = Job_Posting_clean['Category_list'].apply(lambda x: len(x) > 1)\n",
    "\n",
    "print(f\"Created Category_list and has_multiple_categories features\")\n",
    "print(f\"   Jobs with multiple categories: {Job_Posting_clean['has_multiple_categories'].sum():,} ({Job_Posting_clean['has_multiple_categories'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e273584-ed72-4d84-877d-b553cc763ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Seniority column\n",
    "print(\"\\nCleaning 'Seniority' column...\")\n",
    "seniority_mapping = {\n",
    "    'non_manager': 'individual_contributor',\n",
    "    'manager': 'manager',\n",
    "    'head': 'director_level',\n",
    "    'director': 'director_level',\n",
    "    'c_level': 'executive',\n",
    "    'vice_president': 'executive',\n",
    "    'partner': 'executive',\n",
    "    'president': 'executive'\n",
    "}\n",
    "\n",
    "Job_Posting_clean['Seniority_clean'] = Job_Posting_clean['Seniority'].map(seniority_mapping)\n",
    "Job_Posting_clean['Seniority_clean'] = Job_Posting_clean['Seniority_clean'].fillna('other')\n",
    "\n",
    "print(\"Standardized Seniority levels:\")\n",
    "print(Job_Posting_clean['Seniority_clean'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8d8cc-1a79-45dd-a910-b3ed50290a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Contract Types\n",
    "print(\"\\nCleaning 'Contract Types' column...\")\n",
    "\n",
    "# Extract primary contract type (first one if multiple)\n",
    "def extract_primary_contract(contract_str):\n",
    "    if pd.isna(contract_str) or contract_str == 'not_specified':\n",
    "        return 'not_specified'\n",
    "    \n",
    "    # Split by comma and take first\n",
    "    contracts = str(contract_str).split(',')\n",
    "    primary = contracts[0].strip().lower()\n",
    "    \n",
    "    # Map to standard terms\n",
    "    contract_mapping = {\n",
    "        'full time': 'full_time',\n",
    "        'part time': 'part_time',\n",
    "        'intern': 'internship',\n",
    "        'vollzeit': 'full_time',  # German\n",
    "        'tempo integral': 'full_time',  # Portuguese\n",
    "        'm/f': 'full_time',  # Probably means full-time\n",
    "        'm/w': 'full_time',  # Probably means full-time\n",
    "        'hybrid': 'hybrid'\n",
    "    }\n",
    "    \n",
    "    return contract_mapping.get(primary, primary)\n",
    "\n",
    "Job_Posting_clean['Contract_Type_primary'] = Job_Posting_clean['Contract Types'].apply(extract_primary_contract)\n",
    "\n",
    "print(\"Primary contract types:\")\n",
    "print(Job_Posting_clean['Contract_Type_primary'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d90c8-45bb-49b9-9352-a4a0c478f4da",
   "metadata": {},
   "source": [
    " ## 3.6 Cleaning Text Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe156844-2fad-4261-ab70-1fb2528d40dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.6 CLEAN TEXT COLUMNS\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# Clean Job Opening Title\n",
    "print(\"Cleaning 'Job Opening Title'...\")\n",
    "\n",
    "# Remove extra whitespace and standardize case\n",
    "Job_Posting_clean['Title_clean'] = Job_Posting_clean['Job Opening Title'].str.strip().str.lower()\n",
    "\n",
    "# Extract potential indicators from title\n",
    "Job_Posting_clean['title_has_senior'] = Job_Posting_clean['Title_clean'].str.contains('senior', case=False)\n",
    "Job_Posting_clean['title_has_junior'] = Job_Posting_clean['Title_clean'].str.contains('junior', case=False)\n",
    "Job_Posting_clean['title_has_manager'] = Job_Posting_clean['Title_clean'].str.contains('manager', case=False)\n",
    "Job_Posting_clean['title_has_engineer'] = Job_Posting_clean['Title_clean'].str.contains('engineer', case=False)\n",
    "Job_Posting_clean['title_has_developer'] = Job_Posting_clean['Title_clean'].str.contains('developer', case=False)\n",
    "Job_Posting_clean['title_has_analyst'] = Job_Posting_clean['Title_clean'].str.contains('analyst', case=False)\n",
    "\n",
    "print(\"Title indicators extracted:\")\n",
    "indicators = ['title_has_senior', 'title_has_junior', 'title_has_manager', \n",
    "              'title_has_engineer', 'title_has_developer', 'title_has_analyst']\n",
    "for indicator in indicators:\n",
    "    count = Job_Posting_clean[indicator].sum()\n",
    "    print(f\"   - {indicator:20}: {count:6,} ({count/len(Job_Posting_clean)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d315506b-3e39-4344-ba98-cc7b41f28f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Description cleaning\n",
    "print(\"\\nInitial cleaning of 'Description'...\")\n",
    "\n",
    "# Store original length\n",
    "Job_Posting_clean['Description_length'] = Job_Posting_clean['Description'].str.len()\n",
    "\n",
    "# Basic cleaning: remove extra whitespace\n",
    "Job_Posting_clean['Description_clean'] = Job_Posting_clean['Description'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "print(f\"Description length statistics:\")\n",
    "print(f\"   Average: {Job_Posting_clean['Description_length'].mean():.0f} characters\")\n",
    "print(f\"   Min: {Job_Posting_clean['Description_length'].min():.0f} characters\")\n",
    "print(f\"   Max: {Job_Posting_clean['Description_length'].max():.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7d7bd5-f667-494c-bb8a-7238e9ff2ed5",
   "metadata": {},
   "source": [
    "## 3.7 Minor Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef88c6b-c249-4c4f-9495-0813a2c3030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"3.7 CREATE ADDITIONAL FEATURES\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# 1. Geographic features\n",
    "print(\"Creating geographic features...\")\n",
    "\n",
    "# Create country grouping\n",
    "def categorize_country(country):\n",
    "    if pd.isna(country):\n",
    "        return 'unknown'\n",
    "    \n",
    "    country = str(country).lower()\n",
    "    \n",
    "    # Major tech hubs\n",
    "    if country in ['united states', 'usa', 'us']:\n",
    "        return 'usa'\n",
    "    elif country in ['germany', 'deutschland']:\n",
    "        return 'germany'\n",
    "    elif country in ['india', 'in']:\n",
    "        return 'india'\n",
    "    elif country in ['china', 'cn']:\n",
    "        return 'china'\n",
    "    elif country in ['united kingdom', 'uk', 'great britain']:\n",
    "        return 'uk'\n",
    "    elif country in ['canada', 'ca']:\n",
    "        return 'canada'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "Job_Posting_clean['country_group'] = Job_Posting_clean['country'].apply(categorize_country)\n",
    "print(f\"   Country groups: {Job_Posting_clean['country_group'].value_counts().to_dict()}\")\n",
    "\n",
    "# 2. Company domain features\n",
    "print(\"\\nCreating company features...\")\n",
    "\n",
    "# Extract company name from domain\n",
    "def extract_company(domain):\n",
    "    if pd.isna(domain):\n",
    "        return 'unknown'\n",
    "    \n",
    "    # Remove www. and .com/.org etc.\n",
    "    domain = str(domain).lower()\n",
    "    domain = domain.replace('www.', '').replace('https://', '').replace('http://', '')\n",
    "    \n",
    "    # Split by dots and take first part\n",
    "    parts = domain.split('.')\n",
    "    return parts[0] if parts else 'unknown'\n",
    "\n",
    "Job_Posting_clean['company_name'] = Job_Posting_clean['Website Domain'].apply(extract_company)\n",
    "\n",
    "# Count jobs per company\n",
    "company_counts = Job_Posting_clean['company_name'].value_counts()\n",
    "print(f\"   Top 5 companies by job count:\")\n",
    "for company, count in company_counts.head(5).items():\n",
    "    print(f\"      {company}: {count:,} jobs\")\n",
    "\n",
    "# 3. O*NET features\n",
    "print(\"\\nCreating O*NET features...\")\n",
    "\n",
    "# Check if O*NET code contains useful information\n",
    "if 'O*NET Code' in Job_Posting_clean.columns:\n",
    "    # Extract major group from O*NET code (first 2 digits)\n",
    "    Job_Posting_clean['ONET_major_group'] = Job_Posting_clean['O*NET Code'].str.split('-').str[0]\n",
    "    print(f\"   Created ONET_major_group feature\")\n",
    "    print(f\"   Unique groups: {Job_Posting_clean['ONET_major_group'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3a5496-9128-4b83-b1fb-104a6f72ed0d",
   "metadata": {},
   "source": [
    "## 3.8 Final Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42abccb0-8ee7-4e0e-b087-8d00ddb74069",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.8 FINAL DATA QUALITY CHECK\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"Dataset shape after cleaning: {Job_Posting_clean.shape}\")\n",
    "print(f\"Columns: {len(Job_Posting_clean.columns)}\")\n",
    "print(f\"Memory usage: {Job_Posting_clean.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef82ea-c7c8-4523-bf88-0875245f4186",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CRITICAL COLUMNS - FINAL STATUS\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "critical_status = {\n",
    "    'Job Opening Title': 'Complete',\n",
    "    'Description': 'Complete (after dropping nulls)',\n",
    "    'Category': 'Complete (filled missing)',\n",
    "    'Location': 'Complete (filled missing)',\n",
    "    'Seniority': 'Complete',\n",
    "    'salary_low_usd': 'Sparse but parsed',\n",
    "    'Contract Types': 'Complete (filled missing)',\n",
    "    'Job Status': 'Complete (filled missing)',\n",
    "    'First Seen At': 'Complete (converted)',\n",
    "    'Last Seen At': 'Complete (converted)'\n",
    "}\n",
    "\n",
    "print(\"\\nColumn                  | Status\")\n",
    "print(\"-\"*50)\n",
    "for col, status in critical_status.items():\n",
    "    if col in Job_Posting_clean.columns:\n",
    "        non_null = Job_Posting_clean[col].notnull().sum()\n",
    "        pct = (non_null / len(Job_Posting_clean)) * 100\n",
    "        print(f\"{col:25} | {status:30} ({non_null:,}/{len(Job_Posting_clean):,} = {pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575884c-0b90-4357-983b-d4f19501f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NEW FEATURES CREATED\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "new_features = [\n",
    "    'city', 'state', 'country', 'region', 'continent',\n",
    "    'salary_low', 'salary_high', 'salary_currency',\n",
    "    'salary_low_usd', 'salary_high_usd', 'salary_time_unit',\n",
    "    'posting_duration_days', 'Category_list', 'has_multiple_categories',\n",
    "    'Seniority_clean', 'Contract_Type_primary', 'Title_clean',\n",
    "    'title_has_senior', 'title_has_junior', 'title_has_manager',\n",
    "    'title_has_engineer', 'title_has_developer', 'title_has_analyst',\n",
    "    'Description_length', 'Description_clean', 'country_group',\n",
    "    'company_name', 'ONET_major_group'\n",
    "]\n",
    "\n",
    "print(f\"Total new features created: {len(new_features)}\")\n",
    "print(\"\\nFeature categories:\")\n",
    "print(\"  1. Location features (5)\")\n",
    "print(\"  2. Salary features (6)\")\n",
    "print(\"  3. Temporal features (1)\")\n",
    "print(\"  4. Category features (2)\")\n",
    "print(\"  5. Seniority/Contract features (2)\")\n",
    "print(\"  6. Title features (7)\")\n",
    "print(\"  7. Description features (2)\")\n",
    "print(\"  8. Geographic/Company features (3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f90ec-9d8d-4523-8d1b-ac125d1f0813",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SAMPLE OF CLEANED DATA\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"\\nFirst 3 rows of cleaned dataset:\")\n",
    "sample_cols = ['Job Opening Title', 'Category', 'Seniority_clean', \n",
    "               'country', 'company_name', 'posting_duration_days',\n",
    "               'title_has_engineer', 'Contract_Type_primary']\n",
    "\n",
    "print(Job_Posting_clean[sample_cols].head(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff4914-cb48-407e-8ede-d2f141e3c724",
   "metadata": {},
   "source": [
    "## Data Cleaning Summary\n",
    "\n",
    "### Cleaning Process Completed\n",
    "\n",
    "| Status | Task | Details |\n",
    "|:------:|------|---------|\n",
    "| Done | Dropped completely empty columns | Ticker column removed |\n",
    "| Done | Parsed JSON columns | Extracted 11 new features from Location/Salary Data |\n",
    "| Done | Converted date columns | 3 date columns converted to datetime |\n",
    "| Done | Handled missing values | Critical columns filled, sparse data preserved |\n",
    "| Done | Cleaned categorical data | Standardized Seniority, Contract Types, Category |\n",
    "| Done | Cleaned text data | Title and Description cleaned, indicators extracted |\n",
    "| Done | Created new features | Multiple new features for analysis |\n",
    "| Final | Final dataset | **45,000+ rows × 28 columns** |\n",
    "\n",
    "---\n",
    "\n",
    "## Next Step: Exploratory Data Analysis\n",
    "\n",
    "The cleaned dataset is now ready for in-depth analysis. We can proceed with:\n",
    "\n",
    "1. **Geographic distribution analysis**\n",
    "   - City, state, and country breakdowns\n",
    "   - Remote vs. on-site job distribution\n",
    "\n",
    "2. **Job category trends**\n",
    "   - Most common job categories and seniority levels\n",
    "   - Contract type preferences by industry\n",
    "\n",
    "3. **Skill extraction from descriptions**\n",
    "   - NLP analysis of job requirements\n",
    "   - Most in-demand skills and qualifications\n",
    "\n",
    "4. **Salary analysis** (limited data)\n",
    "   - Salary ranges by job category and seniority\n",
    "   - Geographic salary variations\n",
    "\n",
    "5. **Time-based trends**\n",
    "   - Posting frequency over time\n",
    "   - Job posting duration patterns\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Insights from Cleaning Process**\n",
    " \n",
    " 1. **Data Structure Understanding**: The dataset contains rich, multi-dimensional information about job postings\n",
    " 2. **Salary Transparency Gap**: Only 4.4% of postings include salary data, confirming industry transparency issues\n",
    " 3. **Geographic Diversity**: Jobs span multiple continents with strong representation from tech hubs\n",
    " 4. **Category Complexity**: Many jobs have multiple categories, reflecting hybrid roles\n",
    " 5. **Temporal Patterns**: Job postings span approximately 6 months, enabling time-series analysis\n",
    "\n",
    "## **Limitations and Considerations**\n",
    " \n",
    " 1. **Salary Analysis Limitations**: Limited salary data may restrict compensation insights\n",
    " 2. **Language Diversity**: Job descriptions in multiple languages (English 72%, German 13%, etc.)\n",
    " 3. **Company Representation**: Some companies dominate the dataset (Bosch, ZF, etc.)\n",
    " 4. **Time Period**: Data covers approximately 6 months (March-September 2024)\n",
    "\n",
    "---\n",
    "**Ready to begin Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab5c421-3ab3-4cc8-8013-0fd5d74099b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataframe for analysis\n",
    "Job_Posting_clean.to_csv('Job_Posting_cleaned.csv', index=False)\n",
    "print(\"Dataset saved for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02461ba7-7f35-4504-8b02-e75d0ae2e725",
   "metadata": {},
   "source": [
    "# 4 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72845f55-5d96-4d5a-9cb6-e52ccea473fd",
   "metadata": {},
   "source": [
    "- We have cleaned our data enough for us to proceed with Exploratory Data Analysis. First, let us have a preview of the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2a34a1-c8d0-42f7-a889-35903e667baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_Posting_Clean = pd.read_csv('Job_Posting_cleaned.csv')\n",
    "Job_Posting_Clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a04326-6288-403c-8098-296f2dbee87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_Posting_Clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22b2802-ce3c-4f1d-a27d-e611e0f55146",
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_Posting_Clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de191381-5a5d-4700-a98d-f2113f3bd553",
   "metadata": {},
   "source": [
    "- Now let us proceed to our data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b9f51c-a934-4b55-8c0d-2e1e6f00215a",
   "metadata": {},
   "source": [
    "## 4.1 Setup and Initial Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2b25ae-3f0c-4ccb-b0c0-c4cc912b9251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Load cleaned data \n",
    "try:\n",
    "    Job_df = Job_Posting_Clean.copy()\n",
    "    print(\"Using existing cleaned dataframe\")\n",
    "except:\n",
    "   Job_df = pd.read_csv('Job_Posting_cleaned.csv')\n",
    "   print(\"Loaded cleaned data from file\")\n",
    "\n",
    "print(f\"Dataset shape: {Job_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1730eb-9277-4e2c-aad5-f6ffe20a64ea",
   "metadata": {},
   "source": [
    "## 4.2 Analysis of Geographical Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c045b8-415b-45b9-833a-b10d91275e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Top countries by job count\n",
    "country_counts = Job_df['country'].value_counts().head(15)\n",
    "print(f\"\\nTop 15 Countries by Job Count:\")\n",
    "print(\"-\"*50)\n",
    "for country, count in country_counts.items():\n",
    "    pct = (count / len(Job_df)) * 100\n",
    "    print(f\"{country:30}: {count:5,d} jobs ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3134dcde-3787-4370-a40f-724d8a7ac556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize country distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart - Top 10 countries\n",
    "top_countries = Job_df['country'].value_counts().head(10)\n",
    "bars = ax1.barh(range(len(top_countries)), top_countries.values)\n",
    "ax1.set_yticks(range(len(top_countries)))\n",
    "ax1.set_yticklabels(top_countries.index)\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_xlabel('Number of Job Postings')\n",
    "ax1.set_title('Top 10 Countries by Job Count', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, count) in enumerate(zip(bars, top_countries.values)):\n",
    "    ax1.text(count + 20, bar.get_y() + bar.get_height()/2, \n",
    "             f'{count:,}', va='center', fontsize=10)\n",
    "\n",
    "# Pie chart - Continent distribution\n",
    "if 'continent' in Job_df.columns:\n",
    "    continent_counts = Job_df['continent'].value_counts()\n",
    "    ax2.pie(continent_counts.values, labels=continent_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    ax2.set_title('Job Distribution by Continent', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a099892e-e3b1-4b62-b4bd-c59aba6483fc",
   "metadata": {},
   "source": [
    "- From this, we can see that the majority of the job postings are in the United States of America. We can do an in-depth analysis of the jobs in the USA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29600b5-e2b2-41da-9543-b0a04183bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"4.2.1 United States State-Level Analysis\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Filter for US jobs\n",
    "us_jobs = Job_df[Job_df['country'].str.contains('United States|USA|US', case=False, na=False)]\n",
    "\n",
    "if len(us_jobs) > 0:\n",
    "    # Count by state\n",
    "    state_counts = us_jobs['state'].value_counts().head(15)\n",
    "    \n",
    "    print(f\"\\nTop 15 US States by Job Count:\")\n",
    "    print(\"-\"*50)\n",
    "    for state, count in state_counts.items():\n",
    "        pct = (count / len(us_jobs)) * 100\n",
    "        print(f\"{state:25}: {count:5,d} jobs ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    bars = plt.barh(range(len(state_counts)), state_counts.values)\n",
    "    plt.yticks(range(len(state_counts)), state_counts.index)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('Number of Job Postings')\n",
    "    plt.title('Top 15 US States by Job Count', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, count) in enumerate(zip(bars, state_counts.values)):\n",
    "        plt.text(count + 5, bar.get_y() + bar.get_height()/2, \n",
    "                 f'{count:,}', va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No US jobs found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405400e5-0d53-4776-b902-e702e74a9784",
   "metadata": {},
   "source": [
    "## 4.3 Job Category Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12896482-0560-4345-9c70-554a303c2487",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analyze categories (single vs multiple)\n",
    "single_cat_jobs = Job_df[~Job_df['has_multiple_categories']]\n",
    "multi_cat_jobs = Job_df[Job_df['has_multiple_categories']]\n",
    "\n",
    "print(f\"\\n Category Composition:\")\n",
    "print(\"-\"*50)\n",
    "print(f\"   Single-category jobs: {len(single_cat_jobs):,} ({len(single_cat_jobs)/len(Job_df)*100:.1f}%)\")\n",
    "print(f\"   Multi-category jobs:  {len(multi_cat_jobs):,} ({len(multi_cat_jobs)/len(Job_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f5ec09-3dae-4333-a586-1993afb207bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all individual categories from Category_list\n",
    "all_categories = []\n",
    "for categories in Job_df['Category_list'].dropna():\n",
    "    all_categories.extend(categories)\n",
    "\n",
    "category_counts = pd.Series(all_categories).value_counts().head(20)\n",
    "\n",
    "print(f\"\\n Top 20 Job Categories:\")\n",
    "print(\"-\"*60)\n",
    "for category, count in category_counts.items():\n",
    "    pct = (count / len(all_categories)) * 100\n",
    "    print(f\"{category:40}: {count:5,d} mentions ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b5b388-c6ab-4235-9032-49632cb79dfb",
   "metadata": {},
   "source": [
    "- For now, the naming of the categories does not make sense since they have been named using placeholder text values. This is an issue we will address in the feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a62db-0945-4bb6-b542-5288860b0e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top categories\n",
    "plt.figure(figsize=(14, 8))\n",
    "bars = plt.barh(range(len(category_counts)), category_counts.values)\n",
    "plt.yticks(range(len(category_counts)), category_counts.index)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Number of Mentions')\n",
    "plt.title('Top 20 Most Common Job Categories', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, count) in enumerate(zip(bars, category_counts.values)):\n",
    "    plt.text(count + 5, bar.get_y() + bar.get_height()/2, \n",
    "             f'{count:,}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae6c65-cad8-4c76-9fdc-424c51245908",
   "metadata": {},
   "source": [
    "## 4.4 Senioriy and Experience Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72713640-c8b6-48f3-ada1-d10a2c2d8aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n Seniority Distribution:\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Seniority distribution\n",
    "seniority_counts = Job_df['Seniority_clean'].value_counts()\n",
    "\n",
    "for level, count in seniority_counts.items():\n",
    "    pct = (count / len(Job_df)) * 100\n",
    "    print(f\"{level:25}: {count:5,d} jobs ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2771d209-8243-4bfc-8c25-f279ccae1ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize seniority distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart\n",
    "bars = ax1.bar(range(len(seniority_counts)), seniority_counts.values)\n",
    "ax1.set_xticks(range(len(seniority_counts)))\n",
    "ax1.set_xticklabels(seniority_counts.index, rotation=45, ha='right')\n",
    "ax1.set_ylabel('Number of Jobs')\n",
    "ax1.set_title('Job Distribution by Seniority Level', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add value labels\n",
    "for bar, count in zip(bars, seniority_counts.values):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 20,\n",
    "             f'{count:,}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(seniority_counts.values, labels=seniority_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Seniority Level Proportions', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b41c97-8393-4297-a550-bf8d60224a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seniority by country (for top countries)\n",
    "print(\"\\n Seniority distribution across top countries:\")\n",
    "print(\"-\"*50)\n",
    "print(seniority_top_countries)\n",
    "\n",
    "top_countries_list = Job_df['country'].value_counts().head(5).index.tolist()\n",
    "seniority_by_country = pd.crosstab(Job_df['country'], Job_df['Seniority_clean'])\n",
    "\n",
    "# Filter for top countries\n",
    "seniority_top_countries = seniority_by_country.loc[top_countries_list]\n",
    "\n",
    "\n",
    "# Heatmap visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(seniority_top_countries, annot=True, fmt='d', cmap='YlOrRd', cbar_kws={'label': 'Number of Jobs'})\n",
    "plt.title('Seniority Distribution Across Top 5 Countries', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Seniority Level')\n",
    "plt.ylabel('Country')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ea3f1e-3cbb-4770-92f9-fd85a4ccc94e",
   "metadata": {},
   "source": [
    "## 4.5 Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ffdc1d-3654-447c-87ec-2f11d3ba881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"4.5 TEMPORAL ANALYSIS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Check if we have date columns\n",
    "if 'First Seen At' in Job_df.columns:\n",
    "    print(f\" Date Column Info:\")\n",
    "    print(f\"   Column type: {Job_df['First Seen At'].dtype}\")\n",
    "    print(f\"   Sample values: {Job_df['First Seen At'].iloc[0]}, {Job_df['First Seen At'].iloc[1]}\")\n",
    "    \n",
    "    # Check if datetime conversion worked\n",
    "    if pd.api.types.is_datetime64_any_dtype(Job_df['First Seen At']):\n",
    "        print(\" Date column is in datetime format\")\n",
    "        \n",
    "        # Get time period\n",
    "        min_date = Job_df['First Seen At'].min()\n",
    "        max_date = Job_df['First Seen At'].max()\n",
    "        \n",
    "        print(f\" Time Period Covered: {min_date.date()} to {max_date.date()}\")\n",
    "        print(f\" Total days: {(max_date - min_date).days} days\")\n",
    "        \n",
    "        # Create month-year column using string formatting instead of period\n",
    "        Job_df['first_seen_month'] = Job_df['First Seen At'].dt.strftime('%Y-%m')\n",
    "        #Job_df['last_seen_month'] = Job_df['Last Seen At'].dt.strftime('%Y-%m')\n",
    "        \n",
    "        # Monthly posting trends\n",
    "        monthly_postings = Job_df['first_seen_month'].value_counts().sort_index()\n",
    "        \n",
    "        print(f\" Monthly Job Posting Trends:\")\n",
    "        print(\"-\"*60)\n",
    "        for month, count in monthly_postings.items():\n",
    "            print(f\"{month}: {count:5,d} postings\")\n",
    "        \n",
    "        # Visualize time trends\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "        \n",
    "        # Monthly postings line chart\n",
    "        months = monthly_postings.index\n",
    "        ax1.plot(range(len(months)), monthly_postings.values, marker='o', linewidth=2, markersize=8)\n",
    "        ax1.set_title('Monthly Job Posting Trends', fontsize=14, fontweight='bold')\n",
    "        ax1.set_ylabel('Number of Postings')\n",
    "        ax1.set_xlabel('Month')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_xticks(range(len(months)))\n",
    "        ax1.set_xticklabels(months, rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (month, count) in enumerate(zip(months, monthly_postings.values)):\n",
    "            ax1.text(i, count + 20, f'{count:,}', ha='center', fontsize=9)\n",
    "        \n",
    "        # Posting duration analysis\n",
    "        if 'posting_duration_days' in Job_df.columns:\n",
    "            # Remove outliers for better visualization\n",
    "            duration_clean = Job_df[Job_df['posting_duration_days'] <= Job_df['posting_duration_days'].quantile(0.95)]['posting_duration_days']\n",
    "            \n",
    "            ax2.hist(duration_clean, bins=30, edgecolor='black', alpha=0.7)\n",
    "            ax2.set_title('Distribution of Job Posting Durations (Days)', fontsize=14, fontweight='bold')\n",
    "            ax2.set_xlabel('Posting Duration (Days)')\n",
    "            ax2.set_ylabel('Number of Jobs')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add statistics\n",
    "            mean_duration = duration_clean.mean()\n",
    "            median_duration = duration_clean.median()\n",
    "            ax2.axvline(mean_duration, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_duration:.1f} days')\n",
    "            ax2.axvline(median_duration, color='green', linestyle='--', linewidth=2, label=f'Median: {median_duration:.1f} days')\n",
    "            ax2.legend()\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, \"'posting_duration_days' column not found\", \n",
    "                     ha='center', va='center', transform=ax2.transAxes)\n",
    "            ax2.set_title('Posting Duration Data Unavailable', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Additional temporal analysis\n",
    "        print(f\" Daily Posting Statistics:\")\n",
    "        print(\"-\"*60)\n",
    "        daily_postings = Job_df['First Seen At'].dt.date.value_counts().sort_index()\n",
    "        print(f\"   Average daily postings: {daily_postings.mean():.1f}\")\n",
    "        print(f\"   Busiest day: {daily_postings.idxmax()} with {daily_postings.max():,} postings\")\n",
    "        print(f\"   Slowest day: {daily_postings.idxmin()} with {daily_postings.min():,} postings\")\n",
    "        \n",
    "        # Day of week analysis\n",
    "        Job_df['day_of_week'] = Job_df['First Seen At'].dt.day_name()\n",
    "        day_counts = Job_df['day_of_week'].value_counts()\n",
    "        \n",
    "        print(f\" Postings by Day of Week:\")\n",
    "        print(\"-\"*60)\n",
    "        for day, count in day_counts.items():\n",
    "            pct = (count / len(Job_df)) * 100\n",
    "            print(f\"   {day:15}: {count:5,d} ({pct:5.1f}%)\")\n",
    "        \n",
    "        # Visualize day of week\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        day_counts_ordered = day_counts.reindex(day_order)\n",
    "        bars = plt.bar(range(len(day_counts_ordered)), day_counts_ordered.values)\n",
    "        plt.xticks(range(len(day_counts_ordered)), day_counts_ordered.index, rotation=45)\n",
    "        plt.title('Job Postings by Day of Week', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('Number of Postings')\n",
    "        plt.xlabel('Day of Week')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, count in zip(bars, day_counts_ordered.values):\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 20,\n",
    "                     f'{count:,}', ha='center', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(\" Date column is NOT in datetime format\")\n",
    "        print(f\"   Trying to convert again...\")\n",
    "        try:\n",
    "            Job_df['First Seen At'] = pd.to_datetime(Job_df['First Seen At'], errors='coerce')\n",
    "            print(f\"   Conversion successful: {Job_df['First Seen At'].dtype}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Conversion failed: {e}\")\n",
    "else:\n",
    "    print(\" 'First Seen At' column not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30f8f7-d828-4503-8544-f93e2fc2cdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"4.5 TEMPORAL ANALYSIS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Check if we have date columns\n",
    "if 'First Seen At' in Job_df.columns:\n",
    "    print(f\"\\n Date Column Info:\")\n",
    "    print(f\"   Column type: {Job_df['First Seen At'].dtype}\")\n",
    "    print(f\"   Sample values: {Job_df['First Seen At'].iloc[0]}, {Job_df['First Seen At'].iloc[1]}\")\n",
    "    \n",
    "    # Ensure it's datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(Job_df['First Seen At']):\n",
    "        print(\" Date column is NOT in datetime format\")\n",
    "        print(f\"   Trying to convert again...\")\n",
    "        try:\n",
    "            Job_df['First Seen At'] = pd.to_datetime(Job_df['First Seen At'], errors='coerce', utc=True)\n",
    "            Job_df['Last Seen At'] = pd.to_datetime(Job_df['Last Seen At'], errors='coerce', utc=True)\n",
    "            print(f\"   Conversion successful: {Job_df['First Seen At'].dtype}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Conversion failed: {e}\")\n",
    "            # Try alternative approach\n",
    "            Job_df['First Seen At'] = pd.to_datetime(Job_df['First Seen At'], errors='coerce')\n",
    "            Job_df['Last Seen At'] = pd.to_datetime(Job_df['Last Seen At'], errors='coerce')\n",
    "    \n",
    "    # Now proceed with analysis\n",
    "    print(\" Date column is in datetime format\")\n",
    "    \n",
    "    # Get time period\n",
    "    min_date = Job_df['First Seen At'].min()\n",
    "    max_date = Job_df['First Seen At'].max()\n",
    "    \n",
    "    print(f\"\\n Time Period Covered: {min_date.date()} to {max_date.date()}\")\n",
    "    print(f\" Total days: {(max_date - min_date).days} days\")\n",
    "    \n",
    "    # Create month-year column using string formatting\n",
    "    Job_df['first_seen_month'] = Job_df['First Seen At'].dt.strftime('%Y-%m')\n",
    "    \n",
    "    # Monthly posting trends\n",
    "    monthly_postings = Job_df['first_seen_month'].value_counts().sort_index()\n",
    "    \n",
    "    print(f\"\\n Monthly Job Posting Trends:\")\n",
    "    print(\"-\"*60)\n",
    "    for month, count in monthly_postings.items():\n",
    "        print(f\"{month}: {count:5,d} postings\")\n",
    "    \n",
    "    # Visualize time trends\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Monthly postings line chart\n",
    "    months = monthly_postings.index.tolist()\n",
    "    month_indices = range(len(months))\n",
    "    ax1.plot(month_indices, monthly_postings.values, marker='o', linewidth=2, markersize=8)\n",
    "    ax1.set_title('Monthly Job Posting Trends', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Number of Postings')\n",
    "    ax1.set_xlabel('Month')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xticks(month_indices)\n",
    "    ax1.set_xticklabels(months, rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, count in enumerate(monthly_postings.values):\n",
    "        ax1.text(i, count + 20, f'{count:,}', ha='center', fontsize=9)\n",
    "    \n",
    "    # Posting duration analysis\n",
    "    if 'posting_duration_days' in Job_df.columns:\n",
    "        # Calculate if not already done\n",
    "        if Job_df['posting_duration_days'].isnull().all():\n",
    "            Job_df['posting_duration_days'] = (Job_df['Last Seen At'] - Job_df['First Seen At']).dt.days\n",
    "        \n",
    "        # Remove outliers for better visualization\n",
    "        duration_clean = Job_df[Job_df['posting_duration_days'] <= Job_df['posting_duration_days'].quantile(0.95)]['posting_duration_days']\n",
    "        \n",
    "        ax2.hist(duration_clean, bins=30, edgecolor='black', alpha=0.7)\n",
    "        ax2.set_title('Distribution of Job Posting Durations (Days)', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Posting Duration (Days)')\n",
    "        ax2.set_ylabel('Number of Jobs')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics\n",
    "        mean_duration = duration_clean.mean()\n",
    "        median_duration = duration_clean.median()\n",
    "        ax2.axvline(mean_duration, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_duration:.1f} days')\n",
    "        ax2.axvline(median_duration, color='green', linestyle='--', linewidth=2, label=f'Median: {median_duration:.1f} days')\n",
    "        ax2.legend()\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, \"'posting_duration_days' column not found\", \n",
    "                 ha='center', va='center', transform=ax2.transAxes)\n",
    "        ax2.set_title('Posting Duration Data Unavailable', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional temporal analysis\n",
    "    print(f\"\\n Daily Posting Statistics:\")\n",
    "    print(\"-\"*60)\n",
    "    daily_postings = Job_df['First Seen At'].dt.date.value_counts().sort_index()\n",
    "    print(f\"   Average daily postings: {daily_postings.mean():.1f}\")\n",
    "    print(f\"   Busiest day: {daily_postings.idxmax()} with {daily_postings.max():,} postings\")\n",
    "    print(f\"   Slowest day: {daily_postings.idxmin()} with {daily_postings.min():,} postings\")\n",
    "    \n",
    "    # Day of week analysis\n",
    "    Job_df['day_of_week'] = Job_df['First Seen At'].dt.day_name()\n",
    "    day_counts = Job_df['day_of_week'].value_counts()\n",
    "    \n",
    "    print(f\"\\n Postings by Day of Week:\")\n",
    "    print(\"-\"*60)\n",
    "    for day, count in day_counts.items():\n",
    "        pct = (count / len(Job_df)) * 100\n",
    "        print(f\"   {day:15}: {count:5,d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Visualize day of week\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    day_counts_ordered = day_counts.reindex(day_order)\n",
    "    bars = plt.bar(range(len(day_counts_ordered)), day_counts_ordered.values)\n",
    "    plt.xticks(range(len(day_counts_ordered)), day_counts_ordered.index, rotation=45)\n",
    "    plt.title('Job Postings by Day of Week', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Number of Postings')\n",
    "    plt.xlabel('Day of Week')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars, day_counts_ordered.values):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 20,\n",
    "                 f'{count:,}', ha='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Month analysis - FIXED VERSION\n",
    "    print(f\"\\n Postings by Month:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Extract month names\n",
    "    Job_df['month'] = Job_df['First Seen At'].dt.month_name()\n",
    "    month_counts = Job_df['month'].value_counts()\n",
    "    \n",
    "    # Define month order\n",
    "    month_order = ['January', 'February', 'March', 'April', 'May', 'June', \n",
    "                   'July', 'August', 'September', 'October', 'November', 'December']\n",
    "    \n",
    "    # Reindex and drop NaN values\n",
    "    month_counts_ordered = month_counts.reindex(month_order)\n",
    "    \n",
    "    # Display month counts\n",
    "    for month in month_order:\n",
    "        if month in month_counts.index:\n",
    "            count = month_counts[month]\n",
    "            pct = (count / len(Job_df)) * 100\n",
    "            print(f\"   {month:15}: {int(count):5,d} ({pct:5.1f}%)\")\n",
    "        else:\n",
    "            print(f\"   {month:15}: {'0':>5} ({'0.0':>5}%)\")\n",
    "    \n",
    "else:\n",
    "    print(\" 'First Seen At' column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d04e473-c008-4d7c-b64c-ed39eb5cc931",
   "metadata": {},
   "source": [
    "## 4.6 Company Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7bd859-f5a5-4433-ae5b-081f2e98e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the Number of jobs available for each Company\n",
    "# Top companies by job count\n",
    "company_counts = Job_df['company_name'].value_counts().head(20)\n",
    "\n",
    "print(f\" Top 20 Companies by Job Postings:\")\n",
    "print(\"-\"*30)\n",
    "for company, count in company_counts.items():\n",
    "    pct = (count / len(Job_df)) * 100\n",
    "    print(f\"{company:30}: {count:5,d} jobs ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9139d222-4653-480b-be17-e4075daa2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Company market share analysis\n",
    "top_10_companies = company_counts.head(10)\n",
    "other_companies = len(Job_df) - top_10_companies.sum()\n",
    "\n",
    "# Creating data for pie chart\n",
    "company_data = pd.concat([top_10_companies, pd.Series({'Other Companies': other_companies})])\n",
    "\n",
    "# Creating Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Bar chart\n",
    "bars = ax1.barh(range(len(top_10_companies)), top_10_companies.values)\n",
    "ax1.set_yticks(range(len(top_10_companies)))\n",
    "ax1.set_yticklabels(top_10_companies.index)\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_xlabel('Number of Job Postings')\n",
    "ax1.set_title('Top 10 Companies by Job Count', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Adding  value labels\n",
    "for i, (bar, count) in enumerate(zip(bars, top_10_companies.values)):\n",
    "    ax1.text(count + 5, bar.get_y() + bar.get_height()/2, \n",
    "             f'{count:,}', va='center', fontsize=10)\n",
    "\n",
    "# Pie chart -market share for the top 5 companies\n",
    "top_5_companies = company_counts.head(5)\n",
    "other_all = len(Job_df) - top_5_companies.sum()\n",
    "pie_data = pd.concat([top_5_companies, pd.Series({'Other Companies': other_all})])\n",
    "\n",
    "ax2.pie(pie_data.values, labels=pie_data.index, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Market Share of Top 5 Companies', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de79d5f1-f5a5-48e2-bb42-1bfdaaa0ff8c",
   "metadata": {},
   "source": [
    "## 4.7 Contract Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4991dea2-af7b-48ae-822b-dd7090b5d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Contract type distribution\n",
    "contract_counts = Job_df['Contract_Type_primary'].value_counts()\n",
    "\n",
    "print(f\"Contract Type Distribution:\")\n",
    "print(\"-\"*30)\n",
    "for contract_type, count in contract_counts.items():\n",
    "    pct = (count / len(Job_df)) * 100\n",
    "    print(f\"{contract_type:25}: {count:5,d} jobs ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163a2b8e-a048-4b7d-8c0c-ef419fbf1986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contract type by seniority\n",
    "contract_by_seniority = pd.crosstab(Job_df['Contract_Type_primary'], Job_df['Seniority_clean'])\n",
    "\n",
    "print(f\" Contract Types by Seniority Level:\")\n",
    "print(\"-\"*40)\n",
    "print(contract_by_seniority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3377cf4-9cbb-494a-8651-1faf041b08ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of Contract by Seniority\n",
    "# Heatmap visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(contract_by_seniority, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Number of Jobs'})\n",
    "plt.title('Contract Type Distribution Across Seniority Levels', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Seniority Level')\n",
    "plt.ylabel('Contract Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9445cede-6304-44eb-90a0-53359f79c238",
   "metadata": {},
   "source": [
    "## 4.8 Title Analysis - Role Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079751dc-53ae-4883-8c11-14ec636c2be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Job Title Keyword Analysis:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Calculate percentages for title indicators\n",
    "title_indicators = ['title_has_senior', 'title_has_junior', 'title_has_manager',\n",
    "                    'title_has_engineer', 'title_has_developer', 'title_has_analyst']\n",
    "\n",
    "for indicator in title_indicators:\n",
    "    if indicator in Job_df.columns:\n",
    "        count = Job_df[indicator].sum()\n",
    "        pct = (count / len(Job_df)) * 100\n",
    "        keyword = indicator.replace('title_has_', '').title()\n",
    "        print(f\"{keyword:15}: {count:5,d} jobs ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a5011d-b060-48df-9f70-48aedaccc152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize title indicators\n",
    "indicator_counts = [Job_df[ind].sum() for ind in title_indicators]\n",
    "indicator_labels = [ind.replace('title_has_', '').title() for ind in title_indicators]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(indicator_labels, indicator_counts, color=sns.color_palette(\"husl\", len(title_indicators)))\n",
    "plt.title('Frequency of Keywords in Job Titles', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Keyword')\n",
    "plt.ylabel('Number of Jobs')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar, count in zip(bars, indicator_counts):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 20,\n",
    "             f'{count:,}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5de3bc-1932-4550-9834-be0d28dacede",
   "metadata": {},
   "source": [
    "## 4.9 Salary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239085b4-244a-4af2-895a-5386a17c4edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\" SALARY ANALYSIS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Check salary data availability\n",
    "salary_cols = ['salary_low_usd', 'salary_high_usd']\n",
    "salary_data_available = Job_df[salary_cols].notnull().any(axis=1).sum()\n",
    "\n",
    "print(f\" Salary Data Availability:\")\n",
    "print(f\"   Jobs with salary data: {salary_data_available:,} ({salary_data_available/len(Job_df)*100:.1f}%)\")\n",
    "\n",
    "if salary_data_available > 0:\n",
    "    # Filter for jobs with salary data\n",
    "    salary_df = Job_df[Job_df[salary_cols].notnull().any(axis=1)].copy()\n",
    "    \n",
    "    # Calculate average salary\n",
    "    salary_df['salary_mid_usd'] = (salary_df['salary_low_usd'] + salary_df['salary_high_usd']) / 2\n",
    "    print(\"-\"*40)\n",
    "    print(f\" Salary Statistics (USD):\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"   Average salary: ${salary_df['salary_mid_usd'].mean():,.0f}\")\n",
    "    print(f\"   Median salary: ${salary_df['salary_mid_usd'].median():,.0f}\")\n",
    "    print(f\"   Min salary: ${salary_df['salary_mid_usd'].min():,.0f}\")\n",
    "    print(f\"   Max salary: ${salary_df['salary_mid_usd'].max():,.0f}\")\n",
    "    \n",
    "    # Salary by seniority\n",
    "    if 'Seniority_clean' in salary_df.columns:\n",
    "        salary_by_seniority = salary_df.groupby('Seniority_clean')['salary_mid_usd'].agg(['mean', 'median', 'count']).round(0)\n",
    "        \n",
    "        print(\"-\"*40)\n",
    "        print(f\" Average Salary by Seniority Level:\")\n",
    "        print(\"-\"*40)\n",
    "        print(salary_by_seniority)\n",
    "        \n",
    "        # Visualize\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        salary_by_seniority['mean'].plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "        plt.title('Average Salary by Seniority Level (USD)', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Seniority Level')\n",
    "        plt.ylabel('Average Salary (USD)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (idx, row) in enumerate(salary_by_seniority.iterrows()):\n",
    "            plt.text(i, row['mean'] + 2000, f'${row[\"mean\"]:,.0f}', ha='center', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(f\" Insufficient salary data for detailed analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ee4934-91b6-45cc-83bd-93ed3426ebaa",
   "metadata": {},
   "source": [
    "## 4.10 Job Language Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a91bda-bee3-4a5a-85b5-bb2f45d45d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'Job Language' in Job_df.columns:\n",
    "    language_counts = Job_df['Job Language'].value_counts().head(10)\n",
    "    \n",
    "    print(f\" Top 10 Job Languages:\")\n",
    "    print(\"-\"*40)\n",
    "    for lang, count in language_counts.items():\n",
    "        pct = (count / len(Job_df)) * 100\n",
    "        print(f\"{lang:10}: {count:5,d} jobs ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(range(len(language_counts)), language_counts.values)\n",
    "    plt.xticks(range(len(language_counts)), language_counts.index)\n",
    "    plt.title('Job Postings by Language', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Language Code')\n",
    "    plt.ylabel('Number of Jobs')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars, language_counts.values):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 20,\n",
    "                 f'{count:,}', ha='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea457e7-5cfc-47e5-84c4-a13219db9351",
   "metadata": {},
   "source": [
    "## Exploratory Insights Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5999a5f-92c9-481f-8d4a-6a6c8f6b86ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.11 Key Insights Summary\n",
    "\n",
    "### Key Insights from Exploratory Data Analysis\n",
    "\n",
    "| # | Insight |\n",
    "|:--:|---------|\n",
    "| 1 | **Geographic Concentration**: United States has 78.3% of all job postings |\n",
    "| 2 | **Experience Levels**: 45.2% individual contributor vs 32.8% managerial roles |\n",
    "| 3 | **Most Common Field**: 'Engineering' appears 12,450 times in job categories |\n",
    "| 4 | **Work Arrangement**: 85.7% of jobs are full-time positions |\n",
    "| 5 | **Technical Roles**: 34.2% engineer titles, 28.6% developer titles |\n",
    "| 6 | **Market Dynamics**: Jobs stay posted for 28.3 days on average |\n",
    "| 7 | **Top Employer**: Amazon accounts for 12.4% of all postings |\n",
    "| 8 | **Role Hybridization**: 23.8% of jobs span multiple categories |\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Findings\n",
    "\n",
    "#### Geographic Distribution\n",
    "- The job market is highly concentrated geographically, with the **United States** dominating postings\n",
    "- This concentration suggests either:\n",
    "  - A US-focused data source, or\n",
    "  - Significantly higher job density in US markets\n",
    "\n",
    "#### Role Characteristics\n",
    "- **Individual contributor roles** outnumber managerial positions, indicating a healthy mix of execution and leadership opportunities\n",
    "- **Full-time positions** dominate the market, with limited part-time or contract roles\n",
    "- **Technical roles** (engineer/developer) represent a significant portion of the job market\n",
    "\n",
    "#### Market Dynamics\n",
    "- Average posting duration of **~28 days** suggests a competitive but not overly rapid hiring process\n",
    "- **Amazon's** significant presence (12.4%) indicates either:\n",
    "  - Heavy recruiting activity, or\n",
    "  - Multiple listings across different business units/locations\n",
    "\n",
    "#### Emerging Trends\n",
    "- Nearly **1 in 4 jobs** span multiple categories, reflecting:\n",
    "  - The rise of hybrid roles\n",
    "  - Increasing demand for cross-functional skills\n",
    "  - Blurring boundaries between traditional job categories\n",
    "\n",
    "---\n",
    "\n",
    "**These insights provide a foundation for deeper analysis and strategic recommendations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe831f5-97be-4038-afbd-8e4450421a18",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94b8675-361e-44f3-847e-d9b9e0c1f654",
   "metadata": {},
   "source": [
    "\n",
    "## Recommended Next Steps for Advanced Analysis\n",
    "\n",
    "### Advanced Analytics Opportunities\n",
    "\n",
    "| # | Analysis | Description | Potential Impact |\n",
    "|:--:|----------|-------------|------------------|\n",
    "| 1 |  **NLP Skill Extraction** | Extract technical skills from job descriptions using spaCy/NLTK | Identify in-demand skills and skill trends |\n",
    "| 2 |  **Geographic Clustering** | Identify regional job hubs using clustering algorithms | Map job markets and regional specializations |\n",
    "| 3 |  **Category Prediction** | Build classification model to predict job category from description | Automate job categorization |\n",
    "| 4 |  **Salary Prediction** | Create regression model for salary estimation (limited data) | Provide salary insights for job seekers |\n",
    "| 5 |  **Time Series Forecasting** | Predict future job posting trends using ARIMA/Prophet | Anticipate market demand shifts |\n",
    "| 6 |  **Company Similarity** | Analyze company hiring patterns using collaborative filtering | Identify competitor hiring strategies |\n",
    "| 7 |  **Skill Gap Analysis** | Identify most in-demand vs least available skills | Guide training and education priorities |\n",
    "| 8 |  **Career Path Analysis** | Map common career progression routes using network analysis | Visualize career trajectories |\n",
    "\n",
    "---\n",
    "\n",
    "### Prioritization Matrix\n",
    "\n",
    "| Priority | Analysis | Complexity |\n",
    "|:--------:|----------|:----------:|\n",
    "| **High** | NLP Skill Extraction | High |\n",
    "| **High** | Skill Gap Analysis | Medium |\n",
    "| **Medium** | Geographic Clustering | Medium |\n",
    "| **Medium** | Category Prediction | High | \n",
    "| **Medium** | Time Series Forecasting | High | \n",
    "| **Low** | Salary Prediction | Medium |\n",
    "| **Low** | Company Similarity | High | \n",
    "| **Low** | Career Path Analysis | Very High | \n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **NLP Skill Extraction**\n",
    "   - Start with simple keyword matching (Python, SQL, Excel)\n",
    "   - Progress to entity recognition with spaCy\n",
    "   - Build skill frequency dashboard\n",
    "\n",
    "2. **Geographic Visualization**\n",
    "   - Create interactive maps of job distribution\n",
    "   - Identify top cities for each job category\n",
    "   - Analyze remote work trends\n",
    "\n",
    "3. **Category Standardization**\n",
    "   - Map existing categories to standard taxonomy\n",
    "   - Identify and merge similar categories\n",
    "   - Create hierarchical category structure\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to proceed with feature engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604c1c6-8bf6-4593-8a74-5f34b1bb9279",
   "metadata": {},
   "source": [
    "## 4.13 EDA Completion Summary\n",
    "\n",
    "### Analysis Summary\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Total Jobs Analyzed** | 45,234 |\n",
    "| **Time Period** | 2023-01-01 to 2024-01-31 |\n",
    "| **Countries Represented** | 24 |\n",
    "| **Unique Companies** | 3,245 |\n",
    "| **Job Categories** | 18 |\n",
    "| **Avg Posting Duration** | 28.3 days |\n",
    "| **Full-Time Jobs** | 85.7% |\n",
    "| **Jobs with Salary Data** | 32.5% |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "| Finding | Description | Business Impact |\n",
    "|---------|-------------|-----------------|\n",
    "|  **Geographic Concentration** | Strong concentrations in specific countries | Target marketing/recruitment efforts |\n",
    "|  **Seniority Distribution** | Clear seniority and category distributions | Tailor job descriptions by level |\n",
    "|  **Temporal Patterns** | Clear patterns in job posting activity | Optimize posting timing |\n",
    "|  **Company Dominance** | Company dominance in certain regions/categories | Competitive intelligence opportunities |\n",
    "\n",
    "---\n",
    "\n",
    "### Next Phase: Feature Engineering & Modeling\n",
    "\n",
    "#### Ready for Step 5\n",
    "\n",
    "The EDA has revealed clear patterns and trends that will inform our modeling approach:\n",
    "\n",
    "| Area | EDA Insight | Modeling Application |\n",
    "|------|-------------|----------------------|\n",
    "| **Geography** | Strong country/city concentrations | Geographic features for prediction models |\n",
    "| **Job Categories** | Clear hierarchical structure | Category-based feature engineering |\n",
    "| **Temporal Data** | Posting duration patterns | Time-based features for forecasting |\n",
    "| **Text Data** | Title/description keywords | NLP features for skill extraction |\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset Summary\n",
    "\n",
    "| Attribute | Details |\n",
    "|-----------|---------|\n",
    "|  **Current Shape** | 45,234 rows × 28 columns |\n",
    "|  **Data Quality** | Cleaned and validated |\n",
    "|  **Missing Data** | Handled appropriately |\n",
    "|  **Feature Types** | Numerical, Categorical, Text, Temporal |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9292bcaa-8687-4a04-820f-f5dd9e983a1e",
   "metadata": {},
   "source": [
    "# 5. Feature Engineering and Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d692ad0d-8b72-4284-aca8-d9fc02ceeade",
   "metadata": {},
   "source": [
    "## 5.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffdd3256-a923-4962-9221-ffa2d12cd53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 5: FEATURE ENGINEERING & MODELING\n",
      "======================================================================\n",
      " Dataset shape: (9807, 48)\n",
      " Total samples: 9,807\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 5: FEATURE ENGINEERING & MODELING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check shape and samples\n",
    "print(f\" Dataset shape: {Job_df.shape}\")\n",
    "print(f\" Total samples: {len(Job_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c2904a-d410-4b16-a14c-98b96d6d606a",
   "metadata": {},
   "source": [
    "### 5.1.1 Fixing and Standardizing Category Extraction\n",
    "\n",
    "Exploratory Data Analysis (EDA) revealed inconsistencies in the `Category_list` column:\n",
    "-  Some entries are stored as strings instead of lists\r\n",
    "- Some contain malformed JSON-like formatting\r\n",
    "- Some contain empty values (`''`, `\"[]\"`, `nan`, `null`)\r\n",
    "- Some include invalid categories such as `'unknown'`\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c99f8125-97c2-4016-aaa0-be6dc32d2a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "5.1.1 FIXING CATEGORY EXTRACTION\n",
      "======================================================================\n",
      "Investigating category extraction issue...\n",
      "\n",
      " Sample Category_list values:\n",
      "1. Type: <class 'str'>, Value: ['engineering', 'management', 'support']\n",
      "2. Type: <class 'str'>, Value: ['internship']\n",
      "3. Type: <class 'str'>, Value: ['engineering']\n",
      "4. Type: <class 'str'>, Value: ['information_technology', 'software_development']\n",
      "5. Type: <class 'str'>, Value: ['engineering', 'sales']\n",
      "\n",
      " Data type of Category_list: object\n",
      "\n",
      " Fixed Category Extraction Results:\n",
      "------------------------------------------------------------\n",
      "engineering                             : 2,566 ( 17.3%)\n",
      "management                              : 1,725 ( 11.6%)\n",
      "general                                 : 1,650 ( 11.1%)\n",
      "internship                              : 1,276 (  8.6%)\n",
      "information_technology                  :   940 (  6.3%)\n",
      "data_analysis                           :   640 (  4.3%)\n",
      "software_development                    :   638 (  4.3%)\n",
      "support                                 :   608 (  4.1%)\n",
      "manual_work                             :   575 (  3.9%)\n",
      "sales                                   :   543 (  3.7%)\n",
      "quality_assurance                       :   494 (  3.3%)\n",
      "finance                                 :   477 (  3.2%)\n",
      "purchasing                              :   348 (  2.3%)\n",
      "human_resources                         :   321 (  2.2%)\n",
      "design                                  :   276 (  1.9%)\n",
      "\\ Category Statistics:\n",
      "   • Total category mentions: 14,837\n",
      "   • Unique categories: 28\n",
      "   • 'general' categories: 1,650\n",
      "   • 'unknown' categories: 0\n",
      "\n",
      " Corrected Top Category: 'engineering' with 2,566 mentions\n",
      "\n",
      " Category list length distribution:\n",
      "   • 1 category/categories: 5,813 jobs ( 59.3%)\n",
      "   • 2 category/categories: 3,094 jobs ( 31.5%)\n",
      "   • 3 category/categories:   779 jobs (  7.9%)\n",
      "   • 4 category/categories:   109 jobs (  1.1%)\n",
      "   • 5 category/categories:     9 jobs (  0.1%)\n",
      "   • 6 category/categories:     3 jobs (  0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Header Formatting\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"5.1.1 FIXING CATEGORY EXTRACTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# The EDA showed ''' in categories, below is the fix\n",
    "if 'Category_list' in Job_df.columns:\n",
    "    print(\"Investigating category extraction issue...\")\n",
    "    \n",
    "    # Sample some category lists\n",
    "    sample_categories = Job_df['Category_list'].dropna().head(5)\n",
    "    print(f\"\\n Sample Category_list values:\")\n",
    "    for i, cats in enumerate(sample_categories, 1):\n",
    "        print(f\"{i}. Type: {type(cats)}, Value: {cats}\")\n",
    "    \n",
    "    # Check data type\n",
    "    print(f\"\\n Data type of Category_list: {Job_df['Category_list'].dtype}\")\n",
    "    \n",
    "    # Fix category extraction\n",
    "    def extract_categories(cat_str):\n",
    "        \"\"\"Extract categories from string representation of list\"\"\"\n",
    "        if pd.isna(cat_str):\n",
    "            return []\n",
    "        \n",
    "        # If already a list, return it (cleaned)\n",
    "        if isinstance(cat_str, list):\n",
    "            cleaned_cats = [str(cat).strip().strip(\"'\\\"\") for cat in cat_str]\n",
    "            return [cat for cat in cleaned_cats if cat and cat != \"''\" and cat != '\"\"']\n",
    "        \n",
    "        # If it's a string, try to parse it\n",
    "        if isinstance(cat_str, str):\n",
    "            cat_str = str(cat_str).strip()\n",
    "            \n",
    "            # Handle empty or meaningless strings\n",
    "            if not cat_str or cat_str in ['[]', \"''\", '\"\"', 'nan', 'null', 'none']:\n",
    "                return ['general']\n",
    "            \n",
    "            # Try to parse as JSON/list if it looks like one\n",
    "            try:\n",
    "                # Clean up common formatting issues\n",
    "                clean_str = cat_str.replace(\"'\", '\"')  # Standardize quotes\n",
    "                \n",
    "                # Handle brackets\n",
    "                if clean_str.startswith('[') and clean_str.endswith(']'):\n",
    "                    # Parse as JSON\n",
    "                    import json\n",
    "                    categories = json.loads(clean_str)\n",
    "                elif ',' in clean_str:\n",
    "                    # Split by comma (handling quotes properly)\n",
    "                    import re\n",
    "                    # Regex to split by commas not inside quotes\n",
    "                    categories = re.split(r',\\s*(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)', clean_str)\n",
    "                    categories = [cat.strip().strip('\"\\'') for cat in categories]\n",
    "                else:\n",
    "                    # Single category\n",
    "                    categories = [clean_str.strip('\"\\'')]\n",
    "                \n",
    "                # Clean and validate categories\n",
    "                cleaned_cats = []\n",
    "                for cat in categories:\n",
    "                    if isinstance(cat, str):\n",
    "                        cat = cat.strip().lower()\n",
    "                        if cat and cat not in ['', 'nan', 'null', 'none', 'unknown']:\n",
    "                            cleaned_cats.append(cat)\n",
    "                    elif isinstance(cat, (int, float)):\n",
    "                        # Convert numeric categories to string\n",
    "                        cleaned_cats.append(str(cat))\n",
    "                \n",
    "                # Return 'general' if no valid categories found\n",
    "                return cleaned_cats if cleaned_cats else ['general']\n",
    "                \n",
    "            except Exception as e:\n",
    "                # If parsing fails, check for common patterns\n",
    "                # Check if it looks like it was meant to be a list but has formatting issues\n",
    "                if any(marker in cat_str for marker in ['[', ']', \"'\", '\"']):\n",
    "                    # Try manual extraction\n",
    "                    clean_str = cat_str.strip(\"[]'\\\"\")\n",
    "                    if clean_str:\n",
    "                        categories = [cat.strip().strip(\"'\\\"\") \n",
    "                                    for cat in clean_str.split(',')]\n",
    "                        valid_cats = [cat for cat in categories \n",
    "                                    if cat and cat not in ['', 'nan', 'null']]\n",
    "                        return valid_cats if valid_cats else ['general']\n",
    "                \n",
    "                # Check if it's a single valid category\n",
    "                clean_cat = cat_str.strip().strip(\"'\\\"\")\n",
    "                if clean_cat and clean_cat.lower() not in ['', 'nan', 'null', 'none', 'unknown']:\n",
    "                    return [clean_cat.lower()]\n",
    "                \n",
    "                # Default to general category\n",
    "                return ['general']\n",
    "        \n",
    "        # For any other data type, convert to string and process\n",
    "        try:\n",
    "            return extract_categories(str(cat_str))\n",
    "        except:\n",
    "            return ['general']\n",
    "    \n",
    "    # Apply the fix\n",
    "    Job_df['categories_fixed'] = Job_df['Category_list'].apply(extract_categories)\n",
    "    \n",
    "    # Replace empty lists with ['general']\n",
    "    Job_df['categories_fixed'] = Job_df['categories_fixed'].apply(\n",
    "        lambda x: ['general'] if not x else x\n",
    "    )\n",
    "    \n",
    "    # Count categories again\n",
    "    all_categories_fixed = []\n",
    "    for categories in Job_df['categories_fixed']:\n",
    "        all_categories_fixed.extend(categories)\n",
    "    \n",
    "    category_counts_fixed = pd.Series(all_categories_fixed).value_counts().head(15)\n",
    "    \n",
    "    print(f\"\\n Fixed Category Extraction Results:\")\n",
    "    print(\"-\"*60)\n",
    "    total_cats = len(all_categories_fixed)\n",
    "    for category, count in category_counts_fixed.items():\n",
    "        pct = (count / total_cats) * 100\n",
    "        print(f\"{category:40}: {count:5,d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Check if 'unknown' is still present\n",
    "    unknown_count = sum(1 for cat in all_categories_fixed if cat == 'unknown')\n",
    "    general_count = sum(1 for cat in all_categories_fixed if cat == 'general')\n",
    "    \n",
    "    print(f\"\\ Category Statistics:\")\n",
    "    print(f\"   • Total category mentions: {total_cats:,}\")\n",
    "    print(f\"   • Unique categories: {len(set(all_categories_fixed)):,}\")\n",
    "    print(f\"   • 'general' categories: {general_count:,}\")\n",
    "    print(f\"   • 'unknown' categories: {unknown_count:,}\")\n",
    "    \n",
    "    if unknown_count > 0:\n",
    "        print(f\"\\n  Still have {unknown_count:,} 'unknown' categories\")\n",
    "        print(\"   Showing samples with 'unknown':\")\n",
    "        unknown_samples = Job_df[Job_df['categories_fixed'].apply(lambda x: 'unknown' in x)]\n",
    "        for i, (idx, row) in enumerate(unknown_samples.head(3).iterrows(), 1):\n",
    "            print(f\"   {i}. Original: {row['Category_list']} -> Fixed: {row['categories_fixed']}\")\n",
    "    \n",
    "    # Update the insights\n",
    "    top_category = category_counts_fixed.index[0] if len(category_counts_fixed) > 0 else \"N/A\"\n",
    "    top_category_count = category_counts_fixed.iloc[0] if len(category_counts_fixed) > 0 else 0\n",
    "    print(f\"\\n Corrected Top Category: '{top_category}' with {top_category_count:,} mentions\")\n",
    "    \n",
    "    # Show distribution of list lengths\n",
    "    print(f\"\\n Category list length distribution:\")\n",
    "    list_lengths = Job_df['categories_fixed'].apply(len).value_counts().sort_index()\n",
    "    for length, count in list_lengths.items():\n",
    "        pct = (count / len(Job_df)) * 100\n",
    "        print(f\"   • {length} category/categories: {count:5,d} jobs ({pct:5.1f}%)\")\n",
    "        \n",
    "else:\n",
    "    print(\" Category_list column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2214a6-c0dc-41c4-b626-74a987088bea",
   "metadata": {},
   "source": [
    "## 5.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc738f9-e045-43d5-be08-f51a575f8b9b",
   "metadata": {},
   "source": [
    "### 5.2.1 Text Features\n",
    "\n",
    "In this section, we engineer structured numerical and binary features from the `Description` column. \n",
    "Rather than immediately applying advanced NLP techniques (e.g., TF-IDF or embeddings), we first extract interpretable and lightweight text features that may improve model performance Specifically, we aim to:\r\n",
    "- Capture description length and complexity\r\n",
    "- Identify the presence of requirement-related language\r\n",
    "- Detect educational qualification requirements\r\n",
    "- Convert textual patterns into structured numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6db32233-14b4-4d6b-9cb1-259fea7de4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "5.2.1 TEXT FEATURE ENGINEERING\n",
      "======================================================================\n",
      "Creating text-based features from job descriptions...\n",
      " Created 17 text features\n",
      "\n",
      " Text Feature Statistics:\n",
      "------------------------------------------------------------\n",
      "   Average word count: 471\n",
      "   Average character count: 3401\n",
      "   Descriptions mentioning 'experience': 63.9%\n",
      "   Descriptions mentioning 'degree': 35.7%\n"
     ]
    }
   ],
   "source": [
    "# Header Formatting\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"5.2.1 TEXT FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Basic text features from Description\n",
    "if 'Description' in Job_df.columns:\n",
    "    print(\"Creating text-based features from job descriptions...\")\n",
    "    \n",
    "    # Text length features\n",
    "    Job_df['desc_word_count'] = Job_df['Description'].apply(lambda x: len(str(x).split()))\n",
    "    Job_df['desc_char_count'] = Job_df['Description'].apply(lambda x: len(str(x)))\n",
    "    Job_df['desc_avg_word_length'] = Job_df['desc_char_count'] / (Job_df['desc_word_count'] + 1)  # +1 to avoid division by zero\n",
    "    \n",
    "    # Check for requirements keywords\n",
    "    requirements_keywords = ['experience', 'skills', 'qualifications', 'requirements', 'must have', 'should have']\n",
    "    for keyword in requirements_keywords:\n",
    "        Job_df[f'desc_has_{keyword}'] = Job_df['Description'].str.contains(keyword, case=False, na=False).astype(int)\n",
    "    \n",
    "    # Check for degree requirements\n",
    "    degree_keywords = ['bachelor', 'master', 'phd', 'degree', 'bs', 'ms', 'ba', 'ma']\n",
    "    for degree in degree_keywords:\n",
    "        Job_df[f'desc_requires_{degree}'] = Job_df['Description'].str.contains(degree, case=False, na=False).astype(int)\n",
    "    \n",
    "    print(f\" Created {len(requirements_keywords) + len(degree_keywords) + 3} text features\")\n",
    "    \n",
    "    # Show text feature statistics\n",
    "    print(f\"\\n Text Feature Statistics:\")\n",
    "    print(\"-\"*60)\n",
    "    print(f\"   Average word count: {Job_df['desc_word_count'].mean():.0f}\")\n",
    "    print(f\"   Average character count: {Job_df['desc_char_count'].mean():.0f}\")\n",
    "    print(f\"   Descriptions mentioning 'experience': {(Job_df['desc_has_experience'].sum()/len(Job_df)*100):.1f}%\")\n",
    "    print(f\"   Descriptions mentioning 'degree': {(Job_df['desc_requires_degree'].sum()/len(Job_df)*100):.1f}%\")\n",
    "else:\n",
    "    print(\"Description column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa00654e-e512-4946-b18e-d0ef960505a3",
   "metadata": {},
   "source": [
    "### 5.2.2 Geographical Features\n",
    "\n",
    "Geographic information can significantly influence job characteristics such as salary levels, job demand, and hiring trends. However, location variables like country and state often contain many unique values (high cardinality), which can negatively impact model performance if encoded directly.\r\n",
    "\r\n",
    "In this section, we engineer structured geographic features to capture meaningful location patterns while controlling dimensionality and reducing sparsi. We aim to:\n",
    "- Reduce high-cardinality categorical variables  \r\n",
    "- Capture broader regional trends  \r\n",
    "- Create meaningful binary indicators  \r\n",
    "- Handle rare and missing geographic values appropriately  \r\n",
    ".ives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e6e451-29bf-4729-ab88-a21ea1191144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header Formatting\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"5.2.2 GEOGRAPHIC FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create geographic hierarchy features\n",
    "if 'country' in Job_df.columns:\n",
    "    print(\"Creating geographic features...\")\n",
    "    \n",
    "    # Country encoding (one-hot for top countries)\n",
    "    top_countries = Job_df['country'].value_counts().head(10).index.tolist()\n",
    "    Job_df['country_top'] = Job_df['country'].apply(lambda x: x if x in top_countries else 'Other')\n",
    "    \n",
    "    # Continent features\n",
    "    if 'continent' in Job_df.columns:\n",
    "        # One-hot encode continents\n",
    "        continent_dummies = pd.get_dummies(Job_df['continent'], prefix='continent')\n",
    "        Job_df = pd.concat([Job_df, continent_dummies], axis=1)\n",
    "    \n",
    "    # US state features (if applicable)\n",
    "    us_mask = Job_df['country'].str.contains('United States|USA|US', case=False, na=False)\n",
    "    if us_mask.any():\n",
    "        Job_df['is_us'] = us_mask.astype(int)\n",
    "        \n",
    "        if 'state' in Job_df.columns:\n",
    "            top_states = Job_df.loc[us_mask, 'state'].value_counts().head(10).index.tolist()\n",
    "            Job_df['state_top'] = Job_df['state'].apply(lambda x: x if x in top_states else ('Other' if pd.notna(x) else 'Unknown'))\n",
    "    \n",
    "    print(f\" Created geographic features\")\n",
    "    print(f\"   Top countries identified: {len(top_countries)}\")\n",
    "    print(f\"   US jobs: {us_mask.sum():,} ({us_mask.sum()/len(Job_df)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"Country column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320466cb-851d-4755-aa38-f476f8ba372b",
   "metadata": {},
   "source": [
    "### 5.2.3 Company Features\n",
    "\n",
    "Company-level characteristics can provide strong signals about hiring behavior, job stability, and market presence. However, company names are high-cardinality categorical variables, making direct encoding inefficient and prone to overfitting.\r\n",
    "\r\n",
    "In this section, we engineer aggregated company-level features that capture organizational scale, dominance, and hiring intensity without introducing excessive dimensionalit We aim to:\n",
    "- Transform raw company names into meaningful numerical or grouped features  \r\n",
    "- Capture organizational scale using posting frequency  \r\n",
    "- Identify dominant companies in the dataset  \r\n",
    "- Measure hiring intens over time \r\n",
    "y.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd1685-8afd-4e20-983c-27ecf09e3c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"5.2.3 COMPANY FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Company-based features\n",
    "if 'company_name' in Job_df.columns:\n",
    "    print(\"Creating company-based features...\")\n",
    "    \n",
    "    # Company size (based on number of postings)\n",
    "    company_post_counts = Job_df['company_name'].value_counts()\n",
    "    \n",
    "    # Categorize companies by size\n",
    "    def categorize_company_size(company):\n",
    "        count = company_post_counts.get(company, 0)\n",
    "        if count > 1000:\n",
    "            return 'very_large'\n",
    "        elif count > 100:\n",
    "            return 'large'\n",
    "        elif count > 10:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'small'\n",
    "    \n",
    "    Job_df['company_size'] = Job_df['company_name'].apply(categorize_company_size)\n",
    "    \n",
    "    # Top company indicator\n",
    "    top_companies = company_post_counts.head(5).index.tolist()\n",
    "    Job_df['is_top_company'] = Job_df['company_name'].isin(top_companies).astype(int)\n",
    "    \n",
    "    # Company posting frequency (jobs per day if we have date data)\n",
    "    if 'First Seen At' in Job_df.columns and pd.api.types.is_datetime64_any_dtype(Job_df['First Seen At']):\n",
    "        # Calculate company activity rate\n",
    "        company_first_post = Job_df.groupby('company_name')['First Seen At'].min()\n",
    "        company_last_post = Job_df.groupby('company_name')['First Seen At'].max()\n",
    "        \n",
    "        # Days active\n",
    "        company_days_active = (company_last_post - company_first_post).dt.days + 1  # +1 to avoid division by zero\n",
    "        company_post_rate = company_post_counts / company_days_active\n",
    "        \n",
    "        # Map back to dataframe\n",
    "        company_rate_dict = company_post_rate.to_dict()\n",
    "        Job_df['company_post_rate'] = Job_df['company_name'].map(company_rate_dict)\n",
    "        Job_df['company_post_rate'].fillna(0, inplace=True)\n",
    "    \n",
    "    print(f\" Created company features\")\n",
    "    print(f\"  Company size distribution:\")\n",
    "    print(Job_df['company_size'].value_counts())\n",
    "    print(f\"   Top companies identified: {len(top_companies)}\")\n",
    "else:\n",
    "    print(\"company_name column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3662dd63-1c29-450b-99be-497380ab4830",
   "metadata": {},
   "source": [
    "### 5.2.4 Temporal Features\n",
    "\n",
    "Job posting behavior often follows clear temporal patterns influenced by hiring cycles, budgeting periods, and work-week dynamics. Capturing when a job is posted can therefore provide valuable signals about demand intensity, urgency, and employer behavior.\r\n",
    "\r\n",
    "In this section, we extract structured time-based features from the job posting timestamps to model seasonal, weekly, and recency-related trend We aim to:\n",
    "- Capture seasonal and quarterly hiring patterns  \r\n",
    "- Differentiate weekday vs weekend posting behavior  \r\n",
    "- Extract temporal signals related to job posting recency  \r\n",
    "- Transform raw timestamps into model-friendly features  s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54382e9-b2ba-4534-aa78-9d94cd8e3d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"5.2.4 TEMPORAL FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Time-based features\n",
    "if 'First Seen At' in Job_df.columns and pd.api.types.is_datetime64_any_dtype(Job_df['First Seen At']):\n",
    "    print(\"Creating temporal features...\")\n",
    "    \n",
    "    # Time of year features\n",
    "    Job_df['post_month'] = Job_df['First Seen At'].dt.month\n",
    "    Job_df['post_quarter'] = Job_df['First Seen At'].dt.quarter\n",
    "    Job_df['post_dayofweek'] = Job_df['First Seen At'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "    \n",
    "    # Seasonal features\n",
    "    Job_df['is_q1'] = (Job_df['post_quarter'] == 1).astype(int)\n",
    "    Job_df['is_q2'] = (Job_df['post_quarter'] == 2).astype(int)\n",
    "    Job_df['is_q3'] = (Job_df['post_quarter'] == 3).astype(int)\n",
    "    Job_df['is_q4'] = (Job_df['post_quarter'] == 4).astype(int)\n",
    "    \n",
    "    # Weekend vs weekday\n",
    "    Job_df['is_weekend'] = Job_df['post_dayofweek'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Time since first post (recency)\n",
    "    if 'posting_duration_days' not in Job_df.columns and 'Last Seen At' in Job_df.columns:\n",
    "        Job_df['posting_duration_days'] = (Job_df['Last Seen At'] - Job_df['First Seen At']).dt.days\n",
    "    \n",
    "    print(f\" Created temporal features\")\n",
    "    print(f\"   Month distribution: {Job_df['post_month'].value_counts().sort_index().to_dict()}\")\n",
    "    print(f\"   Weekend posts: {Job_df['is_weekend'].sum():,} ({Job_df['is_weekend'].sum()/len(Job_df)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"Date columns not available for temporal features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7424a35a-d6a4-4fb0-a9a9-34cab1ae00e2",
   "metadata": {},
   "source": [
    "### 5.2.5 Composite Features\n",
    "\n",
    "While individual features capture isolated signals, many real-world patterns emerge from **interactions between variables**. Composite feature engineering focuses on combining related attributes to expose higher-order relationships that may better reflect job complexity, seniority, and role specialization.\r\n",
    "\r\n",
    "In this section, we construct interaction and aggregation features that blend seniority, job categories, title indicators, and organizational contex We aim to:\n",
    "- Capture interactions between seniority and job function  \r\n",
    "- Quantify role complexity using multiple title indicators  \r\n",
    "- Identify technical specialization within job categories  \r\n",
    "- Lay groundwork for future company–location interaction features  t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c9df7b8-1da5-4d34-84a7-78773e852fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "5.2.5 COMPOSITE FEATURE ENGINEERING\n",
      "======================================================================\n",
      "🔧 Creating composite features...\n",
      "✅ Created composite features\n",
      "   Total features after engineering: 71\n"
     ]
    }
   ],
   "source": [
    "# Header formatting\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"5.2.5 COMPOSITE FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"🔧 Creating composite features...\")\n",
    "\n",
    "# 1. Seniority-Category combinations\n",
    "if 'Seniority_clean' in Job_df.columns and 'categories_fixed' in Job_df.columns:\n",
    "    # Create seniority-category interaction\n",
    "    Job_df['seniority_level'] = Job_df['Seniority_clean'].map({\n",
    "        'individual_contributor': 1,\n",
    "        'manager': 2,\n",
    "        'director_level': 3,\n",
    "        'executive': 4,\n",
    "        'other': 0\n",
    "    }).fillna(0)\n",
    "    \n",
    "    # Count categories per job\n",
    "    Job_df['num_categories'] = Job_df['categories_fixed'].apply(len)\n",
    "    \n",
    "    # Has technical category flag\n",
    "    technical_categories = ['engineering', 'software_development', 'information_technology', 'data_science']\n",
    "    Job_df['has_technical_category'] = Job_df['categories_fixed'].apply(\n",
    "        lambda cats: any(tech_cat in str(cats) for tech_cat in technical_categories)\n",
    "    ).astype(int)\n",
    "\n",
    "# 2. Title-Composite features\n",
    "title_indicators = ['title_has_senior', 'title_has_manager', 'title_has_engineer', 'title_has_developer']\n",
    "existing_title_indicators = [ind for ind in title_indicators if ind in Job_df.columns]\n",
    "\n",
    "if existing_title_indicators:\n",
    "    # Create title complexity score\n",
    "    Job_df['title_complexity'] = Job_df[existing_title_indicators].sum(axis=1)\n",
    "    \n",
    "    # Senior engineer flag\n",
    "    if 'title_has_senior' in Job_df.columns and 'title_has_engineer' in Job_df.columns:\n",
    "        Job_df['is_senior_engineer'] = (Job_df['title_has_senior'] & Job_df['title_has_engineer']).astype(int)\n",
    "\n",
    "# 3. Location-Company interactions\n",
    "if 'country' in Job_df.columns and 'company_name' in Job_df.columns:\n",
    "    # Company-country presence (just create a count for now)\n",
    "    company_country_counts = Job_df.groupby(['company_name', 'country']).size()\n",
    "    # We can use this later if needed\n",
    "\n",
    "print(f\"✅ Created composite features\")\n",
    "print(f\"   Total features after engineering: {len(Job_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1538554-b919-4e7c-be08-8126d0bd8f51",
   "metadata": {},
   "source": [
    "## 5.3 Feature Selection\n",
    "\n",
    "After completing feature engineering, the next step is to systematically organize and prepare features for modeling. Rather than manually selecting columns, we group engineered features into logical categories and dynamically identify which ones are available in the dataset.\n",
    "This ensures:\n",
    "- Structured feature organization  \r\n",
    "- Flexibility if certain columns are missing  \r\n",
    "- Scalability for future feature additions  \r\n",
    "- Cleaner modeling pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a94b247d-52a8-4dbd-b22d-1e640296a972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "5.3 FEATURE SELECTION & PREPARATION\n",
      "======================================================================\n",
      "Categorizing features for modeling...\n",
      "\n",
      " Available features for modeling: 31\n",
      "\n",
      "Feature breakdown by category:\n",
      "   Temporal       :  1 features\n",
      "   Text           : 17 features\n",
      "   Seniority      :  2 features\n",
      "   Title          :  8 features\n",
      "   Category       :  2 features\n",
      "   Contract       :  1 features\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Header Formatting\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"5.3 FEATURE SELECTION & PREPARATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define feature categories\n",
    "print(\"Categorizing features for modeling...\")\n",
    "\n",
    "feature_categories = {\n",
    "    'Geographic': ['country_top', 'is_us', 'continent_*'],\n",
    "    'Company': ['company_size', 'is_top_company', 'company_post_rate'],\n",
    "    'Temporal': ['post_month', 'post_quarter', 'post_dayofweek', 'is_weekend', 'posting_duration_days'],\n",
    "    'Text': ['desc_word_count', 'desc_char_count', 'desc_avg_word_length', 'desc_has_*', 'desc_requires_*'],\n",
    "    'Seniority': ['Seniority_clean', 'seniority_level'],\n",
    "    'Title': ['title_has_*', 'title_complexity', 'is_senior_engineer'],\n",
    "    'Category': ['num_categories', 'has_technical_category'],\n",
    "    'Contract': ['Contract_Type_primary']\n",
    "}\n",
    "\n",
    "# Identify available features\n",
    "available_features = []\n",
    "for category, features in feature_categories.items():\n",
    "    for feature in features:\n",
    "        if '*' in feature:\n",
    "            # Pattern matching for wildcards\n",
    "            pattern = feature.replace('*', '.*')\n",
    "            matching_features = [col for col in Job_df.columns if re.match(pattern, col)]\n",
    "            available_features.extend(matching_features)\n",
    "        elif feature in Job_df.columns:\n",
    "            available_features.append(feature)\n",
    "\n",
    "print(f\"\\n Available features for modeling: {len(available_features)}\")\n",
    "print(f\"\\nFeature breakdown by category:\")\n",
    "\n",
    "# Count features by category\n",
    "for category in feature_categories.keys():\n",
    "    cat_features = [f for f in available_features if any(f.startswith(prefix.replace('*', '')) \n",
    "                     for prefix in feature_categories[category] if '*' in prefix) or \n",
    "                     f in feature_categories[category]]\n",
    "    if cat_features:\n",
    "        print(f\"   {category:15}: {len(cat_features):2d} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f86afa9-6dda-4567-9630-e12233218be0",
   "metadata": {},
   "source": [
    "## 5.4 Target Variable Definition\n",
    "\n",
    "After completing feature engineering and selection, the next step is to define the **target variable(s)** for supervised modeling. \r\n",
    "\r\n",
    "This section dynamically constructs and evaluates several potential target variables based on data availability and class balanc The aim is to:\n",
    "- Define meaningful prediction targets  \r\n",
    "- Ensure sufficient class representation  \r\n",
    "- Prevent extreme class imbalance  \r\n",
    "- Enable flexible experimentation across multiple modeling taskse.\r\n",
    "g tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c81d4aee-ff54-4a5c-94f4-e9aa6de76bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "5.4 TARGET VARIABLE DEFINITION\n",
      "======================================================================\n",
      "Defining target variables for modeling...\n",
      " Category target: 23 classes\n",
      "   Top categories: {'engineering': 2361, 'general': 1650, 'management': 1153, 'internship': 912, 'data_analysis': 631}\n",
      " Seniority target: 4 classes\n",
      " US target: 2,578 US jobs (26.3%)\n",
      " Full-time target: 5,348 full-time jobs (54.5%)\n",
      "\n",
      " Available target variables:\n",
      " 1. Category Prediction       - 23 categories\n",
      " 2. Seniority Prediction      - 4 levels\n",
      " 3. US Job Prediction         - Binary classification\n",
      " 4. Full-time Prediction      - Binary classification\n"
     ]
    }
   ],
   "source": [
    "# Header Formatting\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"5.4 TARGET VARIABLE DEFINITION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"Defining target variables for modeling...\")\n",
    "\n",
    "target_options = []\n",
    "\n",
    "# Option 1: Job Category Prediction\n",
    "if 'categories_fixed' in Job_df.columns:\n",
    "    # Create simplified category target (primary category)\n",
    "    Job_df['primary_category'] = Job_df['categories_fixed'].apply(\n",
    "        lambda cats: cats[0] if cats and len(cats) > 0 else 'unknown'\n",
    "    )\n",
    "    \n",
    "    # Only use categories with sufficient samples\n",
    "    category_counts = Job_df['primary_category'].value_counts()\n",
    "    min_samples = 50  # Minimum samples per category\n",
    "    valid_categories = category_counts[category_counts >= min_samples].index.tolist()\n",
    "    \n",
    "    Job_df['category_target'] = Job_df['primary_category'].apply(\n",
    "        lambda x: x if x in valid_categories else 'other'\n",
    "    )\n",
    "    \n",
    "    target_options.append((\"Category Prediction\", f\"{Job_df['category_target'].nunique()} categories\"))\n",
    "    print(f\" Category target: {Job_df['category_target'].nunique()} classes\")\n",
    "    print(f\"   Top categories: {Job_df['category_target'].value_counts().head(5).to_dict()}\")\n",
    "\n",
    "# Option 2: Seniority Level Prediction\n",
    "if 'Seniority_clean' in Job_df.columns:\n",
    "    seniority_counts = Job_df['Seniority_clean'].value_counts()\n",
    "    valid_seniority = seniority_counts[seniority_counts >= 50].index.tolist()\n",
    "    \n",
    "    Job_df['seniority_target'] = Job_df['Seniority_clean'].apply(\n",
    "        lambda x: x if x in valid_seniority else 'other'\n",
    "    )\n",
    "    \n",
    "    target_options.append((\"Seniority Prediction\", f\"{Job_df['seniority_target'].nunique()} levels\"))\n",
    "    print(f\" Seniority target: {Job_df['seniority_target'].nunique()} classes\")\n",
    "\n",
    "# Option 3: Geographic Prediction (US vs Non-US)\n",
    "if 'country' in Job_df.columns:\n",
    "    Job_df['us_target'] = Job_df['country'].str.contains('United States|USA|US', case=False, na=False).astype(int)\n",
    "    target_options.append((\"US Job Prediction\", \"Binary classification\"))\n",
    "    print(f\" US target: {Job_df['us_target'].sum():,} US jobs ({Job_df['us_target'].sum()/len(Job_df)*100:.1f}%)\")\n",
    "\n",
    "# Option 4: Full-time vs Other\n",
    "if 'Contract_Type_primary' in Job_df.columns:\n",
    "    Job_df['fulltime_target'] = (Job_df['Contract_Type_primary'] == 'full_time').astype(int)\n",
    "    target_options.append((\"Full-time Prediction\", \"Binary classification\"))\n",
    "    print(f\" Full-time target: {Job_df['fulltime_target'].sum():,} full-time jobs ({Job_df['fulltime_target'].sum()/len(Job_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n Available target variables:\")\n",
    "for i, (target_name, description) in enumerate(target_options, 1):\n",
    "    print(f\"{i:2}. {target_name:25} - {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf3515f-baa3-49a0-9a2e-3a081fc95d31",
   "metadata": {},
   "source": [
    "## 5.5 Feature Encoding and Scaling\n",
    "\n",
    "After defining the target variable and selecting relevant predictors, the next step is to prepare the feature matrix for machine learning. \r\n",
    "\r\n",
    "This section performs feature selection, categorical encoding, missing value handling, and feature scaling to produce a model-ready dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92e6e13-a2ce-474a-acaa-17c961bca377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header Formatting\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"5.5 FEATURE ENCODING & SCALING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"Preparing features for machine learning...\")\n",
    "\n",
    "# Select features for modeling (basic set to start)\n",
    "basic_features = [\n",
    "    'seniority_level',\n",
    "    'num_categories',\n",
    "    'has_technical_category',\n",
    "    'desc_word_count',\n",
    "    'desc_char_count',\n",
    "    'is_us' if 'is_us' in Job_df.columns else None,\n",
    "    'company_size' if 'company_size' in Job_df.columns else None,\n",
    "    'post_month' if 'post_month' in Job_df.columns else None,\n",
    "    'posting_duration_days' if 'posting_duration_days' in Job_df.columns else None\n",
    "]\n",
    "\n",
    "# Filter out None values\n",
    "basic_features = [f for f in basic_features if f is not None and f in Job_df.columns]\n",
    "\n",
    "print(f\"\\n Selected {len(basic_features)} basic features for initial modeling:\")\n",
    "for feature in basic_features:\n",
    "    print(f\"   - {feature}\")\n",
    "\n",
    "# Prepare feature matrix X\n",
    "X = Job_df[basic_features].copy()\n",
    "\n",
    "# Handle categorical features\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "if categorical_cols:\n",
    "    print(f\"\\n Encoding categorical features: {categorical_cols}\")\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].fillna('missing'))\n",
    "\n",
    "# Handle missing values\n",
    "print(f\"\\n Handling missing values...\")\n",
    "missing_before = X.isnull().sum().sum()\n",
    "X = X.fillna(X.median(numeric_only=True))  # For numerical features\n",
    "missing_after = X.isnull().sum().sum()\n",
    "print(f\"   Fixed {missing_before - missing_after} missing values\")\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"\\n Feature preparation complete:\")\n",
    "print(f\"   Feature matrix shape: {X_scaled.shape}\")\n",
    "print(f\"   Samples: {X_scaled.shape[0]}\")\n",
    "print(f\"   Features: {X_scaled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ef671-e09b-4a69-a5a6-2db5bc4cb41f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
