{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38244c4f-93f2-41a4-91e8-ff801e6185d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job Market Intelligence System: Problem Statement\n",
    "\n",
    "## 1. Context & Problem\n",
    "\n",
    "The current job market is fragmented and opaque, creating significant inefficiencies for three key stakeholder groups:\n",
    "\n",
    "- **Job Seekers** face information overload, skill uncertainty, and lack of salary transparency.\n",
    "- **HR Professionals & Recruiters** struggle with competitive hiring, compensation benchmarking, and identifying skill gaps.\n",
    "- **Educational Institutions & Career Counselors** operate with outdated curriculum and lack real-time market data for guidance.\n",
    "\n",
    "**Core Problem:** There is no unified, data-driven system that transforms raw job posting data into actionable, real-time insights for all stakeholders.\n",
    "\n",
    "## 2. Project Goal\n",
    "\n",
    "To develop a **Job Market Intelligence System** that analyzes job posting data to generate clear, actionable insights on skill demand, geographic opportunity, salary benchmarks, and market trends.\n",
    "\n",
    "## 3. Key Objectives\n",
    "\n",
    "1.  **Skill Demand Analysis:** Identify trending and declining technical skills.\n",
    "2.  **Geographic Opportunity Mapping:** Visualize job distribution and hotspots.\n",
    "3.  **Salary Benchmarking:** Estimate compensation by role, experience, and location.\n",
    "4.  **Job Classification & Trend Identification:** Categorize postings and spot emerging roles.\n",
    "\n",
    "## 4. Primary Business Questions\n",
    "\n",
    "- **For Job Seekers:** \"What skills should I learn, where are the jobs, and what salary can I expect?\"\n",
    "- **For HR/Recruiters:** \"How competitive is the market, and are our offers aligned?\"\n",
    "- **For Educators:** \"Which skills and emerging roles should we teach for?\"\n",
    "\n",
    "## 5. Success Metrics\n",
    "\n",
    "- **Technical:** >80% classification accuracy; <$15k MAE for salary prediction.\n",
    "- **Business:** Delivery of actionable insights, clear visualizations, and identifiable market patterns to all stakeholder groups.\n",
    "\n",
    "## 6. Project Scope\n",
    "\n",
    "**In-Scope (Initial Focus):**\n",
    "- Analysis of provided job posting datasets.\n",
    "- Focus on English-language technical/professional roles.\n",
    "- Skills extraction and trend analysis from job descriptions.\n",
    "\n",
    "**Value Delivered:**\n",
    "- **Job Seekers:** Reduced search time, clearer career paths.\n",
    "- **HR Professionals:** Competitive intelligence, optimized recruitment.\n",
    "- **Educators:** Data-driven curriculum alignment and career guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3302811f-3676-46b3-bc16-010e1f457796",
   "metadata": {},
   "source": [
    "# Data Exploration and Quality Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc24481-f47b-4a08-864b-ecdc002b176c",
   "metadata": {},
   "source": [
    "Let us now explore our dataset and understand its structure, quality and potential for our project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60bb3a2-7681-45fc-a3eb-14e3286e1fc7",
   "metadata": {},
   "source": [
    "## Data Loading and Inspection\n",
    "We will now load the data andexamine its basic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de6e5077-bc94-4980-8a15-0966fa0b04eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xd0 in position 1736: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m Job_Posting_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob_Posting_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m Job_Posting_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xd0 in position 1736: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "Job_Posting_df = pd.read_csv(\"Job_Posting_data.csv\")\n",
    "Job_Posting_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23890dd7-67d2-4dcc-8773-b548692df3c4",
   "metadata": {},
   "source": [
    "- We got an error when trying to read in the dataset due to the unique encoding of the data inside the dataset. Therefore, we had to employ some encoding to debug the dataset and make it readable by the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aab2e970-d665-480d-9dd9-4c9e3f470d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying different encodings...\n",
      "‚úÖ SUCCESS with ISO-8859-1 encoding!\n",
      "   Shape: (9919, 21)\n",
      "   Columns: 21\n",
      "\n",
      "First 3 rows:\n",
      "  Website Domain  Ticker                                  Job Opening Title  \\\n",
      "0      bosch.com     NaN  IN_RBAI_Assistant Manager_Dispensing Process E...   \n",
      "1      bosch.com     NaN  Professional Internship: Hardware Development ...   \n",
      "2         zf.com     NaN                      Process Expert BMS Production   \n",
      "\n",
      "                                     Job Opening URL         First Seen At  \\\n",
      "0  https://jobs.smartrecruiters.com/BoschGroup/74...  2024-05-29T19:59:45Z   \n",
      "1  https://jobs.smartrecruiters.com/BoschGroup/74...  2024-05-04T01:00:12Z   \n",
      "2  https://jobs.zf.com/job/Shenyang-Process-Exper...  2024-04-19T06:47:24Z   \n",
      "\n",
      "           Last Seen At                 Location  \\\n",
      "0  2024-07-31T14:35:44Z   Indiana, United States   \n",
      "1  2024-07-29T17:46:16Z  Delaware, United States   \n",
      "2  2024-05-16T02:25:08Z                    China   \n",
      "\n",
      "                                       Location Data  \\\n",
      "0  [{\"city\":null,\"state\":\"Indiana\",\"zip_code\":nul...   \n",
      "1  [{\"city\":null,\"state\":\"Delaware\",\"zip_code\":nu...   \n",
      "2  [{\"city\":null,\"state\":null,\"zip_code\":null,\"co...   \n",
      "\n",
      "                           Category    Seniority  ...  \\\n",
      "0  engineering, management, support      manager  ...   \n",
      "1                        internship  non_manager  ...   \n",
      "2                       engineering  non_manager  ...   \n",
      "\n",
      "                                         Description Salary  \\\n",
      "0  **IN\\_RBAI\\_Assistant Manager\\_Dispensing Proc...    NaN   \n",
      "1  **Professional Internship: Hardware Developmen...    NaN   \n",
      "2  ZF is a global technology company supplying sy...    NaN   \n",
      "\n",
      "                                         Salary Data  \\\n",
      "0  {\"salary_low\":null,\"salary_high\":null,\"salary_...   \n",
      "1  {\"salary_low\":null,\"salary_high\":null,\"salary_...   \n",
      "2  {\"salary_low\":null,\"salary_high\":null,\"salary_...   \n",
      "\n",
      "               Contract Types Job Status Job Language Job Last Processed At  \\\n",
      "0                   full time     closed           en  2024-08-02T14:47:55Z   \n",
      "1  full time, internship, m/f     closed           en  2024-07-31T17:50:07Z   \n",
      "2                         NaN     closed           en  2024-05-18T02:32:04Z   \n",
      "\n",
      "   O*NET Code                       O*NET Family  \\\n",
      "0  43-1011.00  Office and Administrative Support   \n",
      "1  17-2061.00       Architecture and Engineering   \n",
      "2  51-9141.00                         Production   \n",
      "\n",
      "                               O*NET Occupation Name  \n",
      "0  First-Line Supervisors of Office and Administr...  \n",
      "1                        Computer Hardware Engineers  \n",
      "2               Semiconductor Processing Technicians  \n",
      "\n",
      "[3 rows x 21 columns]\n",
      "\n",
      "Column names:\n",
      "   1. Website Domain\n",
      "   2. Ticker\n",
      "   3. Job Opening Title\n",
      "   4. Job Opening URL\n",
      "   5. First Seen At\n",
      "   6. Last Seen At\n",
      "   7. Location\n",
      "   8. Location Data\n",
      "   9. Category\n",
      "  10. Seniority\n",
      "  11. Keywords\n",
      "  12. Description\n",
      "  13. Salary\n",
      "  14. Salary Data\n",
      "  15. Contract Types\n",
      "  16. Job Status\n",
      "  17. Job Language\n",
      "  18. Job Last Processed At\n",
      "  19. O*NET Code\n",
      "  20. O*NET Family\n",
      "  21. O*NET Occupation Name\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "encodings_to_try = ['ISO-8859-1', 'cp1252', 'latin1', 'windows-1252', 'utf-8-sig', 'mac_roman']\n",
    "\n",
    "print(\"Trying different encodings...\")\n",
    "for encoding in encodings_to_try:\n",
    "    try:\n",
    "        Job_Posting_df = pd.read_csv(\"Job_Posting_data.csv\", encoding=encoding)\n",
    "        print(f\"‚úÖ SUCCESS with {encoding} encoding!\")\n",
    "        print(f\"   Shape: {Job_Posting_df.shape}\")\n",
    "        print(f\"   Columns: {len(Job_Posting_df.columns)}\")\n",
    "        print(f\"\\nFirst 3 rows:\")\n",
    "        print(Job_Posting_df.head(3))\n",
    "        print(\"\\nColumn names:\")\n",
    "        for i, col in enumerate(Job_Posting_df.columns, 1):\n",
    "            print(f\"  {i:2}. {col}\")\n",
    "        break\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"‚ùå Failed with {encoding}: {str(e)[:50]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed with {encoding}: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f51d6f-7cc9-492b-a2cb-e1fe1b83f56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Website Domain</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Job Opening Title</th>\n",
       "      <th>Job Opening URL</th>\n",
       "      <th>First Seen At</th>\n",
       "      <th>Last Seen At</th>\n",
       "      <th>Location</th>\n",
       "      <th>Location Data</th>\n",
       "      <th>Category</th>\n",
       "      <th>Seniority</th>\n",
       "      <th>...</th>\n",
       "      <th>Description</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Salary Data</th>\n",
       "      <th>Contract Types</th>\n",
       "      <th>Job Status</th>\n",
       "      <th>Job Language</th>\n",
       "      <th>Job Last Processed At</th>\n",
       "      <th>O*NET Code</th>\n",
       "      <th>O*NET Family</th>\n",
       "      <th>O*NET Occupation Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bosch.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IN_RBAI_Assistant Manager_Dispensing Process E...</td>\n",
       "      <td>https://jobs.smartrecruiters.com/BoschGroup/74...</td>\n",
       "      <td>2024-05-29T19:59:45Z</td>\n",
       "      <td>2024-07-31T14:35:44Z</td>\n",
       "      <td>Indiana, United States</td>\n",
       "      <td>[{\"city\":null,\"state\":\"Indiana\",\"zip_code\":nul...</td>\n",
       "      <td>engineering, management, support</td>\n",
       "      <td>manager</td>\n",
       "      <td>...</td>\n",
       "      <td>**IN\\_RBAI\\_Assistant Manager\\_Dispensing Proc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"salary_low\":null,\"salary_high\":null,\"salary_...</td>\n",
       "      <td>full time</td>\n",
       "      <td>closed</td>\n",
       "      <td>en</td>\n",
       "      <td>2024-08-02T14:47:55Z</td>\n",
       "      <td>43-1011.00</td>\n",
       "      <td>Office and Administrative Support</td>\n",
       "      <td>First-Line Supervisors of Office and Administr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bosch.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Professional Internship: Hardware Development ...</td>\n",
       "      <td>https://jobs.smartrecruiters.com/BoschGroup/74...</td>\n",
       "      <td>2024-05-04T01:00:12Z</td>\n",
       "      <td>2024-07-29T17:46:16Z</td>\n",
       "      <td>Delaware, United States</td>\n",
       "      <td>[{\"city\":null,\"state\":\"Delaware\",\"zip_code\":nu...</td>\n",
       "      <td>internship</td>\n",
       "      <td>non_manager</td>\n",
       "      <td>...</td>\n",
       "      <td>**Professional Internship: Hardware Developmen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"salary_low\":null,\"salary_high\":null,\"salary_...</td>\n",
       "      <td>full time, internship, m/f</td>\n",
       "      <td>closed</td>\n",
       "      <td>en</td>\n",
       "      <td>2024-07-31T17:50:07Z</td>\n",
       "      <td>17-2061.00</td>\n",
       "      <td>Architecture and Engineering</td>\n",
       "      <td>Computer Hardware Engineers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zf.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Process Expert BMS Production</td>\n",
       "      <td>https://jobs.zf.com/job/Shenyang-Process-Exper...</td>\n",
       "      <td>2024-04-19T06:47:24Z</td>\n",
       "      <td>2024-05-16T02:25:08Z</td>\n",
       "      <td>China</td>\n",
       "      <td>[{\"city\":null,\"state\":null,\"zip_code\":null,\"co...</td>\n",
       "      <td>engineering</td>\n",
       "      <td>non_manager</td>\n",
       "      <td>...</td>\n",
       "      <td>ZF is a global technology company supplying sy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"salary_low\":null,\"salary_high\":null,\"salary_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>closed</td>\n",
       "      <td>en</td>\n",
       "      <td>2024-05-18T02:32:04Z</td>\n",
       "      <td>51-9141.00</td>\n",
       "      <td>Production</td>\n",
       "      <td>Semiconductor Processing Technicians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bosch.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DevOps Developer with Python for ADAS Computin...</td>\n",
       "      <td>https://jobs.smartrecruiters.com/BoschGroup/74...</td>\n",
       "      <td>2024-08-16T10:20:37Z</td>\n",
       "      <td>2024-08-22T11:14:49Z</td>\n",
       "      <td>Romania</td>\n",
       "      <td>[{\"city\":null,\"state\":null,\"zip_code\":null,\"co...</td>\n",
       "      <td>information_technology, software_development</td>\n",
       "      <td>non_manager</td>\n",
       "      <td>...</td>\n",
       "      <td>**DevOps Developer with Python for ADAS Comput...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"salary_low\":null,\"salary_high\":null,\"salary_...</td>\n",
       "      <td>full time</td>\n",
       "      <td>closed</td>\n",
       "      <td>en</td>\n",
       "      <td>2024-08-23T00:33:30Z</td>\n",
       "      <td>15-1252.00</td>\n",
       "      <td>Computer and Mathematical</td>\n",
       "      <td>Software Developers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bosch.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Senior Engineer Sales - Video Systems and Solu...</td>\n",
       "      <td>https://jobs.smartrecruiters.com/BoschGroup/74...</td>\n",
       "      <td>2024-07-01T17:31:20Z</td>\n",
       "      <td>2024-08-01T05:11:33Z</td>\n",
       "      <td>India</td>\n",
       "      <td>[{\"city\":null,\"state\":null,\"zip_code\":null,\"co...</td>\n",
       "      <td>engineering, sales</td>\n",
       "      <td>non_manager</td>\n",
       "      <td>...</td>\n",
       "      <td>**Senior Engineer Sales - Video Systems and So...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"salary_low\":null,\"salary_high\":null,\"salary_...</td>\n",
       "      <td>full time</td>\n",
       "      <td>closed</td>\n",
       "      <td>en</td>\n",
       "      <td>2024-08-02T19:03:16Z</td>\n",
       "      <td>41-9031.00</td>\n",
       "      <td>Sales and Related</td>\n",
       "      <td>Sales Engineers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Website Domain  Ticker                                  Job Opening Title  \\\n",
       "0      bosch.com     NaN  IN_RBAI_Assistant Manager_Dispensing Process E...   \n",
       "1      bosch.com     NaN  Professional Internship: Hardware Development ...   \n",
       "2         zf.com     NaN                      Process Expert BMS Production   \n",
       "3      bosch.com     NaN  DevOps Developer with Python for ADAS Computin...   \n",
       "4      bosch.com     NaN  Senior Engineer Sales - Video Systems and Solu...   \n",
       "\n",
       "                                     Job Opening URL         First Seen At  \\\n",
       "0  https://jobs.smartrecruiters.com/BoschGroup/74...  2024-05-29T19:59:45Z   \n",
       "1  https://jobs.smartrecruiters.com/BoschGroup/74...  2024-05-04T01:00:12Z   \n",
       "2  https://jobs.zf.com/job/Shenyang-Process-Exper...  2024-04-19T06:47:24Z   \n",
       "3  https://jobs.smartrecruiters.com/BoschGroup/74...  2024-08-16T10:20:37Z   \n",
       "4  https://jobs.smartrecruiters.com/BoschGroup/74...  2024-07-01T17:31:20Z   \n",
       "\n",
       "           Last Seen At                 Location  \\\n",
       "0  2024-07-31T14:35:44Z   Indiana, United States   \n",
       "1  2024-07-29T17:46:16Z  Delaware, United States   \n",
       "2  2024-05-16T02:25:08Z                    China   \n",
       "3  2024-08-22T11:14:49Z                  Romania   \n",
       "4  2024-08-01T05:11:33Z                    India   \n",
       "\n",
       "                                       Location Data  \\\n",
       "0  [{\"city\":null,\"state\":\"Indiana\",\"zip_code\":nul...   \n",
       "1  [{\"city\":null,\"state\":\"Delaware\",\"zip_code\":nu...   \n",
       "2  [{\"city\":null,\"state\":null,\"zip_code\":null,\"co...   \n",
       "3  [{\"city\":null,\"state\":null,\"zip_code\":null,\"co...   \n",
       "4  [{\"city\":null,\"state\":null,\"zip_code\":null,\"co...   \n",
       "\n",
       "                                       Category    Seniority  ...  \\\n",
       "0              engineering, management, support      manager  ...   \n",
       "1                                    internship  non_manager  ...   \n",
       "2                                   engineering  non_manager  ...   \n",
       "3  information_technology, software_development  non_manager  ...   \n",
       "4                            engineering, sales  non_manager  ...   \n",
       "\n",
       "                                         Description Salary  \\\n",
       "0  **IN\\_RBAI\\_Assistant Manager\\_Dispensing Proc...    NaN   \n",
       "1  **Professional Internship: Hardware Developmen...    NaN   \n",
       "2  ZF is a global technology company supplying sy...    NaN   \n",
       "3  **DevOps Developer with Python for ADAS Comput...    NaN   \n",
       "4  **Senior Engineer Sales - Video Systems and So...    NaN   \n",
       "\n",
       "                                         Salary Data  \\\n",
       "0  {\"salary_low\":null,\"salary_high\":null,\"salary_...   \n",
       "1  {\"salary_low\":null,\"salary_high\":null,\"salary_...   \n",
       "2  {\"salary_low\":null,\"salary_high\":null,\"salary_...   \n",
       "3  {\"salary_low\":null,\"salary_high\":null,\"salary_...   \n",
       "4  {\"salary_low\":null,\"salary_high\":null,\"salary_...   \n",
       "\n",
       "               Contract Types Job Status Job Language Job Last Processed At  \\\n",
       "0                   full time     closed           en  2024-08-02T14:47:55Z   \n",
       "1  full time, internship, m/f     closed           en  2024-07-31T17:50:07Z   \n",
       "2                         NaN     closed           en  2024-05-18T02:32:04Z   \n",
       "3                   full time     closed           en  2024-08-23T00:33:30Z   \n",
       "4                   full time     closed           en  2024-08-02T19:03:16Z   \n",
       "\n",
       "   O*NET Code                       O*NET Family  \\\n",
       "0  43-1011.00  Office and Administrative Support   \n",
       "1  17-2061.00       Architecture and Engineering   \n",
       "2  51-9141.00                         Production   \n",
       "3  15-1252.00          Computer and Mathematical   \n",
       "4  41-9031.00                  Sales and Related   \n",
       "\n",
       "                               O*NET Occupation Name  \n",
       "0  First-Line Supervisors of Office and Administr...  \n",
       "1                        Computer Hardware Engineers  \n",
       "2               Semiconductor Processing Technicians  \n",
       "3                                Software Developers  \n",
       "4                                    Sales Engineers  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Job_Posting_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a940490-5577-4d27-9cca-ba524d8cc2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9919, 21)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Job_Posting_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a64ae62-99a2-4a02-8012-c53c42d3f8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9919 entries, 0 to 9918\n",
      "Data columns (total 21 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Website Domain         9919 non-null   object \n",
      " 1   Ticker                 0 non-null      float64\n",
      " 2   Job Opening Title      9919 non-null   object \n",
      " 3   Job Opening URL        9919 non-null   object \n",
      " 4   First Seen At          9919 non-null   object \n",
      " 5   Last Seen At           9919 non-null   object \n",
      " 6   Location               9508 non-null   object \n",
      " 7   Location Data          9919 non-null   object \n",
      " 8   Category               8250 non-null   object \n",
      " 9   Seniority              9919 non-null   object \n",
      " 10  Keywords               7646 non-null   object \n",
      " 11  Description            9807 non-null   object \n",
      " 12  Salary                 576 non-null    object \n",
      " 13  Salary Data            9919 non-null   object \n",
      " 14  Contract Types         8004 non-null   object \n",
      " 15  Job Status             6772 non-null   object \n",
      " 16  Job Language           9917 non-null   object \n",
      " 17  Job Last Processed At  9919 non-null   object \n",
      " 18  O*NET Code             9916 non-null   object \n",
      " 19  O*NET Family           9916 non-null   object \n",
      " 20  O*NET Occupation Name  9916 non-null   object \n",
      "dtypes: float64(1), object(20)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "Job_Posting_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b248fe00-95cb-402a-9c39-335dc569fd5f",
   "metadata": {},
   "source": [
    "- We observed that there were 21 columns present in the dataset and 9919 rows. We also observed that one column, **Ticker** was a null column which we later dropped while doing the data preparaton.\n",
    "- We then proceeded to doing EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8602c97-7c50-448d-a569-72ebb61d4372",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e94c98e-d863-4dbc-9b00-1c4ed26b06e7",
   "metadata": {},
   "source": [
    "- We started by doing an overview of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d2f786c-ddf4-4f7e-8be8-c69f9e77af36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATASET OVERVIEW\n",
      "======================================================================\n",
      "üìä Total Records: 9,919\n",
      "üìä Total Features: 21\n",
      "üìÖ Data loaded: 2026-02-12 14:03:02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üìä Total Records: {Job_Posting_df.shape[0]:,}\")\n",
    "print(f\"üìä Total Features: {Job_Posting_df.shape[1]}\")\n",
    "print(f\"üìÖ Data loaded: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ee188-f7e8-43ff-ab50-8b671a123a44",
   "metadata": {},
   "source": [
    "Columns Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14b9a8bd-04ec-4a83-a60e-9dcf85edd4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COLUMN SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Index | Column Name                    | Non-Null | Dtype\n",
      "------------------------------------------------------------\n",
      "    1 | Website Domain                 |   9,919 (100.0%) | object\n",
      "    2 | Ticker                         |       0 (  0.0%) | float64\n",
      "    3 | Job Opening Title              |   9,919 (100.0%) | object\n",
      "    4 | Job Opening URL                |   9,919 (100.0%) | object\n",
      "    5 | First Seen At                  |   9,919 (100.0%) | object\n",
      "    6 | Last Seen At                   |   9,919 (100.0%) | object\n",
      "    7 | Location                       |   9,508 ( 95.9%) | object\n",
      "    8 | Location Data                  |   9,919 (100.0%) | object\n",
      "    9 | Category                       |   8,250 ( 83.2%) | object\n",
      "   10 | Seniority                      |   9,919 (100.0%) | object\n",
      "   11 | Keywords                       |   7,646 ( 77.1%) | object\n",
      "   12 | Description                    |   9,807 ( 98.9%) | object\n",
      "   13 | Salary                         |     576 (  5.8%) | object\n",
      "   14 | Salary Data                    |   9,919 (100.0%) | object\n",
      "   15 | Contract Types                 |   8,004 ( 80.7%) | object\n",
      "   16 | Job Status                     |   6,772 ( 68.3%) | object\n",
      "   17 | Job Language                   |   9,917 (100.0%) | object\n",
      "   18 | Job Last Processed At          |   9,919 (100.0%) | object\n",
      "   19 | O*NET Code                     |   9,916 (100.0%) | object\n",
      "   20 | O*NET Family                   |   9,916 (100.0%) | object\n",
      "   21 | O*NET Occupation Name          |   9,916 (100.0%) | object\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COLUMN SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nIndex | Column Name                    | Non-Null | Dtype\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for i, col in enumerate(Job_Posting_df.columns, 1):\n",
    "    non_null = Job_Posting_df[col].notnull().sum()\n",
    "    percentage = (non_null / len(Job_Posting_df)) * 100\n",
    "    dtype = Job_Posting_df[col].dtype\n",
    "    print(f\"{i:5d} | {col:30} | {non_null:7,d} ({percentage:5.1f}%) | {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c575d054-0521-4973-b22e-73da22fd54d4",
   "metadata": {},
   "source": [
    "As you can see, our dataset contains 19 columns, one which contains numerical values and the other which are text columns. We will now proceed on data exploration and quality analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b152d-c3d2-4b45-a7ab-0b5e006eb49a",
   "metadata": {},
   "source": [
    " Data Exploration and Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b0d7978-5f76-4d22-8666-190e0d97871d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATASET OVERVIEW\n",
      "======================================================================\n",
      "üìä Total Records: 9,919\n",
      "üìä Total Features: 21\n",
      "üìÖ Data loaded: 2026-02-14 14:06:51\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üìä Total Records: {Job_Posting_df.shape[0]:,}\")\n",
    "print(f\"üìä Total Features: {Job_Posting_df.shape[1]}\")\n",
    "print(f\"üìÖ Data loaded: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91546a19-4536-48fd-958c-22eb75951971",
   "metadata": {},
   "source": [
    "- Let's now do a column summary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94cc04ce-2fa4-4064-acf3-3bf16b91ef6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COLUMN SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Index | Column Name                    | Non-Null | Dtype\n",
      "------------------------------------------------------------\n",
      "    1 | Website Domain                 |   9,919 (100.0%) | object\n",
      "    2 | Ticker                         |       0 (  0.0%) | float64\n",
      "    3 | Job Opening Title              |   9,919 (100.0%) | object\n",
      "    4 | Job Opening URL                |   9,919 (100.0%) | object\n",
      "    5 | First Seen At                  |   9,919 (100.0%) | object\n",
      "    6 | Last Seen At                   |   9,919 (100.0%) | object\n",
      "    7 | Location                       |   9,508 ( 95.9%) | object\n",
      "    8 | Location Data                  |   9,919 (100.0%) | object\n",
      "    9 | Category                       |   8,250 ( 83.2%) | object\n",
      "   10 | Seniority                      |   9,919 (100.0%) | object\n",
      "   11 | Keywords                       |   7,646 ( 77.1%) | object\n",
      "   12 | Description                    |   9,807 ( 98.9%) | object\n",
      "   13 | Salary                         |     576 (  5.8%) | object\n",
      "   14 | Salary Data                    |   9,919 (100.0%) | object\n",
      "   15 | Contract Types                 |   8,004 ( 80.7%) | object\n",
      "   16 | Job Status                     |   6,772 ( 68.3%) | object\n",
      "   17 | Job Language                   |   9,917 (100.0%) | object\n",
      "   18 | Job Last Processed At          |   9,919 (100.0%) | object\n",
      "   19 | O*NET Code                     |   9,916 (100.0%) | object\n",
      "   20 | O*NET Family                   |   9,916 (100.0%) | object\n",
      "   21 | O*NET Occupation Name          |   9,916 (100.0%) | object\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COLUMN SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nIndex | Column Name                    | Non-Null | Dtype\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for i, col in enumerate(Job_Posting_df.columns, 1):\n",
    "    non_null = Job_Posting_df[col].notnull().sum()\n",
    "    percentage = (non_null / len(Job_Posting_df)) * 100\n",
    "    dtype = Job_Posting_df[col].dtype\n",
    "    print(f\"{i:5d} | {col:30} | {non_null:7,d} ({percentage:5.1f}%) | {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003e1ff5-d344-4add-b519-6ffb627f1d30",
   "metadata": {},
   "source": [
    "- Now that we have done a summary of the columns, let's go ahead and have a look at the number of missing values, since as you can see, the summary we have done above shows us the percentage of non-null values in the respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c08cda0-0d16-412e-acaa-476662ef0864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MISSING VALUES ANALYSIS - TOP 10 WORST COLUMNS\n",
      "======================================================================\n",
      "        Column  Non-Null  Null Count     Null %   Dtype\n",
      "        Ticker         0        9919 100.000000 float64\n",
      "        Salary       576        9343  94.192963  object\n",
      "    Job Status      6772        3147  31.726989  object\n",
      "      Keywords      7646        2273  22.915616  object\n",
      "Contract Types      8004        1915  19.306382  object\n",
      "      Category      8250        1669  16.826293  object\n",
      "      Location      9508         411   4.143563  object\n",
      "   Description      9807         112   1.129146  object\n",
      "  O*NET Family      9916           3   0.030245  object\n",
      "    O*NET Code      9916           3   0.030245  object\n"
     ]
    }
   ],
   "source": [
    "# Missing Values Analysis\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MISSING VALUES ANALYSIS - TOP 10 WORST COLUMNS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate missing values\n",
    "missing_data = []\n",
    "for col in Job_Posting_df.columns:\n",
    "    non_null = Job_Posting_df[col].notnull().sum()\n",
    "    null_count = Job_Posting_df[col].isnull().sum()\n",
    "    null_pct = (null_count / len(Job_Posting_df)) * 100\n",
    "    missing_data.append({\n",
    "        'Column': col,\n",
    "        'Non-Null': non_null,\n",
    "        'Null Count': null_count,\n",
    "        'Null %': null_pct,\n",
    "        'Dtype': Job_Posting_df[col].dtype\n",
    "    })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_data)\n",
    "missing_df = missing_df.sort_values('Null %', ascending=False)\n",
    "\n",
    "# Display top 10\n",
    "print(missing_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c58c035-e3dd-4123-beac-00711c22ad58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MISSING DATA CATEGORIZATION\n",
      "======================================================================\n",
      "\n",
      "Complete (0%)                 :  9 columns\n",
      "   Seniority, Location Data, Salary Data, ... and 6 more\n",
      "\n",
      "Good (<5%)                    :  6 columns\n",
      "   Location, Description, O*NET Family, ... and 3 more\n",
      "\n",
      "High (20-50%)                 :  2 columns\n",
      "   Job Status, Keywords\n",
      "\n",
      "Moderate (5-20%)              :  2 columns\n",
      "   Contract Types, Category\n",
      "\n",
      "Completely Missing (100%)     :  1 columns\n",
      "   Ticker\n",
      "\n",
      "Very High (50-99%)            :  1 columns\n",
      "   Salary\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MISSING DATA CATEGORIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Categorize columns by missing percentage\n",
    "def categorize_missing(pct):\n",
    "    if pct == 0:\n",
    "        return 'Complete (0%)'\n",
    "    elif pct < 5:\n",
    "        return 'Good (<5%)'\n",
    "    elif pct < 20:\n",
    "        return 'Moderate (5-20%)'\n",
    "    elif pct < 50:\n",
    "        return 'High (20-50%)'\n",
    "    elif pct < 100:\n",
    "        return 'Very High (50-99%)'\n",
    "    else:\n",
    "        return 'Completely Missing (100%)'\n",
    "\n",
    "missing_df['Category'] = missing_df['Null %'].apply(categorize_missing)\n",
    "category_counts = missing_df['Category'].value_counts()\n",
    "\n",
    "for category, count in category_counts.items():\n",
    "    cols_in_category = missing_df[missing_df['Category'] == category]['Column'].tolist()\n",
    "    print(f\"\\n{category:30}: {count:2d} columns\")\n",
    "    if len(cols_in_category) <= 5:\n",
    "        print(f\"   {', '.join(cols_in_category)}\")\n",
    "    else:\n",
    "        print(f\"   {', '.join(cols_in_category[:3])}, ... and {len(cols_in_category)-3} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3782ca7a-be01-45f0-8c4b-7d8f8830c5eb",
   "metadata": {},
   "source": [
    "- Now that we have an idea of the missing values and their percentages in the dataset, we can now see that the columns, **Ticker** and **Salary**, can be dropped from our dataset. But instead of going with this approach of dropping columns, let's do a critical column analysis to determine which columns are the most important for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56a9cdb5-166b-4ee3-855d-3997c09bfff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CRITICAL COLUMNS ASSESSMENT\n",
      "======================================================================\n",
      "\n",
      "Column                  | Non-Null |   %   | Status\n",
      "------------------------------------------------------------\n",
      "Job Opening Title       |    9,919 | 100.0% | ‚úÖ Excellent\n",
      "                      Primary identifier - ESSENTIAL\n",
      "Description             |    9,807 |  98.9% | ‚úÖ Excellent\n",
      "                      Contains skills/requirements - ESSENTIAL\n",
      "Category                |    8,250 |  83.2% | ‚ö†Ô∏è  Acceptable\n",
      "                      Job classification - IMPORTANT\n",
      "Location                |    9,508 |  95.9% | ‚úÖ Excellent\n",
      "                      Geographic info - IMPORTANT\n",
      "Seniority               |    9,919 | 100.0% | ‚úÖ Excellent\n",
      "                      Experience level - IMPORTANT\n",
      "Salary                  |      576 |   5.8% | ‚ùå Critical Issue\n",
      "                      Compensation - DESIRABLE but limited\n",
      "Contract Types          |    8,004 |  80.7% | ‚ö†Ô∏è  Acceptable\n",
      "                      Job type - DESIRABLE\n",
      "Job Status              |    6,772 |  68.3% | üî∂ Concerning\n",
      "                      Open/Closed status - DESIRABLE\n"
     ]
    }
   ],
   "source": [
    "# 2.3 Critical Column Assessment\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CRITICAL COLUMNS ASSESSMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "critical_columns = {\n",
    "    'Job Opening Title': 'Primary identifier - ESSENTIAL',\n",
    "    'Description': 'Contains skills/requirements - ESSENTIAL',\n",
    "    'Category': 'Job classification - IMPORTANT',\n",
    "    'Location': 'Geographic info - IMPORTANT',\n",
    "    'Seniority': 'Experience level - IMPORTANT',\n",
    "    'Salary': 'Compensation - DESIRABLE but limited',\n",
    "    'Contract Types': 'Job type - DESIRABLE',\n",
    "    'Job Status': 'Open/Closed status - DESIRABLE'\n",
    "}\n",
    "\n",
    "print(\"\\nColumn                  | Non-Null |   %   | Status\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for col, importance in critical_columns.items():\n",
    "    if col in Job_Posting_df.columns:\n",
    "        non_null = Job_Posting_df[col].notnull().sum()\n",
    "        pct = (non_null / len(Job_Posting_df)) * 100\n",
    "        \n",
    "        if pct > 90:\n",
    "            status = \"‚úÖ Excellent\"\n",
    "        elif pct > 70:\n",
    "            status = \"‚ö†Ô∏è  Acceptable\"\n",
    "        elif pct > 50:\n",
    "            status = \"üî∂ Concerning\"\n",
    "        else:\n",
    "            status = \"‚ùå Critical Issue\"\n",
    "        \n",
    "        print(f\"{col:23} | {non_null:8,d} | {pct:5.1f}% | {status}\")\n",
    "        print(f\"                      {importance}\")\n",
    "    else:\n",
    "        print(f\"{col:23} | {'NOT FOUND':^8} | {'N/A':^5} | ‚ùå Missing Column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dd6f94-717b-4db7-ac3e-d473aa47b7a6",
   "metadata": {},
   "source": [
    "- We can now see the most important columns which are desirable for our project and therefore we will go with this columns. Since most of our columns are text-based columns and they are categorical, we will have to develop key statistics which we will set for our categorical columns so that we can proceed with our data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ee44b1a-fd10-4825-af90-57d746b82274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CATEGORICAL COLUMNS ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "üìä Category:\n",
      "----------------------------------------\n",
      "Non-null values: 8,250/9,919 (83.2%)\n",
      "Unique values: 509\n",
      "\n",
      "Top 10 values:\n",
      "  NaN: 1,669 ( 16.8%)\n",
      "  engineering                                       :   986 (  9.9%)\n",
      "  management                                        :   603 (  6.1%)\n",
      "  internship                                        :   598 (  6.0%)\n",
      "  manual_work                                       :   273 (  2.8%)\n",
      "  software_development                              :   271 (  2.7%)\n",
      "  engineering, quality_assurance                    :   185 (  1.9%)\n",
      "  purchasing                                        :   182 (  1.8%)\n",
      "  engineering, information_technology               :   177 (  1.8%)\n",
      "  engineering, software_development                 :   171 (  1.7%)\n",
      "\n",
      "üìä Seniority:\n",
      "----------------------------------------\n",
      "Non-null values: 9,919/9,919 (100.0%)\n",
      "Unique values: 8\n",
      "\n",
      "Top 10 values:\n",
      "  non_manager                                       : 7,981 ( 80.5%)\n",
      "  manager                                           : 1,809 ( 18.2%)\n",
      "  head                                              :    75 (  0.8%)\n",
      "  director                                          :    33 (  0.3%)\n",
      "  c_level                                           :    13 (  0.1%)\n",
      "  vice_president                                    :     4 (  0.0%)\n",
      "  partner                                           :     3 (  0.0%)\n",
      "  president                                         :     1 (  0.0%)\n",
      "\n",
      "üìä Job Status:\n",
      "----------------------------------------\n",
      "Non-null values: 6,772/9,919 (68.3%)\n",
      "Unique values: 1\n",
      "\n",
      "Top 10 values:\n",
      "  closed                                            : 6,772 ( 68.3%)\n",
      "  NaN: 3,147 ( 31.7%)\n",
      "\n",
      "üìä Job Language:\n",
      "----------------------------------------\n",
      "Non-null values: 9,917/9,919 (100.0%)\n",
      "Unique values: 23\n",
      "\n",
      "Top 10 values:\n",
      "  en                                                : 7,150 ( 72.1%)\n",
      "  de                                                : 1,248 ( 12.6%)\n",
      "  pt                                                :   543 (  5.5%)\n",
      "  es                                                :   276 (  2.8%)\n",
      "  fr                                                :   174 (  1.8%)\n",
      "  pl                                                :   115 (  1.2%)\n",
      "  cs                                                :    96 (  1.0%)\n",
      "  nl                                                :    88 (  0.9%)\n",
      "  sl                                                :    68 (  0.7%)\n",
      "  hu                                                :    47 (  0.5%)\n",
      "\n",
      "üìä Contract Types:\n",
      "----------------------------------------\n",
      "Non-null values: 8,004/9,919 (80.7%)\n",
      "Unique values: 674\n",
      "\n",
      "Top 10 values:\n",
      "  full time                                         : 3,170 ( 32.0%)\n",
      "  NaN: 1,915 ( 19.3%)\n",
      "  m/f                                               :   521 (  5.3%)\n",
      "  m/w                                               :   520 (  5.2%)\n",
      "  intern                                            :   257 (  2.6%)\n",
      "  vollzeit                                          :   214 (  2.2%)\n",
      "  part time                                         :   160 (  1.6%)\n",
      "  tempo integral                                    :   146 (  1.5%)\n",
      "  full time, hybrid                                 :   128 (  1.3%)\n",
      "  hybrid, full time                                 :   123 (  1.2%)\n"
     ]
    }
   ],
   "source": [
    "## 2.4 Key Statistics for Numeric/Categorical Columns\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CATEGORICAL COLUMNS ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "categorical_cols = ['Category', 'Seniority', 'Job Status', 'Job Language', 'Contract Types']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in Job_Posting_df.columns and Job_Posting_df[col].notnull().sum() > 0:\n",
    "        print(f\"\\nüìä {col}:\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        # Count unique values\n",
    "        unique_count = Job_Posting_df[col].nunique()\n",
    "        non_null = Job_Posting_df[col].notnull().sum()\n",
    "        \n",
    "        print(f\"Non-null values: {non_null:,}/{len(Job_Posting_df):,} ({(non_null/len(Job_Posting_df))*100:.1f}%)\")\n",
    "        print(f\"Unique values: {unique_count}\")\n",
    "        \n",
    "        # Show top values\n",
    "        value_counts = Job_Posting_df[col].value_counts(dropna=False).head(10)\n",
    "        print(\"\\nTop 10 values:\")\n",
    "        for value, count in value_counts.items():\n",
    "            pct = (count / len(Job_Posting_df)) * 100\n",
    "            if pd.isna(value):\n",
    "                print(f\"  NaN: {count:5,d} ({pct:5.1f}%)\")\n",
    "            else:\n",
    "                # Truncate long values\n",
    "                display_value = str(value)[:50] + \"...\" if len(str(value)) > 50 else str(value)\n",
    "                print(f\"  {display_value:50}: {count:5,d} ({pct:5.1f}%)\")\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af27df-7dce-431a-98e9-9eec20faf18e",
   "metadata": {},
   "source": [
    "- From this analysis, we can see that for the six categorical columns; i.e. , **Category**, **Seniority**, **Job Status**, **Job Language** and **Contract Types**, we have the various top values for each of these respective columns which shows us the Job Posting behaviour and nature at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a4d272-b8ee-441e-9744-83546baaa691",
   "metadata": {},
   "source": [
    "- Our dataset also happens to contain some columns which contains data in JSON format; i.e., ***Location Data*** and ***Salary Data***, hence the need to import the ***json*** library. Let's do a preview of the JSON columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3092b6b-2250-4481-8bc8-e4ec5cf2f6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "JSON COLUMNS ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "üîç Location Data:\n",
      "----------------------------------------\n",
      "Non-null values: 9,919/9,919 (100.0%)\n",
      "\n",
      "Sample JSON structures:\n",
      "\n",
      "Sample 1:\n",
      "  Type: List with 1 items\n",
      "  Keys in first item: ['city', 'state', 'zip_code', 'country', 'region', 'continent', 'fuzzy_match']\n",
      "\n",
      "Sample 2:\n",
      "  Type: List with 1 items\n",
      "  Keys in first item: ['city', 'state', 'zip_code', 'country', 'region', 'continent', 'fuzzy_match']\n",
      "\n",
      "Sample 3:\n",
      "  Type: List with 1 items\n",
      "  Keys in first item: ['city', 'state', 'zip_code', 'country', 'region', 'continent', 'fuzzy_match']\n",
      "\n",
      "üîç Salary Data:\n",
      "----------------------------------------\n",
      "Non-null values: 9,919/9,919 (100.0%)\n",
      "\n",
      "Sample JSON structures:\n",
      "\n",
      "Sample 1:\n",
      "  Type: Dictionary\n",
      "  Keys: ['salary_low', 'salary_high', 'salary_currency', 'salary_low_usd', 'salary_high_usd', 'salary_time_unit']\n",
      "    salary_low: None\n",
      "    salary_high: None\n",
      "    salary_currency: None\n",
      "\n",
      "Sample 2:\n",
      "  Type: Dictionary\n",
      "  Keys: ['salary_low', 'salary_high', 'salary_currency', 'salary_low_usd', 'salary_high_usd', 'salary_time_unit']\n",
      "    salary_low: None\n",
      "    salary_high: None\n",
      "    salary_currency: None\n",
      "\n",
      "Sample 3:\n",
      "  Type: Dictionary\n",
      "  Keys: ['salary_low', 'salary_high', 'salary_currency', 'salary_low_usd', 'salary_high_usd', 'salary_time_unit']\n",
      "    salary_low: None\n",
      "    salary_high: None\n",
      "    salary_currency: None\n"
     ]
    }
   ],
   "source": [
    "# 2.5 JSON Columns Preview\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"JSON COLUMNS ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "json_columns = ['Location Data', 'Salary Data']\n",
    "\n",
    "for json_col in json_columns:\n",
    "    if json_col in Job_Posting_df.columns:\n",
    "        print(f\"\\nüîç {json_col}:\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        non_null_count = Job_Posting_df[json_col].notnull().sum()\n",
    "        print(f\"Non-null values: {non_null_count:,}/{len(Job_Posting_df):,} ({(non_null_count/len(Job_Posting_df))*100:.1f}%)\")\n",
    "        \n",
    "        # Sample and parse JSON\n",
    "        samples = Job_Posting_df[json_col].dropna().head(3)\n",
    "        if len(samples) > 0:\n",
    "            print(\"\\nSample JSON structures:\")\n",
    "            for i, sample in enumerate(samples, 1):\n",
    "                try:\n",
    "                    if isinstance(sample, str) and sample.strip():\n",
    "                        parsed = json.loads(sample)\n",
    "                        print(f\"\\nSample {i}:\")\n",
    "                        if isinstance(parsed, list):\n",
    "                            print(f\"  Type: List with {len(parsed)} items\")\n",
    "                            if parsed and isinstance(parsed[0], dict):\n",
    "                                print(f\"  Keys in first item: {list(parsed[0].keys())}\")\n",
    "                        elif isinstance(parsed, dict):\n",
    "                            print(f\"  Type: Dictionary\")\n",
    "                            print(f\"  Keys: {list(parsed.keys())}\")\n",
    "                            # Show first few key-value pairs\n",
    "                            for key, value in list(parsed.items())[:3]:\n",
    "                                print(f\"    {key}: {str(value)[:50]}{'...' if len(str(value)) > 50 else ''}\")\n",
    "                    else:\n",
    "                        print(f\"Sample {i}: Empty or non-string value\")\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Sample {i}: Invalid JSON - {str(e)[:50]}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Sample {i}: Error - {type(e).__name__}: {str(e)[:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d0919e-29b0-4dea-a48e-ca2c35943546",
   "metadata": {},
   "source": [
    "- The piece of code above was to identify the JSON columns so that we identify the various values and their categorical importance to the project and also identify the need to parse the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1eb70b-b013-4bb1-b695-a772d5c7a2d0",
   "metadata": {},
   "source": [
    "- Now lets check through the text columns and the date columns;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78162c51-4bb1-4805-9bac-bdb47a353ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEXT COLUMNS PREVIEW\n",
      "======================================================================\n",
      "\n",
      "üìù Job Opening Title:\n",
      "----------------------------------------\n",
      "Non-null: 9,919/9,919 (100.0%)\n",
      "Average length: 37 characters\n",
      "Min length: 3 characters\n",
      "Max length: 117 characters\n",
      "\n",
      "Sample entries:\n",
      "\n",
      "1. IN_RBAI_Assistant Manager_Dispensing Process Engineer_IN\n",
      "\n",
      "2. Professional Internship: Hardware Development (M/F/Div.)\n",
      "\n",
      "3. Process Expert BMS Production\n",
      "\n",
      "üìù Description:\n",
      "----------------------------------------\n",
      "Non-null: 9,807/9,919 (98.9%)\n",
      "Average length: 3401 characters\n",
      "Min length: 165 characters\n",
      "Max length: 8162 characters\n",
      "\n",
      "Sample entries:\n",
      "\n",
      "1. **IN\\_RBAI\\_Assistant Manager\\_Dispensing Process Engineer\\_IN**      * Full-time  * Legal Entity: Bosch Automotive Electronics India Private Ltd.    ...\n",
      "\n",
      "2. **Professional Internship: Hardware Development (M/F/Div.)**      * Full-time  * Legal Entity: Home Comfort      **Company Description**    The Bosch ...\n",
      "\n",
      "3. ZF is a global technology company supplying systems for passenger cars, commercial vehicles and industrial technology, enabling the next generation of...\n"
     ]
    }
   ],
   "source": [
    "# 2.6 Text Columns Preview\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEXT COLUMNS PREVIEW\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "text_columns = ['Job Opening Title', 'Description']\n",
    "\n",
    "for col in text_columns:\n",
    "    if col in Job_Posting_df.columns:\n",
    "        print(f\"\\nüìù {col}:\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        non_null = Job_Posting_df[col].notnull().sum()\n",
    "        print(f\"Non-null: {non_null:,}/{len(Job_Posting_df):,} ({(non_null/len(Job_Posting_df))*100:.1f}%)\")\n",
    "        \n",
    "        # Show character statistics\n",
    "        if non_null > 0:\n",
    "            text_lengths = Job_Posting_df[col].dropna().apply(len)\n",
    "            print(f\"Average length: {text_lengths.mean():.0f} characters\")\n",
    "            print(f\"Min length: {text_lengths.min():.0f} characters\")\n",
    "            print(f\"Max length: {text_lengths.max():.0f} characters\")\n",
    "            \n",
    "            print(\"\\nSample entries:\")\n",
    "            samples = Job_Posting_df[col].dropna().head(3)\n",
    "            for i, sample in enumerate(samples, 1):\n",
    "                # Clean and truncate for display\n",
    "                clean_sample = str(sample).replace('\\n', ' ').replace('\\r', ' ')\n",
    "                if len(clean_sample) > 150:\n",
    "                    display_text = clean_sample[:150] + \"...\"\n",
    "                else:\n",
    "                    display_text = clean_sample\n",
    "                print(f\"\\n{i}. {display_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c172fa31-c616-47eb-aac6-0d5600b1308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATE COLUMNS ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "üìÖ First Seen At:\n",
      "----------------------------------------\n",
      "Format appears to be: ISO 8601 (e.g., 2024-05-29T19:59:45Z)\n",
      "Valid dates: 9,919/9,919 (100.0%)\n",
      "Date range: 2024-03-04 15:41:37+00:00 to 2024-09-04 07:03:16+00:00\n",
      "Time span: 183 days\n",
      "\n",
      "üìÖ Last Seen At:\n",
      "----------------------------------------\n",
      "Format appears to be: ISO 8601 (e.g., 2024-05-29T19:59:45Z)\n",
      "Valid dates: 9,919/9,919 (100.0%)\n",
      "Date range: 2024-03-06 16:31:21+00:00 to 2024-09-04 09:43:42+00:00\n",
      "Time span: 181 days\n",
      "\n",
      "üìÖ Job Last Processed At:\n",
      "----------------------------------------\n",
      "Format appears to be: ISO 8601 (e.g., 2024-05-29T19:59:45Z)\n",
      "Valid dates: 9,919/9,919 (100.0%)\n",
      "Date range: 2024-02-22 16:38:29+00:00 to 2024-09-04 09:43:42+00:00\n",
      "Time span: 194 days\n"
     ]
    }
   ],
   "source": [
    "# 2.7 Date Columns Analysis\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATE COLUMNS ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "date_columns = ['First Seen At', 'Last Seen At', 'Job Last Processed At']\n",
    "\n",
    "for col in date_columns:\n",
    "    if col in Job_Posting_df.columns:\n",
    "        print(f\"\\nüìÖ {col}:\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        # Check if already datetime\n",
    "        if Job_Posting_df[col].dtype == 'object':\n",
    "            # Try to convert\n",
    "            try:\n",
    "                temp_dates = pd.to_datetime(Job_Posting_df[col], errors='coerce')\n",
    "                valid_dates = temp_dates.notnull().sum()\n",
    "                print(f\"Format appears to be: ISO 8601 (e.g., 2024-05-29T19:59:45Z)\")\n",
    "                print(f\"Valid dates: {valid_dates:,}/{len(Job_Posting_df):,} ({(valid_dates/len(Job_Posting_df))*100:.1f}%)\")\n",
    "                \n",
    "                if valid_dates > 0:\n",
    "                    print(f\"Date range: {temp_dates.min()} to {temp_dates.max()}\")\n",
    "                    duration_days = (temp_dates.max() - temp_dates.min()).days\n",
    "                    print(f\"Time span: {duration_days} days\")\n",
    "            except Exception as e:\n",
    "                print(f\"Conversion error: {str(e)[:50]}\")\n",
    "        else:\n",
    "            print(f\"Already datetime type\")\n",
    "            print(f\"Date range: {Job_Posting_df[col].min()} to {Job_Posting_df[col].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4313c8-7233-485c-8d1e-756593e572d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "- The code above show that the date and time columns for our dataset are good to go so we can now do a complete summary of the data quality of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb0d905e-0ac4-4958-a735-00b4769d45fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATA QUALITY ISSUES SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Identified Issues:\n",
      "--------------------------------------------------\n",
      " 1. ‚ùå Ticker column           - 100% missing - consider dropping\n",
      " 2. üî∂ Job Status              - 31.7% missing - may affect analysis\n",
      " 3. üîç Location Data           - Requires JSON parsing for detailed location info\n",
      " 4. üîç Salary Data             - Requires JSON parsing for structured salary info\n"
     ]
    }
   ],
   "source": [
    "# ## 2.8 Data Quality Issues Summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA QUALITY ISSUES SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "issues = []\n",
    "\n",
    "# Issue 1: Completely missing column\n",
    "if 'Ticker' in Job_Posting_df.columns and Job_Posting_df['Ticker'].isnull().all():\n",
    "    issues.append((\"‚ùå Ticker column\", \"100% missing - consider dropping\"))\n",
    "\n",
    "# Issue 2: Very high missing values\n",
    "high_missing_cols = missing_df[missing_df['Null %'] > 50]['Column'].tolist()\n",
    "for col in high_missing_cols:\n",
    "    if col != 'Ticker' and col != 'Salary':  # Salary we already know about\n",
    "        pct = missing_df[missing_df['Column'] == col]['Null %'].iloc[0]\n",
    "        issues.append((f\"‚ö†Ô∏è  {col}\", f\"{pct:.1f}% missing\"))\n",
    "\n",
    "# Issue 3: Critical columns with significant missing data\n",
    "for col in ['Category', 'Job Status']:\n",
    "    if col in Job_Posting_df.columns:\n",
    "        null_pct = (Job_Posting_df[col].isnull().sum() / len(Job_Posting_df)) * 100\n",
    "        if null_pct > 20:\n",
    "            issues.append((f\"üî∂ {col}\", f\"{null_pct:.1f}% missing - may affect analysis\"))\n",
    "\n",
    "# Issue 4: JSON parsing complexity\n",
    "issues.append((\"üîç Location Data\", \"Requires JSON parsing for detailed location info\"))\n",
    "issues.append((\"üîç Salary Data\", \"Requires JSON parsing for structured salary info\"))\n",
    "\n",
    "print(\"\\nIdentified Issues:\")\n",
    "print(\"-\"*50)\n",
    "if issues:\n",
    "    for i, (issue, detail) in enumerate(issues, 1):\n",
    "        print(f\"{i:2}. {issue:25} - {detail}\")\n",
    "else:\n",
    "    print(\"No major issues identified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d154835-eb7a-474e-9d9b-5b882308e04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RECOMMENDED NEXT STEPS\n",
      "======================================================================\n",
      "\n",
      "üõ†Ô∏è  Data Cleaning Priority:\n",
      "--------------------------------------------------\n",
      "1. Drop completely empty columns\n",
      "   ‚Üí Ticker column (0 non-null values)\n",
      "\n",
      "2. Parse JSON columns\n",
      "   ‚Üí Extract city, state, country from Location Data; salary details from Salary Data\n",
      "\n",
      "3. Convert date columns\n",
      "   ‚Üí Convert First Seen At, Last Seen At to datetime format\n",
      "\n",
      "4. Handle missing Category data\n",
      "   ‚Üí Consider imputation or separate 'unknown' category\n",
      "\n",
      "5. Analyze text columns\n",
      "   ‚Üí Extract skills from Description using NLP\n",
      "\n",
      "6. Clean categorical columns\n",
      "   ‚Üí Standardize values in Category, Seniority, Contract Types\n",
      "\n",
      "7. Calculate posting duration\n",
      "   ‚Üí Create new feature: Last Seen At - First Seen At\n",
      "\n",
      "8. Explore Salary Data\n",
      "   ‚Üí Extract and analyze available salary information\n",
      "\n",
      "\n",
      "======================================================================\n",
      "PROJECT STATUS UPDATE\n",
      "======================================================================\n",
      "‚úÖ Dataset loaded successfully: 9,919 job postings\n",
      "‚úÖ Critical columns identified and assessed\n",
      "‚úÖ Data quality issues documented\n",
      "‚úÖ Next steps outlined for cleaning and preparation\n",
      "\n",
      "üìã Ready for Step 3: Data Cleaning and Preparation\n"
     ]
    }
   ],
   "source": [
    "# 2.9 Recommendations for Next Steps\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDED NEXT STEPS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "next_steps = [\n",
    "    (\"1\", \"Drop completely empty columns\", \"Ticker column (0 non-null values)\"),\n",
    "    (\"2\", \"Parse JSON columns\", \"Extract city, state, country from Location Data; salary details from Salary Data\"),\n",
    "    (\"3\", \"Convert date columns\", \"Convert First Seen At, Last Seen At to datetime format\"),\n",
    "    (\"4\", \"Handle missing Category data\", \"Consider imputation or separate 'unknown' category\"),\n",
    "    (\"5\", \"Analyze text columns\", \"Extract skills from Description using NLP\"),\n",
    "    (\"6\", \"Clean categorical columns\", \"Standardize values in Category, Seniority, Contract Types\"),\n",
    "    (\"7\", \"Calculate posting duration\", \"Create new feature: Last Seen At - First Seen At\"),\n",
    "    (\"8\", \"Explore Salary Data\", \"Extract and analyze available salary information\"),\n",
    "]\n",
    "\n",
    "print(\"\\nüõ†Ô∏è  Data Cleaning Priority:\")\n",
    "print(\"-\"*50)\n",
    "for num, step, details in next_steps:\n",
    "    print(f\"{num}. {step}\")\n",
    "    print(f\"   ‚Üí {details}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROJECT STATUS UPDATE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"‚úÖ Dataset loaded successfully: {Job_Posting_df.shape[0]:,} job postings\")\n",
    "print(f\"‚úÖ Critical columns identified and assessed\")\n",
    "print(f\"‚úÖ Data quality issues documented\")\n",
    "print(f\"‚úÖ Next steps outlined for cleaning and preparation\")\n",
    "print(f\"\\nüìã Ready for Step 3: Data Cleaning and Preparation\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39c79823-299e-4d8b-b4c9-39847a60e287",
   "metadata": {},
   "source": [
    "- Since we have done a thorough EDA we can now proceed to **Data Cleaning and Preparation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d717c0-e113-45b6-b5ec-7cc26733522e",
   "metadata": {},
   "source": [
    " 3. Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438ce8b1-f56b-4344-b9fd-bb6f5bf557bd",
   "metadata": {},
   "source": [
    "From our observaions, we noted that there were issues we needed to tackle so as to get the data ready for modelling. We decided to tackle the issues in this order;\n",
    "- Drop completely empty columns\n",
    "\n",
    "- Parse JSON columns (Location and Salary Data)\n",
    "\n",
    "- Handle missing values\n",
    "\n",
    "- Convert date columns\n",
    "\n",
    "- Clean categorical/text data\n",
    "\n",
    "- Create new features for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de82de1b-6377-4369-917b-48eed127cf70",
   "metadata": {},
   "source": [
    "3.1 Initial Setup and Column removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "961ad919-2fad-4a53-ba95-297a243c2a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (9919, 21)\n",
      "\n",
      "======================================================================\n",
      "3.1 DROP COMPLETELY EMPTY COLUMNS\n",
      "======================================================================\n",
      "‚úÖ Dropped 'Ticker' column (100% missing)\n",
      "New shape: (9919, 20)\n",
      "Columns remaining: 20\n"
     ]
    }
   ],
   "source": [
    " #Make a copy for cleaning\n",
    "Job_Posting_clean = Job_Posting_df.copy()\n",
    "print(\"Initial shape:\", Job_Posting_clean.shape)\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"3.1 DROP COMPLETELY EMPTY COLUMNS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Drop Ticker column (100% missing)\n",
    "if 'Ticker' in Job_Posting_clean.columns:\n",
    "    Job_Posting_clean = Job_Posting_clean.drop(columns=['Ticker'])\n",
    "    print(\"‚úÖ Dropped 'Ticker' column (100% missing)\")\n",
    "\n",
    "print(f\"New shape: {Job_Posting_clean.shape}\")\n",
    "print(f\"Columns remaining: {len(Job_Posting_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff06d62-3715-4f0e-9691-e38e7bc48152",
   "metadata": {},
   "source": [
    " 3.2 Parsing JSON columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "554953dc-fa0f-44b3-8f7e-71311b23e196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "3.2 PARSE LOCATION DATA COLUMN\n",
      "======================================================================\n",
      "‚úÖ Extracted location fields from Location Data:\n",
      "   - city: 6,281 non-null\n",
      "   - state: 2,450 non-null\n",
      "   - country: 9,449 non-null\n",
      "   - region: 21 non-null\n",
      "   - continent: 33 non-null\n",
      "\n",
      "üìä Sample extracted location data:\n",
      "Original Location: Indiana, United States\n",
      "Parsed - City: None\n",
      "Parsed - State: Indiana\n",
      "Parsed - Country: United States\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"3.2 PARSE LOCATION DATA COLUMN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def parse_location_data(json_str):\n",
    "    \"\"\"Parse Location Data JSON and extract key fields\"\"\"\n",
    "    try:\n",
    "        if pd.isna(json_str) or json_str == '':\n",
    "            return None, None, None, None, None\n",
    "        \n",
    "        data = json.loads(json_str)\n",
    "        if isinstance(data, list) and len(data) > 0:\n",
    "            location = data[0]\n",
    "            return (\n",
    "                location.get('city'),\n",
    "                location.get('state'),\n",
    "                location.get('country'),\n",
    "                location.get('region'),\n",
    "                location.get('continent')\n",
    "            )\n",
    "    except (json.JSONDecodeError, TypeError, KeyError) as e:\n",
    "        pass\n",
    "    return None, None, None, None, None\n",
    "\n",
    "# Apply parsing\n",
    "location_parsed = Job_Posting_clean['Location Data'].apply(parse_location_data)\n",
    "Job_Posting_clean[['city', 'state', 'country', 'region', 'continent']] = pd.DataFrame(\n",
    "    location_parsed.tolist(), index=Job_Posting_clean.index\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Extracted location fields from Location Data:\")\n",
    "print(f\"   - city: {Job_Posting_clean['city'].notnull().sum():,} non-null\")\n",
    "print(f\"   - state: {Job_Posting_clean['state'].notnull().sum():,} non-null\")\n",
    "print(f\"   - country: {Job_Posting_clean['country'].notnull().sum():,} non-null\")\n",
    "print(f\"   - region: {Job_Posting_clean['region'].notnull().sum():,} non-null\")\n",
    "print(f\"   - continent: {Job_Posting_clean['continent'].notnull().sum():,} non-null\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nüìä Sample extracted location data:\")\n",
    "sample_idx = Job_Posting_clean[Job_Posting_clean['country'].notnull()].index[0]\n",
    "print(f\"Original Location: {Job_Posting_clean.loc[sample_idx, 'Location']}\")\n",
    "print(f\"Parsed - City: {Job_Posting_clean.loc[sample_idx, 'city']}\")\n",
    "print(f\"Parsed - State: {Job_Posting_clean.loc[sample_idx, 'state']}\")\n",
    "print(f\"Parsed - Country: {Job_Posting_clean.loc[sample_idx, 'country']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92a558e2-151a-485e-8edf-417b980b1742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "3.3 PARSE SALARY DATA COLUMN\n",
      "======================================================================\n",
      "‚úÖ Extracted salary fields from Salary Data:\n",
      "   - salary_low          :    434 non-null (4.4%)\n",
      "   - salary_high         :    434 non-null (4.4%)\n",
      "   - salary_currency     :    434 non-null (4.4%)\n",
      "   - salary_low_usd      :    434 non-null (4.4%)\n",
      "   - salary_high_usd     :    434 non-null (4.4%)\n",
      "   - salary_time_unit    :    434 non-null (4.4%)\n",
      "\n",
      "üîç Salary data availability: ‚úÖ Yes\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"3.3 PARSE SALARY DATA COLUMN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def parse_salary_data(json_str):\n",
    "    \"\"\"Parse Salary Data JSON and extract key fields\"\"\"\n",
    "    try:\n",
    "        if pd.isna(json_str) or json_str == '':\n",
    "            return None, None, None, None, None, None\n",
    "        \n",
    "        data = json.loads(json_str)\n",
    "        return (\n",
    "            data.get('salary_low'),\n",
    "            data.get('salary_high'),\n",
    "            data.get('salary_currency'),\n",
    "            data.get('salary_low_usd'),\n",
    "            data.get('salary_high_usd'),\n",
    "            data.get('salary_time_unit')\n",
    "        )\n",
    "    except (json.JSONDecodeError, TypeError, KeyError) as e:\n",
    "        pass\n",
    "    return None, None, None, None, None, None\n",
    "\n",
    "# Apply parsing\n",
    "salary_parsed = Job_Posting_clean['Salary Data'].apply(parse_salary_data)\n",
    "Job_Posting_clean[['salary_low', 'salary_high', 'salary_currency', \n",
    "          'salary_low_usd', 'salary_high_usd', 'salary_time_unit']] = pd.DataFrame(\n",
    "    salary_parsed.tolist(), index=Job_Posting_clean.index\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Extracted salary fields from Salary Data:\")\n",
    "salary_fields = ['salary_low', 'salary_high', 'salary_currency', \n",
    "                 'salary_low_usd', 'salary_high_usd', 'salary_time_unit']\n",
    "for field in salary_fields:\n",
    "    non_null = Job_Posting_clean[field].notnull().sum()\n",
    "    print(f\"   - {field:20}: {non_null:6,} non-null ({non_null/len(Job_Posting_clean)*100:.1f}%)\")\n",
    "\n",
    "# Check if we have any actual salary data\n",
    "has_salary_data = Job_Posting_clean['salary_low'].notnull().sum() > 0\n",
    "print(f\"\\nüîç Salary data availability: {'‚úÖ Yes' if has_salary_data else '‚ùå No actual salary values found'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef52119-8b5e-4877-a310-9b2b365147a8",
   "metadata": {},
   "source": [
    " 3.4 Converting Date Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ab2a422-6e32-4bb3-a629-b0a11f7d1c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "3.4 CONVERT DATE COLUMNS\n",
      "======================================================================\n",
      "‚úÖ Converted First Seen At            : 9,919 valid dates\n",
      "   Range: 2024-03-04 to 2024-09-04\n",
      "‚úÖ Converted Last Seen At             : 9,919 valid dates\n",
      "   Range: 2024-03-06 to 2024-09-04\n",
      "‚úÖ Converted Job Last Processed At    : 9,919 valid dates\n",
      "   Range: 2024-02-22 to 2024-09-04\n",
      "\n",
      "‚úÖ Created new feature: posting_duration_days\n",
      "   Average duration: 39.3 days\n",
      "   Min duration: 0.0 days\n",
      "   Max duration: 182.0 days\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"3.4 CONVERT DATE COLUMNS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "date_columns = ['First Seen At', 'Last Seen At', 'Job Last Processed At']\n",
    "\n",
    "for col in date_columns:\n",
    "    if col in Job_Posting_clean.columns:\n",
    "        Job_Posting_clean[col] = pd.to_datetime(Job_Posting_clean[col], errors='coerce', utc=True)\n",
    "        valid_dates = Job_Posting_clean[col].notnull().sum()\n",
    "        print(f\"‚úÖ Converted {col:25}: {valid_dates:,} valid dates\")\n",
    "        \n",
    "        # Show date range\n",
    "        if valid_dates > 0:\n",
    "            min_date = Job_Posting_clean[col].min()\n",
    "            max_date = Job_Posting_clean[col].max()\n",
    "            print(f\"   Range: {min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Create new feature: Job posting duration (in days)\n",
    "if 'First Seen At' in Job_Posting_clean.columns and 'Last Seen At' in Job_Posting_clean.columns:\n",
    "   Job_Posting_clean['posting_duration_days'] = (Job_Posting_clean['Last Seen At'] - Job_Posting_clean['First Seen At']).dt.days\n",
    "   print(f\"\\n‚úÖ Created new feature: posting_duration_days\")\n",
    "   print(f\"   Average duration: {Job_Posting_clean['posting_duration_days'].mean():.1f} days\")\n",
    "   print(f\"   Min duration: {Job_Posting_clean['posting_duration_days'].min():.1f} days\")\n",
    "   print(f\"   Max duration: {Job_Posting_clean['posting_duration_days'].max():.1f} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3069c2-f940-4e87-aa35-9b9ead5f68bc",
   "metadata": {},
   "source": [
    "3.5 Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e03c840b-63c7-4e0d-af17-33cb4c7ee7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "3.5 HANDLE MISSING VALUES\n",
      "======================================================================\n",
      "Missing values before handling (top 10):\n",
      "region              9898\n",
      "continent           9886\n",
      "salary_time_unit    9485\n",
      "salary_high_usd     9485\n",
      "salary_low_usd      9485\n",
      "salary_currency     9485\n",
      "salary_high         9485\n",
      "salary_low          9485\n",
      "Salary              9343\n",
      "state               7469\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"3.5 HANDLE MISSING VALUES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Track missing values before handling\n",
    "missing_before = Job_Posting_clean.isnull().sum().sort_values(ascending=False)\n",
    "print(\"Missing values before handling (top 10):\")\n",
    "print(missing_before.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c16bd286-0ecb-4245-971c-38706208d1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MISSING VALUE HANDLING STRATEGY\n",
      "======================================================================\n",
      "\n",
      "Handling strategy for key columns:\n",
      "--------------------------------------------------\n",
      "Category             | 1,669 missing ( 16.8%) ‚Üí Fill with 'unknown' category\n",
      "Job Status           | 3,147 missing ( 31.7%) ‚Üí Fill with 'unknown' status\n",
      "Keywords             | 2,273 missing ( 22.9%) ‚Üí Fill with empty string\n",
      "Contract Types       | 1,915 missing ( 19.3%) ‚Üí Fill with 'not_specified'\n",
      "Location             |   411 missing (  4.1%) ‚Üí Keep as is (95.9% complete), fill with 'Unknown'\n",
      "Description          |   112 missing (  1.1%) ‚Üí Drop rows (only 112 missing)\n",
      "city                 | 3,638 missing ( 36.7%) ‚Üí Keep parsed values (some will be null)\n",
      "state                | 7,469 missing ( 75.3%) ‚Üí Keep parsed values\n",
      "country              |   470 missing (  4.7%) ‚Üí Keep parsed values\n",
      "salary_low           | 9,485 missing ( 95.6%) ‚Üí Keep as is (salary data is sparse)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MISSING VALUE HANDLING STRATEGY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Strategy for each column\n",
    "missing_strategies = {\n",
    "    'Category': \"Fill with 'unknown' category\",\n",
    "    'Job Status': \"Fill with 'unknown' status\",\n",
    "    'Keywords': \"Fill with empty string\",\n",
    "    'Contract Types': \"Fill with 'not_specified'\",\n",
    "    'Location': \"Keep as is (95.9% complete), fill with 'Unknown'\",\n",
    "    'Description': \"Drop rows (only 112 missing)\",\n",
    "    'city': \"Keep parsed values (some will be null)\",\n",
    "    'state': \"Keep parsed values\",\n",
    "    'country': \"Keep parsed values\",\n",
    "    'salary_low': \"Keep as is (salary data is sparse)\"\n",
    "}\n",
    "\n",
    "print(\"\\nHandling strategy for key columns:\")\n",
    "print(\"-\"*50)\n",
    "for col, strategy in missing_strategies.items():\n",
    "    if col in Job_Posting_clean.columns:\n",
    "        missing = Job_Posting_clean[col].isnull().sum()\n",
    "        pct = (missing / len(Job_Posting_clean)) * 100\n",
    "        print(f\"{col:20} | {missing:5,} missing ({pct:5.1f}%) ‚Üí {strategy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "054cbd09-741a-4a14-b23b-8f583070588a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "APPLYING MISSING VALUE HANDLING\n",
      "======================================================================\n",
      "‚úÖ Dropped 112 rows with missing Description\n",
      "\n",
      "Missing values after handling (top 10):\n",
      "region              9786\n",
      "continent           9774\n",
      "salary_time_unit    9373\n",
      "salary_high_usd     9373\n",
      "salary_low_usd      9373\n",
      "salary_currency     9373\n",
      "salary_high         9373\n",
      "salary_low          9373\n",
      "Salary              9231\n",
      "state               7381\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Apply missing value handling\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"APPLYING MISSING VALUE HANDLING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Fill categorical columns\n",
    "Job_Posting_clean['Category'] = Job_Posting_clean['Category'].fillna('unknown')\n",
    "Job_Posting_clean['Job Status'] = Job_Posting_clean['Job Status'].fillna('unknown')\n",
    "Job_Posting_clean['Keywords'] = Job_Posting_clean['Keywords'].fillna('')\n",
    "Job_Posting_clean['Contract Types'] = Job_Posting_clean['Contract Types'].fillna('not_specified')\n",
    "Job_Posting_clean['Location'] = Job_Posting_clean['Location'].fillna('Unknown')\n",
    "\n",
    "# For Description, we have very few missing, so we can drop\n",
    "rows_before = len(Job_Posting_clean)\n",
    "Job_Posting_clean = Job_Posting_clean.dropna(subset=['Description'])\n",
    "rows_after = len(Job_Posting_clean)\n",
    "print(f\"‚úÖ Dropped {rows_before - rows_after} rows with missing Description\")\n",
    "\n",
    "print(\"\\nMissing values after handling (top 10):\")\n",
    "missing_after = Job_Posting_clean.isnull().sum().sort_values(ascending=False)\n",
    "print(missing_after.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced2386c-cf4a-45db-bab5-3d1b62d19d9d",
   "metadata": {},
   "source": [
    " 3.6 Standardize Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbfba03b-41ed-4599-b802-ec421990d8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "3.6 CLEAN CATEGORICAL COLUMNS\n",
      "======================================================================\n",
      "üîß Cleaning 'Category' column...\n",
      "‚úÖ Created Category_list and has_multiple_categories features\n",
      "   Jobs with multiple categories: 3,994 (40.7%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"3.6 CLEAN CATEGORICAL COLUMNS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Clean Category column - split multiple categories\n",
    "print(\"üîß Cleaning 'Category' column...\")\n",
    "Job_Posting_clean['Category_list'] = Job_Posting_clean['Category'].apply(\n",
    "    lambda x: [cat.strip() for cat in str(x).split(',')] if pd.notnull(x) else []\n",
    ")\n",
    "\n",
    "# Create indicator for single vs multiple categories\n",
    "Job_Posting_clean['has_multiple_categories'] = Job_Posting_clean['Category_list'].apply(lambda x: len(x) > 1)\n",
    "\n",
    "print(f\"‚úÖ Created Category_list and has_multiple_categories features\")\n",
    "print(f\"   Jobs with multiple categories: {Job_Posting_clean['has_multiple_categories'].sum():,} ({Job_Posting_clean['has_multiple_categories'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e273584-ed72-4d84-877d-b553cc763ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Cleaning 'Seniority' column...\n",
      "‚úÖ Standardized Seniority levels:\n",
      "Seniority_clean\n",
      "individual_contributor    7889\n",
      "manager                   1791\n",
      "director_level             107\n",
      "executive                   20\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Clean Seniority column\n",
    "print(\"\\nüîß Cleaning 'Seniority' column...\")\n",
    "seniority_mapping = {\n",
    "    'non_manager': 'individual_contributor',\n",
    "    'manager': 'manager',\n",
    "    'head': 'director_level',\n",
    "    'director': 'director_level',\n",
    "    'c_level': 'executive',\n",
    "    'vice_president': 'executive',\n",
    "    'partner': 'executive',\n",
    "    'president': 'executive'\n",
    "}\n",
    "\n",
    "Job_Posting_clean['Seniority_clean'] = Job_Posting_clean['Seniority'].map(seniority_mapping)\n",
    "Job_Posting_clean['Seniority_clean'] = Job_Posting_clean['Seniority_clean'].fillna('other')\n",
    "\n",
    "print(\"‚úÖ Standardized Seniority levels:\")\n",
    "print(Job_Posting_clean['Seniority_clean'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23e8d8cc-1a79-45dd-a910-b3ed50290a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Cleaning 'Contract Types' column...\n",
      "‚úÖ Primary contract types:\n",
      "Contract_Type_primary\n",
      "full_time        5348\n",
      "not_specified    1902\n",
      "internship        741\n",
      "hybrid            434\n",
      "part_time         188\n",
      "long term         179\n",
      "all levels        176\n",
      "contract          174\n",
      "remote            170\n",
      "permanent          83\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Clean Contract Types\n",
    "print(\"\\nüîß Cleaning 'Contract Types' column...\")\n",
    "\n",
    "# Extract primary contract type (first one if multiple)\n",
    "def extract_primary_contract(contract_str):\n",
    "    if pd.isna(contract_str) or contract_str == 'not_specified':\n",
    "        return 'not_specified'\n",
    "    \n",
    "    # Split by comma and take first\n",
    "    contracts = str(contract_str).split(',')\n",
    "    primary = contracts[0].strip().lower()\n",
    "    \n",
    "    # Map to standard terms\n",
    "    contract_mapping = {\n",
    "        'full time': 'full_time',\n",
    "        'part time': 'part_time',\n",
    "        'intern': 'internship',\n",
    "        'vollzeit': 'full_time',  # German\n",
    "        'tempo integral': 'full_time',  # Portuguese\n",
    "        'm/f': 'full_time',  # Probably means full-time\n",
    "        'm/w': 'full_time',  # Probably means full-time\n",
    "        'hybrid': 'hybrid'\n",
    "    }\n",
    "    \n",
    "    return contract_mapping.get(primary, primary)\n",
    "\n",
    "Job_Posting_clean['Contract_Type_primary'] = Job_Posting_clean['Contract Types'].apply(extract_primary_contract)\n",
    "\n",
    "print(\"‚úÖ Primary contract types:\")\n",
    "print(Job_Posting_clean['Contract_Type_primary'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d90c8-45bb-49b9-9352-a4a0c478f4da",
   "metadata": {},
   "source": [
    " 3.7 Cleaning Text Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe156844-2fad-4261-ab70-1fb2528d40dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "3.7 CLEAN TEXT COLUMNS\n",
      "======================================================================\n",
      "üîß Cleaning 'Job Opening Title'...\n",
      "‚úÖ Title indicators extracted:\n",
      "   - title_has_senior    :    630 (6.4%)\n",
      "   - title_has_junior    :     78 (0.8%)\n",
      "   - title_has_manager   :  1,044 (10.6%)\n",
      "   - title_has_engineer  :  1,902 (19.4%)\n",
      "   - title_has_developer :    351 (3.6%)\n",
      "   - title_has_analyst   :    361 (3.7%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"3.7 CLEAN TEXT COLUMNS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Clean Job Opening Title\n",
    "print(\"üîß Cleaning 'Job Opening Title'...\")\n",
    "\n",
    "# Remove extra whitespace and standardize case\n",
    "Job_Posting_clean['Title_clean'] = Job_Posting_clean['Job Opening Title'].str.strip().str.lower()\n",
    "\n",
    "# Extract potential indicators from title\n",
    "Job_Posting_clean['title_has_senior'] = Job_Posting_clean['Title_clean'].str.contains('senior', case=False)\n",
    "Job_Posting_clean['title_has_junior'] = Job_Posting_clean['Title_clean'].str.contains('junior', case=False)\n",
    "Job_Posting_clean['title_has_manager'] = Job_Posting_clean['Title_clean'].str.contains('manager', case=False)\n",
    "Job_Posting_clean['title_has_engineer'] = Job_Posting_clean['Title_clean'].str.contains('engineer', case=False)\n",
    "Job_Posting_clean['title_has_developer'] = Job_Posting_clean['Title_clean'].str.contains('developer', case=False)\n",
    "Job_Posting_clean['title_has_analyst'] = Job_Posting_clean['Title_clean'].str.contains('analyst', case=False)\n",
    "\n",
    "print(\"‚úÖ Title indicators extracted:\")\n",
    "indicators = ['title_has_senior', 'title_has_junior', 'title_has_manager', \n",
    "              'title_has_engineer', 'title_has_developer', 'title_has_analyst']\n",
    "for indicator in indicators:\n",
    "    count = Job_Posting_clean[indicator].sum()\n",
    "    print(f\"   - {indicator:20}: {count:6,} ({count/len(Job_Posting_clean)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d315506b-3e39-4344-ba98-cc7b41f28f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Initial cleaning of 'Description'...\n",
      "‚úÖ Description length statistics:\n",
      "   Average: 3401 characters\n",
      "   Min: 165 characters\n",
      "   Max: 8162 characters\n"
     ]
    }
   ],
   "source": [
    "# Initial Description cleaning\n",
    "print(\"\\nüîß Initial cleaning of 'Description'...\")\n",
    "\n",
    "# Store original length\n",
    "Job_Posting_clean['Description_length'] = Job_Posting_clean['Description'].str.len()\n",
    "\n",
    "# Basic cleaning: remove extra whitespace\n",
    "Job_Posting_clean['Description_clean'] = Job_Posting_clean['Description'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "print(f\"‚úÖ Description length statistics:\")\n",
    "print(f\"   Average: {Job_Posting_clean['Description_length'].mean():.0f} characters\")\n",
    "print(f\"   Min: {Job_Posting_clean['Description_length'].min():.0f} characters\")\n",
    "print(f\"   Max: {Job_Posting_clean['Description_length'].max():.0f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2aa058-9f02-48f2-bbe0-167fbd7fd22d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
